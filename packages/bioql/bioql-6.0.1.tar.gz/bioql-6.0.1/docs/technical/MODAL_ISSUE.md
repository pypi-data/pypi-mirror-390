# Modal Server Issue - Quantized Model Instability

## Problem
El modelo base Qwen2.5-7B sin fine-tuning genera valores `inf`/`nan` cuando se usa con cuantizaciÃ³n (4-bit o 8-bit) en Modal.

## Intentos realizados:
1. âœ… Arreglado dependencias (numpy <2.0, scipy)
2. âœ… Agregado `renormalize_logits=True`
3. âœ… Cambiado de 4-bit a 8-bit quantization
4. âœ… Cambiado a greedy decoding (`do_sample=False`)
5. âŒ Todos fallan con el mismo error

## Error:
```
RuntimeError: probability tensor contains either `inf`, `nan` or element < 0
  at torch.multinomial()
```

## SoluciÃ³n temporal:
**Usa modo "template" en VS Code** hasta que el training termine:

```json
{
  "bioql.mode": "template",
  "bioql.enableChat": true
}
```

## SoluciÃ³n definitiva:
**Esperar a que termine el training TRAIN_ROBUST.py** (~2-3 horas mÃ¡s)

El modelo fine-tuned serÃ¡ estable y podrÃ¡:
- Usar con 4-bit quantization sin problemas
- Generar cÃ³digo BioQL de mejor calidad
- Responder especÃ­ficamente a quantum programming tasks

## Status del Training:
- âœ… Dataset generado (100K examples)
- âœ… Modelo Qwen2.5-7B cargado
- âœ… LoRA configurado (10M params entrenables)
- ðŸ”„ Tokenizando dataset actualmente
- â³ Training iniciarÃ¡ pronto

Monitor: https://modal.com/apps/spectrix/main/ap-KAm0DiHDJqgnkwnLeGp6jM

## Alternativa: Modelo sin cuantizaciÃ³n
Si necesitas Modal YA, puedo deployer el modelo sin cuantizaciÃ³n (fp16 completo) pero:
- RequerirÃ¡ GPU mÃ¡s grande (A10G â†’ A100)
- CostarÃ¡ mÃ¡s (~$0.001/seg vs $0.0004/seg)
- SeguirÃ¡ sin estar fine-tuned para BioQL
