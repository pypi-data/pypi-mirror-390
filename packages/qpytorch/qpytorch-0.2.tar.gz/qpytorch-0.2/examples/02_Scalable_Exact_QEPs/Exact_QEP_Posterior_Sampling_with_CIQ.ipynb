{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scalable Exact QEP Posterior Sampling using Contour Integral Quadrature\n",
    "\n",
    "This notebook demonstrates the most simple usage of contour integral quadrature with msMINRES as described [here](https://arxiv.org/pdf/2006.11267.pdf) to sample from the predictive distribution of an exact QEP.\n",
    "\n",
    "Note that to achieve results where Cholesky would run the GPU out of memory, you'll need to have KeOps installed (see our KeOps tutorial in this same folder). Despite this, on this relatively simple example with 1000 training points but seeing to sample at 20000 test points in 1D, we will achieve significant speed ups over Cholesky."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import qpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", qpytorch.utils.warnings.NumericalWarning)\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data is 11 points in [0,1] inclusive regularly spaced\n",
    "train_x = torch.linspace(0, 1, 1000)\n",
    "# True function is sin(2*pi*x) with Gaussian noise\n",
    "train_y = torch.sin(train_x * (2 * math.pi)) + torch.randn(train_x.size()) * 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Are we running with KeOps?\n",
    "\n",
    "If you have KeOps, change the below flag to `True` to run with a significantly larger test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "HAVE_KEOPS = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define an Exact QEP Model and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "POWER = 1.0\n",
    "class ExactQEPModel(qpytorch.models.ExactQEP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactQEPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.power = torch.tensor(POWER)\n",
    "        self.mean_module = qpytorch.means.ConstantMean()\n",
    "        \n",
    "        if HAVE_KEOPS:\n",
    "            self.covar_module = qpytorch.kernels.ScaleKernel(qpytorch.kernels.keops.RBFKernel())\n",
    "        else:\n",
    "            self.covar_module = qpytorch.kernels.ScaleKernel(qpytorch.kernels.RBFKernel())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return qpytorch.distributions.MultivariateQExponential(mean_x, covar_x, power=self.power)\n",
    "\n",
    "# initialize likelihood and model\n",
    "likelihood = qpytorch.likelihoods.QExponentialLikelihood(power=torch.tensor(POWER))\n",
    "model = ExactQEPModel(train_x, train_y, likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    train_x = train_x.cuda()\n",
    "    train_y = train_y.cuda()\n",
    "    model = model.cuda()\n",
    "    likelihood = likelihood.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1/50 - Loss: 2.124   lengthscale: 0.693   noise: 0.693\n",
      "Iter 2/50 - Loss: 2.066   lengthscale: 0.644   noise: 0.644\n",
      "Iter 3/50 - Loss: 1.998   lengthscale: 0.598   noise: 0.598\n",
      "Iter 4/50 - Loss: 1.924   lengthscale: 0.554   noise: 0.554\n",
      "Iter 5/50 - Loss: 1.846   lengthscale: 0.513   noise: 0.513\n",
      "Iter 6/50 - Loss: 1.781   lengthscale: 0.474   noise: 0.473\n",
      "Iter 7/50 - Loss: 1.724   lengthscale: 0.437   noise: 0.437\n",
      "Iter 8/50 - Loss: 1.679   lengthscale: 0.404   noise: 0.402\n",
      "Iter 9/50 - Loss: 1.642   lengthscale: 0.374   noise: 0.370\n",
      "Iter 10/50 - Loss: 1.615   lengthscale: 0.348   noise: 0.340\n",
      "Iter 11/50 - Loss: 1.587   lengthscale: 0.325   noise: 0.312\n",
      "Iter 12/50 - Loss: 1.563   lengthscale: 0.305   noise: 0.287\n",
      "Iter 13/50 - Loss: 1.542   lengthscale: 0.288   noise: 0.263\n",
      "Iter 14/50 - Loss: 1.518   lengthscale: 0.274   noise: 0.241\n",
      "Iter 15/50 - Loss: 1.499   lengthscale: 0.261   noise: 0.221\n",
      "Iter 16/50 - Loss: 1.470   lengthscale: 0.250   noise: 0.202\n",
      "Iter 17/50 - Loss: 1.453   lengthscale: 0.240   noise: 0.185\n",
      "Iter 18/50 - Loss: 1.429   lengthscale: 0.232   noise: 0.169\n",
      "Iter 19/50 - Loss: 1.408   lengthscale: 0.224   noise: 0.155\n",
      "Iter 20/50 - Loss: 1.381   lengthscale: 0.218   noise: 0.141\n",
      "Iter 21/50 - Loss: 1.368   lengthscale: 0.212   noise: 0.129\n",
      "Iter 22/50 - Loss: 1.343   lengthscale: 0.207   noise: 0.117\n",
      "Iter 23/50 - Loss: 1.328   lengthscale: 0.202   noise: 0.107\n",
      "Iter 24/50 - Loss: 1.305   lengthscale: 0.199   noise: 0.098\n",
      "Iter 25/50 - Loss: 1.278   lengthscale: 0.195   noise: 0.089\n",
      "Iter 26/50 - Loss: 1.252   lengthscale: 0.193   noise: 0.081\n",
      "Iter 27/50 - Loss: 1.231   lengthscale: 0.190   noise: 0.073\n",
      "Iter 28/50 - Loss: 1.213   lengthscale: 0.188   noise: 0.067\n",
      "Iter 29/50 - Loss: 1.189   lengthscale: 0.187   noise: 0.061\n",
      "Iter 30/50 - Loss: 1.162   lengthscale: 0.186   noise: 0.055\n",
      "Iter 31/50 - Loss: 1.137   lengthscale: 0.185   noise: 0.050\n",
      "Iter 32/50 - Loss: 1.112   lengthscale: 0.184   noise: 0.046\n",
      "Iter 33/50 - Loss: 1.102   lengthscale: 0.184   noise: 0.041\n",
      "Iter 34/50 - Loss: 1.068   lengthscale: 0.184   noise: 0.037\n",
      "Iter 35/50 - Loss: 1.047   lengthscale: 0.184   noise: 0.034\n",
      "Iter 36/50 - Loss: 1.022   lengthscale: 0.184   noise: 0.031\n",
      "Iter 37/50 - Loss: 1.005   lengthscale: 0.184   noise: 0.028\n",
      "Iter 38/50 - Loss: 0.984   lengthscale: 0.185   noise: 0.025\n",
      "Iter 39/50 - Loss: 0.957   lengthscale: 0.186   noise: 0.023\n",
      "Iter 40/50 - Loss: 0.932   lengthscale: 0.187   noise: 0.021\n",
      "Iter 41/50 - Loss: 0.910   lengthscale: 0.188   noise: 0.019\n",
      "Iter 42/50 - Loss: 0.881   lengthscale: 0.189   noise: 0.017\n",
      "Iter 43/50 - Loss: 0.866   lengthscale: 0.190   noise: 0.016\n",
      "Iter 44/50 - Loss: 0.840   lengthscale: 0.192   noise: 0.014\n",
      "Iter 45/50 - Loss: 0.818   lengthscale: 0.194   noise: 0.013\n",
      "Iter 46/50 - Loss: 0.788   lengthscale: 0.196   noise: 0.012\n",
      "Iter 47/50 - Loss: 0.771   lengthscale: 0.198   noise: 0.011\n",
      "Iter 48/50 - Loss: 0.753   lengthscale: 0.201   noise: 0.010\n",
      "Iter 49/50 - Loss: 0.727   lengthscale: 0.203   noise: 0.009\n",
      "Iter 50/50 - Loss: 0.707   lengthscale: 0.206   noise: 0.008\n"
     ]
    }
   ],
   "source": [
    "# this is for running the notebook in our testing framework\n",
    "import os\n",
    "smoke_test = ('CI' in os.environ)\n",
    "training_iter = 2 if smoke_test else 50\n",
    "\n",
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes QExponentialLikelihood parameters\n",
    "\n",
    "# \"Loss\" for QEPs - the marginal log likelihood\n",
    "mll = qpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "for i in range(training_iter):\n",
    "    # Zero gradients from previous iteration\n",
    "    optimizer.zero_grad()\n",
    "    # Output from model\n",
    "    output = model(train_x)\n",
    "    # Calc loss and backprop gradients\n",
    "    loss = -mll(output, train_y)\n",
    "    loss.backward()\n",
    "    print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f' % (\n",
    "        i + 1, training_iter, loss.item(),\n",
    "        model.covar_module.base_kernel.lengthscale.item(),\n",
    "        model.likelihood.noise.item()\n",
    "    ))\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define test set\n",
    "\n",
    "If we have KeOps installed, we'll test on 5000 points instead of 1000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5000])\n"
     ]
    }
   ],
   "source": [
    "if HAVE_KEOPS:\n",
    "    test_n = 5000\n",
    "else:\n",
    "    test_n = 1000\n",
    "\n",
    "test_x = torch.linspace(0, 1, test_n)\n",
    "if torch.cuda.is_available():\n",
    "    test_x = test_x.cuda()\n",
    "print(test_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw a sample with CIQ\n",
    "\n",
    "To do this, we just add the `ciq_samples` setting to the rsample call. We additionally demonstrate all relevant settings for controlling Contour Integral Quadrature:\n",
    "\n",
    "- The `ciq_samples` setting determines whether or not to use CIQ\n",
    "- The `num_contour_quadrature` setting controls the number of quadrature sites (Q in the paper).\n",
    "- The `minres_tolerance` setting controls the error we tolerate from minres (here, <0.01%).\n",
    "\n",
    "Note that, of these settings, increase num_contour_quadrature is unlikely to improve performance. As Theorem 1 from the paper demonstrates, virtually all of the error in this method is controlled by minres_tolerance. Here, we use a quite tight tolerance for minres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with CIQ\n",
      "CPU times: user 26.3 s, sys: 498 ms, total: 26.8 s\n",
      "Wall time: 4.56 s\n",
      "Running with Cholesky\n",
      "CPU times: user 1min 10s, sys: 1.7 s, total: 1min 11s\n",
      "Wall time: 12.5 s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Get into evaluation (predictive posterior) mode\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "# Test points are regularly spaced along [0,1]\n",
    "# Make predictions by feeding model through likelihood\n",
    "\n",
    "test_x.requires_grad_(True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    observed_pred = likelihood(model(test_x))\n",
    "    \n",
    "    # All relevant settings for using CIQ.\n",
    "    #   ciq_samples(True) - Use CIQ for sampling\n",
    "    #   num_contour_quadrature(10) -- Use 10 quadrature sites (Q in the paper)\n",
    "    #   minres_tolerance -- error tolerance from minres (here, <0.01%).\n",
    "    print(\"Running with CIQ\")\n",
    "    with qpytorch.settings.ciq_samples(True), qpytorch.settings.num_contour_quadrature(10), qpytorch.settings.minres_tolerance(1e-4):\n",
    "        %time y_samples = observed_pred.rsample()\n",
    "    \n",
    "    print(\"Running with Cholesky\")\n",
    "    # Make sure we use Cholesky\n",
    "    with qpytorch.settings.fast_computations(covar_root_decomposition=False):\n",
    "        %time y_samples = observed_pred.rsample()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
