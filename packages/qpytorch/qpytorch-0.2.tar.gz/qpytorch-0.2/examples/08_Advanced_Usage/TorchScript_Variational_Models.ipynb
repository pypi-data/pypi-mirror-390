{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting Variational Models to TorchScript\n",
    "\n",
    "The purpose of this notebook is to demonstrate how to convert a variational QPyTorch model to a ScriptModule that can e.g. be exported to LibTorch.\n",
    "\n",
    "In general the process is quite similar to standard torch models, where we will trace them using `torch.jit.trace`. However there are two key differences:\n",
    "\n",
    "1. The first time you make predictions with a QPyTorch model (exact or approximate), we cache certain computations. These computations can't be traced, but the results of them can be. Therefore, we'll need to pass data through the untraced model once, and then trace the model.\n",
    "1. You can't trace models that return Distribution objects. Therefore, we'll write a simple wrapper than unpacks the MultivariateNormal that our QEPs return in to just a mean and variance tensor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data and Define Model\n",
    "\n",
    "In this tutorial, we'll be tracing an SVQEP model trained for just 10 epochs on the `elevators` UCI dataset. The next two cells are copied directly from our variational tutorial, and download the data and define the variational QEP model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import urllib.request\n",
    "import os\n",
    "from scipy.io import loadmat\n",
    "from math import floor\n",
    "\n",
    "\n",
    "# this is for running the notebook in our testing framework\n",
    "smoke_test = ('CI' in os.environ)\n",
    "\n",
    "if not smoke_test and not os.path.isfile('../elevators.mat'):\n",
    "    print('Downloading \\'elevators\\' UCI dataset...')\n",
    "    urllib.request.urlretrieve('https://drive.google.com/uc?export=download&id=1jhWL3YUHvXIaftia4qeAyDwVxo6j1alk', '../elevators.mat')\n",
    "\n",
    "\n",
    "if smoke_test:  # this is for running the notebook in our testing framework\n",
    "    X, y = torch.randn(1000, 18), torch.randn(1000)\n",
    "else:\n",
    "    data = torch.Tensor(loadmat('../elevators.mat')['data'])\n",
    "    X = data[:, :-1]\n",
    "    X = X - X.min(0)[0]\n",
    "    X = 2 * (X / X.max(0)[0]) - 1\n",
    "    y = data[:, -1]\n",
    "\n",
    "\n",
    "train_n = int(floor(0.8 * len(X)))\n",
    "train_x = X[:train_n, :].contiguous()\n",
    "train_y = y[:train_n].contiguous()\n",
    "\n",
    "test_x = X[train_n:, :].contiguous()\n",
    "test_y = y[train_n:].contiguous()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    train_x, train_y, test_x, test_y = train_x.cuda(), train_y.cuda(), test_x.cuda(), test_y.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import qpytorch\n",
    "\n",
    "from qpytorch.models import ApproximateQEP\n",
    "from qpytorch.variational import CholeskyVariationalDistribution\n",
    "from qpytorch.variational import VariationalStrategy\n",
    "POWER = 1.0\n",
    "\n",
    "class QEPModel(ApproximateQEP):\n",
    "    def __init__(self, inducing_points):\n",
    "        self.power = torch.tensor(POWER)\n",
    "        variational_distribution = CholeskyVariationalDistribution(inducing_points.size(0), power=self.power)\n",
    "        variational_strategy = VariationalStrategy(self, inducing_points, variational_distribution, learn_inducing_locations=True)\n",
    "        super(QEPModel, self).__init__(variational_strategy)\n",
    "        self.mean_module = qpytorch.means.ConstantMean()\n",
    "        self.covar_module = qpytorch.kernels.ScaleKernel(qpytorch.kernels.RBFKernel(ard_num_dims=18))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return qpytorch.distributions.MultivariateQExponential(mean_x, covar_x, power=self.power)\n",
    "\n",
    "inducing_points = torch.randn(500, 18)\n",
    "model = QEPModel(inducing_points=inducing_points)\n",
    "likelihood = qpytorch.likelihoods.QExponentialLikelihood(power=model.power)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    likelihood = likelihood.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a Trained Model\n",
    "\n",
    "To keep things simple for this notebook, we won't be training here. Instead, we'll be loading the parameters for a pre-trained model on elevators that we trained in the SVQEP example notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shiweilan/miniconda/envs/qpytorch/lib/python3.12/site-packages/gpytorch/means/constant_mean.py:20: OldVersionWarning: You have loaded a GP model with a ConstantMean  from a previous version of GPyTorch. The mean module parameter `constant` has been renamed to `raw_constant`. Additionally, the shape of `raw_constant` is now *batch_shape, whereas the shape of `constant` was *batch_shape x 1. We have updated the name/shape of the parameter in your state dict, but we recommend that you re-save your model.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    model_state_dict, likelihood_state_dict = torch.load('svqep_elevators.pt')\n",
    "else:\n",
    "    model_state_dict, likelihood_state_dict = torch.load('svqep_elevators.pt', map_location='cpu')\n",
    "model.load_state_dict(model_state_dict)\n",
    "likelihood.load_state_dict(likelihood_state_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Wrapper\n",
    "\n",
    "Instead of directly tracing the QEP, we'll need to trace a PyTorch Module that returns tensors. In the next cell, we define a wrapper that calls a QEP and then unpacks the resulting Distribution in to a mean and variance.\n",
    "\n",
    "You could also return the full `covariance_matrix` if you wanted that rather than the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanVarModelWrapper(torch.nn.Module):\n",
    "    def __init__(self, qep):\n",
    "        super().__init__()\n",
    "        self.qep = qep\n",
    "    \n",
    "    def forward(self, x):\n",
    "        output_dist = self.qep(x)\n",
    "        return output_dist.mean, output_dist.variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trace the Model\n",
    "\n",
    "In the next cell, we trace the model as normal, with the exception that we first pass data through the wrapped model so that QPyTorch can compute all of the things it needs to cache that can't be traced. Mostly, this just involves some complex linear algebra operations for variational QEPs.\n",
    "\n",
    "Additionally, we'll need to run with the `qpytorch.settings.trace_mode` setting enabled, because PyTorch can't trace custom autograd Functions. Note that this results in some inefficiencies, e.g. for variational models we will always compute the full predictive posterior covariance in the traced model. This is not so bad, because we can always just process minibatches of data.\n",
    "\n",
    "**Note:** You'll get a lot of warnings from the tracer. That's fine. QPyTorch models are pretty large graphs, and include things like `.item()` calls that you wouldn't normally encounter in a basic neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shiweilan/miniconda/envs/qpytorch/lib/python3.12/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-06 to the diagonal\n",
      "  warnings.warn(\n",
      "/Users/shiweilan/miniconda/envs/qpytorch/lib/python3.12/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-05 to the diagonal\n",
      "  warnings.warn(\n",
      "/Users/shiweilan/miniconda/envs/qpytorch/lib/python3.12/site-packages/linear_operator/utils/cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-04 to the diagonal\n",
      "  warnings.warn(\n",
      "/Users/shiweilan/miniconda/envs/qpytorch/lib/python3.12/site-packages/qpytorch/variational/variational_strategy.py:246: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if not self.updated_strategy.item() and not prior:\n",
      "/Users/shiweilan/miniconda/envs/qpytorch/lib/python3.12/site-packages/qpytorch/variational/_variational_strategy.py:361: TracerWarning: Converting a tensor to a Python number might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if not self.variational_params_initialized.item():\n",
      "/Users/shiweilan/miniconda/envs/qpytorch/lib/python3.12/site-packages/linear_operator/operators/_linear_operator.py:1015: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  diag = torch.tensor(jitter_val, dtype=self.dtype, device=self.device)\n",
      "/Users/shiweilan/miniconda/envs/qpytorch/lib/python3.12/site-packages/linear_operator/operators/_linear_operator.py:966: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if not self.is_square:\n",
      "/Users/shiweilan/miniconda/envs/qpytorch/lib/python3.12/site-packages/linear_operator/operators/diag_linear_operator.py:307: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if not (diag_values.dim() and diag_values.size(-1) == 1):\n",
      "/Users/shiweilan/miniconda/envs/qpytorch/lib/python3.12/site-packages/gpytorch/kernels/kernel.py:511: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if not x1_.size(-1) == x2_.size(-1):\n",
      "/Users/shiweilan/miniconda/envs/qpytorch/lib/python3.12/site-packages/gpytorch/lazy/lazy_evaluated_kernel_tensor.py:366: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if res.shape != self.shape:\n",
      "/Users/shiweilan/miniconda/envs/qpytorch/lib/python3.12/site-packages/linear_operator/utils/cholesky.py:21: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if settings.trace_mode.on() or not torch.any(info):\n",
      "/Users/shiweilan/miniconda/envs/qpytorch/lib/python3.12/site-packages/qpytorch/variational/variational_strategy.py:208: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if L.shape != induc_induc_covar.shape:\n",
      "/Users/shiweilan/miniconda/envs/qpytorch/lib/python3.12/site-packages/linear_operator/operators/_linear_operator.py:1873: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  other = torch.tensor(other, dtype=self.dtype, device=self.device)\n",
      "/Users/shiweilan/miniconda/envs/qpytorch/lib/python3.12/site-packages/linear_operator/operators/_linear_operator.py:1883: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if other.numel() == 1:\n",
      "/Users/shiweilan/miniconda/envs/qpytorch/lib/python3.12/site-packages/linear_operator/utils/broadcasting.py:18: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if n != shape_b[-2]:\n",
      "/Users/shiweilan/miniconda/envs/qpytorch/lib/python3.12/site-packages/qpytorch/distributions/multivariate_qexponential.py:474: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if variance.lt(min_variance).any():\n"
     ]
    }
   ],
   "source": [
    "wrapped_model = MeanVarModelWrapper(model)\n",
    "\n",
    "with torch.no_grad(), qpytorch.settings.trace_mode():\n",
    "    fake_input = test_x[:1024, :]\n",
    "    pred = wrapped_model(fake_input)  # Compute caches\n",
    "    traced_model = torch.jit.trace(wrapped_model, fake_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0761, grad_fn=<MeanBackward0>)\n",
      "tensor(0.0761, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "## Compute Errors on a minibatch\n",
    "\n",
    "mean1 = wrapped_model(test_x[:1024, :])[0]\n",
    "mean2 = traced_model(test_x[:1024, :])[0]\n",
    "\n",
    "print(torch.mean(torch.abs(mean1 - test_y[:1024])))\n",
    "print(torch.mean(torch.abs(mean2 - test_y[:1024])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "traced_model.save('traced_model.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
