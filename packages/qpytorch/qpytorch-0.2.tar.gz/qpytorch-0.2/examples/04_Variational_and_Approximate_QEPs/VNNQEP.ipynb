{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f82397f",
   "metadata": {},
   "source": [
    "# VNNQEP: Variational Nearest Neighbor Q-Exponential Procceses\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this notebook, we will give an overview of how to use variational nearest neighbor q-exponential processes (VNNQEP) (https://arxiv.org/abs/2202.01694) to rapidly train on the `elevators` UCI dataset.\n",
    "\n",
    "Similar to SVQEP (https://arxiv.org/abs/1309.6835), VNNQEP is a variational inducing point-based approach. Unlike SVQEP that is typically limited to thousands of inducing points, VNNQEP makes an additional approximation: it assumes that every inducing point and data point only depends on $\\leq K$ other inducing points. This is advantageous for multiple reasons:\n",
    "- The variational KL divergence term affords an unbiased stochastic estimate from a **minibatch of inducing points** \n",
    "- Consequentially, an unbiased estimate of the ELBO can be computed in $O(K^3)$ time.\n",
    "\n",
    "With this scalability, we recommend using $M=N$ inducing points, placing inducing points at every observed input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d828ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import math\n",
    "import torch\n",
    "import qpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Make plots inline\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a75b972",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "from scipy.io import loadmat\n",
    "from math import floor\n",
    "\n",
    "\n",
    "# this is for running the notebook in our testing framework\n",
    "smoke_test = ('CI' in os.environ)\n",
    "\n",
    "if not smoke_test and not os.path.isfile('../elevators.mat'):\n",
    "    print('Downloading \\'elevators\\' UCI dataset...')\n",
    "    urllib.request.urlretrieve('https://drive.google.com/uc?export=download&id=1jhWL3YUHvXIaftia4qeAyDwVxo6j1alk', '../elevators.mat')\n",
    "\n",
    "\n",
    "if smoke_test:  # this is for running the notebook in our testing framework\n",
    "    X, y = torch.randn(100, 3), torch.randn(100)\n",
    "else:\n",
    "    data = torch.Tensor(loadmat('../elevators.mat')['data'])\n",
    "    X = data[:1000, :-1]\n",
    "    X = X - X.min(0)[0]\n",
    "    X = 2 * (X / X.max(0)[0].clamp_min(1e-6)) - 1\n",
    "    y = data[:1000, -1]\n",
    "    y = y.sub(y.mean()).div(y.std())\n",
    "\n",
    "\n",
    "train_n = int(floor(0.8 * len(X)))\n",
    "train_x = X[:train_n, :].contiguous()\n",
    "train_y = y[:train_n].contiguous()\n",
    "\n",
    "test_x = X[train_n:, :].contiguous()\n",
    "test_y = y[train_n:].contiguous()\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    train_x, train_y, test_x, test_y = train_x.cuda(), train_y.cuda(), test_x.cuda(), test_y.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ba9673",
   "metadata": {},
   "source": [
    "## Creating a VNNQEP model\n",
    "\n",
    "Creating a VNNQEP model is similar to creating [other variational models](./SVQEP_Regression_CUDA.ipynb). Here are some things that are specific to VNNQEP:\n",
    "\n",
    "1. You need to use the `qpytorch.variational.NNVariationalStrategy` variational strategy class.\n",
    "2. You need to use the `qpytorch.variational.MeanFieldVariationalDistribution` variational distribution class.\n",
    "3. `inducing_points` should be set to `train_x`. This results in fast optimization and accurate predictions. \n",
    "4. There are two hyperparameters that you need to specify:\n",
    "  - `k`: number of nearest neighbors used. The higher the `k` is, the better the approximation accuracy is, but also more computations are needed. Default value is 256. \n",
    "  - `training_batch_size`: the mini-batch size of inducing points used in stochastic optimization. Default value is 256. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4710cc73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shiweilan/miniconda/envs/qpytorch/lib/python3.10/site-packages/threadpoolctl.py:1226: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "from qpytorch.models import ApproximateQEP\n",
    "from qpytorch.variational.nearest_neighbor_variational_strategy import NNVariationalStrategy\n",
    "POWER = 1.0\n",
    "\n",
    "class QEPModel(ApproximateQEP):\n",
    "    def __init__(self, inducing_points, likelihood, k=256, training_batch_size=256):\n",
    "        self.power = torch.tensor(POWER)\n",
    "\n",
    "        m, d = inducing_points.shape\n",
    "        self.m = m\n",
    "        self.k = k\n",
    "\n",
    "        variational_distribution = qpytorch.variational.MeanFieldVariationalDistribution(m, power=self.power)\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            inducing_points = inducing_points.cuda()\n",
    "\n",
    "        variational_strategy = NNVariationalStrategy(self, inducing_points, variational_distribution, k=k,\n",
    "                                                     training_batch_size=training_batch_size)\n",
    "        super(QEPModel, self).__init__(variational_strategy)\n",
    "        self.mean_module = qpytorch.means.ZeroMean()\n",
    "        self.covar_module = qpytorch.kernels.MaternKernel(nu=2.5, ard_num_dims=d)\n",
    "        \n",
    "        self.likelihood = likelihood\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return qpytorch.distributions.MultivariateQExponential(mean_x, covar_x, power=self.power)\n",
    "\n",
    "    def __call__(self, x, prior=False, **kwargs):\n",
    "        if x is not None:\n",
    "            if x.dim() == 1:\n",
    "                x = x.unsqueeze(-1)\n",
    "        return self.variational_strategy(x=x, prior=False, **kwargs)\n",
    "    \n",
    "if smoke_test:\n",
    "    k = 32\n",
    "    training_batch_size = 32\n",
    "else:\n",
    "    k = 256\n",
    "    training_batch_size = 64\n",
    "\n",
    "likelihood = qpytorch.likelihoods.QExponentialLikelihood(power=torch.tensor(POWER))\n",
    "# Note: one should use full training set as inducing points!\n",
    "model = QEPModel(inducing_points=train_x, likelihood=likelihood, k=k, training_batch_size=training_batch_size)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    likelihood = likelihood.cuda()\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51911712",
   "metadata": {},
   "source": [
    "## Training the Model (mode 1, recommended)\n",
    "\n",
    "The cell below trains the model above, learning both the hyperparameters of the q-exponential process and the parameters of the neural network in an end-to-end fashion using Type-II MLE. VNNQEP's training objective is to miminize the negative ELBO. Note that in the beginning, we introduce that VNNQEP is ameanable to stochastic optimization over both data points and inducing points. That means in each iteration, we will sample a mini-batch of data points and a mini-batch of inducing points, compute the stochastic ELBO estimate, and then take a gradient step to update the model parameters. \n",
    "\n",
    "There are **two training modes available**. In this section we will introduce the mode 1, which is what we recommended in practice and what is implemented in experiments of the original paper. Since VNNQEP sets inducing point locations to observed input locations, `inducing points` are essentially the `train_x`. Therefore, there is no need to separately iterate over training data and inducing points. As a result, we could just sample a mini-batch of inducing points, which would be treated as a mini-batch of training data as well. In this case, the mini-batch size for training data is the same as that for inducing points, which is `training_batch_size` we set above. \n",
    "\n",
    "While we recommend this training mode as it yields faster training, we do provide another training mode that allows users to use different mini-batches of training data and inducing points. See the last part of the notebook. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4eff6df4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30639900b0af47588ddaa0e4b950e55e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18fe4a8595b74a4581d37d48e8ad66c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Minibatch:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shiweilan/Projects/QPyTorch/qpytorch/distributions/multivariate_qexponential.py:426: NumericalWarning: Negative variance values detected. This is likely due to numerical instabilities. Rounding negative variances up to 1e-06.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8af8b6da8712483dbaa591de3142cc20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Minibatch:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "006460f52a174b829f75e80f6ac5c7d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Minibatch:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7810df3b908745a4820f1c6c77e910f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Minibatch:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36240b2de18f4c74ad561cc770b93316",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Minibatch:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "771865fbe8e54a83b4dd9f14fd81893c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Minibatch:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b238c3c9adfe4828a73e269f9ce90074",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Minibatch:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fece8e68a41e41afa4ea9ed33e8a101e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Minibatch:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c002becbfaa49e58af7b6d0034e955d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Minibatch:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4e5724e4c98462e961b778881370e4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Minibatch:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "703545c7ff814ce3988efab1b0bf981f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Minibatch:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93e7bacaa8cc4d9b9906b4a32e59c149",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Minibatch:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd693a1b47f243d3b33365b4afff32e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Minibatch:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cfc225bad934ada9a86f4536c96df6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Minibatch:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61e36a1a19a848c3863606e1e464df37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Minibatch:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65c80a02e62349cea99dd1e656902d83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Minibatch:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4839f0e2e68641f8bd062dffd3cca544",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Minibatch:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e8cf3ed7347417dbbc7c6143340993c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Minibatch:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d05c94f92be4d5599d5879a8f3f1138",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Minibatch:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b734402a8e7e44889030d8a1caab90a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Minibatch:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_epochs = 1 if smoke_test else 20\n",
    "num_batches = model.variational_strategy._total_training_batches\n",
    "\n",
    "\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Our loss object. We're using the VariationalELBO\n",
    "mll = qpytorch.mlls.VariationalELBO(likelihood, model, num_data=train_y.size(0))\n",
    "\n",
    "\n",
    "epochs_iter = tqdm.notebook.tqdm(range(num_epochs), desc=\"Epoch\")\n",
    "for epoch in epochs_iter:\n",
    "    minibatch_iter = tqdm.notebook.tqdm(range(num_batches), desc=\"Minibatch\", leave=False)\n",
    "    \n",
    "    for i in minibatch_iter:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x=None)\n",
    "        # Obtain the indices for mini-batch data\n",
    "        current_training_indices = model.variational_strategy.current_training_indices\n",
    "        # Obtain the y_batch using indices. It is important to keep the same order of train_x and train_y\n",
    "        y_batch = train_y[...,current_training_indices]\n",
    "        if torch.cuda.is_available():\n",
    "            y_batch = y_batch.cuda()\n",
    "        loss = -mll(output, y_batch)\n",
    "        minibatch_iter.set_postfix(loss=loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67972fc5",
   "metadata": {},
   "source": [
    "## Making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed40984a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "test_dataset = TensorDataset(test_x, test_y)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91084c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "means = torch.tensor([0.])\n",
    "test_mse = 0\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch in test_loader:\n",
    "        preds = model(x_batch)\n",
    "        means = torch.cat([means, preds.mean.cpu()])\n",
    "         \n",
    "        diff = torch.pow(preds.mean - y_batch, 2)\n",
    "        diff = diff.sum(dim=-1) / test_x.size(0) # sum over bsz and scaling\n",
    "        diff = diff.mean() # average over likelihood_nsamples\n",
    "        test_mse += diff\n",
    "means = means[1:]\n",
    "test_rmse = test_mse.sqrt().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46a3b9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "test_dataset = TensorDataset(test_x, test_y)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b52a878",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "means = torch.tensor([0.])\n",
    "test_mse = 0\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch in test_loader:\n",
    "        preds = model(x_batch)\n",
    "        means = torch.cat([means, preds.mean.cpu()])\n",
    "        \n",
    "        diff = torch.pow(preds.mean - y_batch, 2)\n",
    "        diff = diff.sum(dim=-1) / test_x.size(0) # sum over bsz and scaling\n",
    "        diff = diff.mean() # average over likelihood_nsamples\n",
    "        test_mse += diff\n",
    "means = means[1:]\n",
    "test_rmse = test_mse.sqrt().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "edcda919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8163265585899353\n"
     ]
    }
   ],
   "source": [
    "print(test_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4915ee55",
   "metadata": {},
   "source": [
    "## Training the Model (mode 2)\n",
    "\n",
    "In this mode, users are able to sample separate mini-batches for training data and inducing points. Note that this will yield a slower training speed, since every iteration requires finding the set of inducing points that matches the current batch of training data for calculating ELBO. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f287d417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the model\n",
    "if smoke_test:\n",
    "    k = 32\n",
    "    training_batch_size = 32\n",
    "else:\n",
    "    k = 256\n",
    "    training_batch_size = 256\n",
    "likelihood = qpytorch.likelihoods.QExponentialLikelihood(power=torch.tensor(POWER))\n",
    "model = QEPModel(inducing_points=train_x, likelihood=likelihood, k=k, training_batch_size=training_batch_size)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    likelihood = likelihood.cuda()\n",
    "    model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4eb1b024",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b0dcb13548741e5b6d2b114f2777e5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c84efe0e9cad4924a97e98a8396d7cd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Minibatch:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/shiweilan/Projects/QPyTorch/qpytorch/distributions/multivariate_qexponential.py:426: NumericalWarning: Negative variance values detected. This is likely due to numerical instabilities. Rounding negative variances up to 1e-06.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "824b14b0366244c58508cc07113d6a62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Minibatch:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcd95f6a8b2d47a69265e44fbe909827",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Minibatch:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1798ebed4d124b83814d17ad98dfd9fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Minibatch:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92bddbb49cc14a60b68e0fbc29afdd5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Minibatch:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec70031fcfb7465580dc41bc73d76403",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Minibatch:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaaa436571fb47fb97a08d935e406f64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Minibatch:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "994c3883fcb340ce9d14bf8c182717eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Minibatch:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cba384962ead4752a266eda08f814f40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Minibatch:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40d841594633429e816f22d1e368c5d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Minibatch:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27aba2b3809f4de3b1040cf8dd45cb0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Minibatch:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6701a70be8b2479ea08bd8bf8225e489",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Minibatch:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0209bde04754580a8d68734824ca6ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Minibatch:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a72e4a7755054cc7894fe53c930e1b6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Minibatch:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f372a306ae0b455791be16bda2e86c44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Minibatch:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a9fa85659bb4a99bd3fdab281b428ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Minibatch:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "036d49bf44724fc2850ba1ab5407c907",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Minibatch:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee2dcea014eb48daa0dced5e6f038cf0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Minibatch:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17a581efd8eb44e2b3169c5b752edd99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Minibatch:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2446b5ff4892433a9e088cc4f2d2580c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Minibatch:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# prepare for dataset\n",
    "train_dataset = TensorDataset(train_x, train_y)\n",
    "# this batch-size does not need to match the training-batch-size specified above\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "num_epochs = 1 if smoke_test else 20\n",
    "\n",
    "\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Our loss object. We're using the VariationalELBO\n",
    "mll = qpytorch.mlls.VariationalELBO(likelihood, model, num_data=train_y.size(0))\n",
    "\n",
    "\n",
    "epochs_iter = tqdm.notebook.tqdm(range(num_epochs), desc=\"Epoch\")\n",
    "for i in epochs_iter:\n",
    "    # Within each iteration, we will go over each minibatch of data\n",
    "    minibatch_iter = tqdm.notebook.tqdm(train_loader, desc=\"Minibatch\", leave=False)\n",
    "    for x_batch, y_batch in minibatch_iter:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x_batch)\n",
    "        loss = -mll(output, y_batch)\n",
    "        minibatch_iter.set_postfix(loss=loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8754a538",
   "metadata": {},
   "source": [
    "## Making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "78894178",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "likelihood.eval()\n",
    "means = torch.tensor([0.])\n",
    "test_mse = 0\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch in test_loader:\n",
    "        preds = model(x_batch)\n",
    "        means = torch.cat([means, preds.mean.cpu()])\n",
    "         \n",
    "        diff = torch.pow(preds.mean - y_batch, 2)\n",
    "        diff = diff.sum(dim=-1) / test_x.size(0) # sum over bsz and scaling\n",
    "        diff = diff.mean() # average over likelihood_nsamples\n",
    "        test_mse += diff\n",
    "means = means[1:]\n",
    "test_rmse = test_mse.sqrt().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09ee8243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8201826810836792\n"
     ]
    }
   ],
   "source": [
    "print(test_rmse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
