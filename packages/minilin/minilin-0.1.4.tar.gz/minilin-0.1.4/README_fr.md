# MiniLin Framework

**Apprenez plus avec moins** - Un framework d'apprentissage profond universel pour les sc√©narios √† faibles ressources

[English](README.md) | [‰∏≠Êñá](README_cn.md) | [–†—É—Å—Å–∫–∏–π](README_ru.md) | [Fran√ßais](README_fr.md) | [ÿßŸÑÿπÿ±ÿ®Ÿäÿ©](README_ar.md)

[![Version Python](https://img.shields.io/badge/python-3.8%2B-blue.svg)](https://www.python.org/downloads/)
[![Licence](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)
[![Version](https://img.shields.io/badge/version-0.1.0-orange.svg)](https://github.com/alltobebetter/minilin)

## üöÄ Qu'est-ce que MiniLin?

MiniLin est un framework d'apprentissage profond con√ßu pour les **sc√©narios √† faibles ressources** o√π les donn√©es sont rares et les ressources de calcul limit√©es. Il fournit un flux de travail automatis√© de bout en bout pour les t√¢ches de texte, d'image et d'audio, avec une optimisation int√©gr√©e pour le d√©ploiement sur des appareils p√©riph√©riques.

### Caract√©ristiques principales

- üéØ **Solution en 3 lignes**: Pipeline ML complet des donn√©es au d√©ploiement
- ü§ñ **S√©lection automatique de strat√©gie**: Choisit automatiquement la strat√©gie d'entra√Ænement optimale
- üì¶ **Mod√®les l√©gers**: Mod√®les efficaces pr√©-int√©gr√©s
- üîß **Compression de mod√®les**: Quantification, √©lagage et distillation de connaissances int√©gr√©s
- üì± **D√©ploiement p√©riph√©rique**: Export vers ONNX, TFLite, TensorRT
- üåê **Multi-modal**: Support pour texte, images et audio
- üéì **Apprentissage few-shot**: LoRA, Adapter et Prompt Tuning
- üîÑ **Augmentation de donn√©es**: R√©tro-traduction, Mixup, CutMix
- üöÄ **D√©ploiement API**: Serveur FastAPI REST API

## üì¶ Installation

### Installation de base
```bash
pip install minilin
```

### Avec d√©pendances optionnelles
```bash
# Pour les t√¢ches de vision
pip install minilin[vision]

# Pour les t√¢ches audio
pip install minilin[audio]

# Pour les fonctionnalit√©s d'optimisation (LoRA, Adapter)
pip install minilin[optimization]

# Pour le d√©ploiement (FastAPI)
pip install minilin[deployment]

# Tout installer
pip install minilin[all]
```

## üéØ D√©marrage rapide

### Utilisation de base (3 lignes!)
```python
from minilin import AutoPipeline

pipeline = AutoPipeline(task="text_classification", data_path="./data")
pipeline.train()
pipeline.deploy(output_path="./model.onnx")
```

### Utilisation avanc√©e
```python
from minilin import AutoPipeline

pipeline = AutoPipeline(
    task="text_classification",
    data_path="./data",
    target_device="mobile",      # Appareil cible: mobile, edge, cloud
    max_samples=500,             # √âchantillons d'entra√Ænement maximum
    compression_level="high"     # Niveau de compression: low, medium, high
)

# Analyser les donn√©es
analysis = pipeline.analyze_data()
print(f"Strat√©gie recommand√©e: {analysis['recommended_strategy']}")

# Entra√Ænement
pipeline.train(epochs=10, batch_size=16, learning_rate=2e-5)

# √âvaluation
metrics = pipeline.evaluate()
print(f"Pr√©cision: {metrics['accuracy']:.4f}")

# D√©ploiement avec quantification
pipeline.deploy(output_path="./model_mobile.onnx", quantization="int8")
```

## üéì Fonctionnalit√©s avanc√©es

### Apprentissage few-shot avec LoRA
```python
from minilin.models import apply_few_shot_method

# Appliquer LoRA pour un fine-tuning efficace
model = apply_few_shot_method(model, method="lora", r=8, alpha=16)

# Entra√Ænement avec seulement 50 exemples!
pipeline.train(max_samples=50, epochs=20)
```

### Distillation de connaissances
```python
from minilin.optimization import KnowledgeDistiller

# Distiller les connaissances d'un grand mod√®le vers un petit mod√®le
distiller = KnowledgeDistiller(
    teacher_model=large_model,
    student_model=small_model,
    temperature=3.0,
    alpha=0.5
)

metrics = distiller.distill(train_loader, val_loader, epochs=5)
```

### Apprentissage multi-modal
```python
from minilin.models import create_multimodal_model

# Cr√©er un mod√®le multi-modal
model = create_multimodal_model(
    text_model_name="distilbert-base-uncased",
    image_model_name="mobilenetv3_small_100",
    num_classes=10,
    fusion_method="attention"
)
```

## üìä T√¢ches support√©es

### T√¢ches textuelles
- ‚úÖ Classification de texte
- ‚úÖ Reconnaissance d'entit√©s nomm√©es (NER)
- ‚úÖ Analyse de sentiment

### T√¢ches de vision
- ‚úÖ Classification d'images
- üîÑ D√©tection d'objets (bient√¥t)

### T√¢ches audio
- ‚úÖ Classification audio
- üîÑ Reconnaissance vocale (bient√¥t)

### T√¢ches multi-modales
- ‚úÖ Texte + Image
- ‚úÖ Texte + Audio
- ‚úÖ Texte + Image + Audio

## üî• Performance

- **Vitesse d'entra√Ænement**: 2-3x plus rapide que l'entra√Ænement standard
- **Taille du mod√®le**: Compress√© √† 10-20% de la taille originale
- **Vitesse d'inf√©rence**: Temps r√©el sur appareils p√©riph√©riques (>30 FPS)
- **Perte de pr√©cision**: <2% apr√®s compression

## üìö Exemples

Consultez le r√©pertoire [examples](examples/) pour plus d'exemples:

- [Classification de texte](examples/text_classification.py)
- [Classification d'images](examples/image_classification.py)
- [Classification audio](examples/audio_classification.py)
- [Apprentissage multi-modal](examples/multimodal_example.py)
- [Fonctionnalit√©s avanc√©es](examples/advanced_features.py)

## ü§ù Contribution

Nous accueillons les contributions! Veuillez consulter [CONTRIBUTING.md](CONTRIBUTING.md) pour plus de d√©tails.

## üìÑ Licence

Ce projet est sous licence MIT - voir le fichier [LICENSE](LICENSE) pour plus de d√©tails.

## üìß Contact

- **GitHub**: https://github.com/alltobebetter/minilin
- **Email**: me@supage.eu.org

---

**Fait avec ‚ù§Ô∏è par l'√©quipe MiniLin**
