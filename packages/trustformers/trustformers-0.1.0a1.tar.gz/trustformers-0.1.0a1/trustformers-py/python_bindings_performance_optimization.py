#!/usr/bin/env python3
"""
Python Bindings Performance Testing and Optimization for TrustformeRS
======================================================================

This module provides comprehensive performance testing and optimization capabilities for TrustformeRS
Python bindings, ensuring optimal performance across different use cases and deployment scenarios.

Key Features:
- Comprehensive benchmarking suite for Python/Rust interface
- Memory usage analysis and leak detection
- Binding overhead measurement and optimization
- Parallel processing performance evaluation
- Cross-platform performance validation
- Automated optimization recommendations
- Continuous performance monitoring

Author: TrustformeRS Development Team
License: MIT License
"""

import os
import sys
import gc
import time
import json
import asyncio
import threading
import multiprocessing
import tempfile
import traceback
from pathlib import Path
from typing import Dict, List, Optional, Union, Any, Tuple, Callable
from dataclasses import dataclass, field
from enum import Enum
from contextlib import contextmanager
from datetime import datetime, timedelta
import logging
import statistics
from functools import wraps
import cProfile
import pstats
import io

# Performance monitoring dependencies
try:
    import psutil
    PSUTIL_AVAILABLE = True
except ImportError:
    PSUTIL_AVAILABLE = False
    print("Warning: psutil not available. Install with: pip install psutil")

# Memory profiling
try:
    import memory_profiler
    MEMORY_PROFILER_AVAILABLE = True
except ImportError:
    MEMORY_PROFILER_AVAILABLE = False
    print("Warning: memory_profiler not available. Install with: pip install memory-profiler")

# Line profiler for detailed analysis
try:
    import line_profiler
    LINE_PROFILER_AVAILABLE = True
except ImportError:
    LINE_PROFILER_AVAILABLE = False
    print("Warning: line_profiler not available. Install with: pip install line_profiler")

# NumPy for numerical operations
try:
    import numpy as np
    NUMPY_AVAILABLE = True
except ImportError:
    NUMPY_AVAILABLE = False
    print("Warning: numpy not available. Install with: pip install numpy")

# Plotting for performance visualization
try:
    import matplotlib.pyplot as plt
    import matplotlib.dates as mdates
    MATPLOTLIB_AVAILABLE = True
except ImportError:
    MATPLOTLIB_AVAILABLE = False
    print("Warning: matplotlib not available. Install with: pip install matplotlib")

class BenchmarkType(Enum):
    """Types of performance benchmarks."""
    BINDING_OVERHEAD = "binding_overhead"
    MEMORY_USAGE = "memory_usage"
    TENSOR_OPERATIONS = "tensor_operations"
    MODEL_LOADING = "model_loading"
    INFERENCE_SPEED = "inference_speed"
    PARALLEL_PROCESSING = "parallel_processing"
    ZERO_COPY = "zero_copy"
    FRAMEWORK_INTEGRATION = "framework_integration"

class OptimizationLevel(Enum):
    """Optimization levels for testing."""
    DEBUG = "debug"
    DEVELOPMENT = "development"
    PRODUCTION = "production"
    MAXIMUM = "maximum"

class PerformanceMetric(Enum):
    """Performance metrics to track."""
    EXECUTION_TIME = "execution_time"
    MEMORY_USAGE = "memory_usage"
    CPU_USAGE = "cpu_usage"
    THROUGHPUT = "throughput"
    LATENCY = "latency"
    ALLOCATION_COUNT = "allocation_count"
    GC_COLLECTIONS = "gc_collections"

@dataclass
class BenchmarkConfig:
    """Configuration for performance benchmarks."""
    benchmark_types: List[BenchmarkType] = field(default_factory=lambda: list(BenchmarkType))
    optimization_level: OptimizationLevel = OptimizationLevel.PRODUCTION
    iterations: int = 100
    warmup_iterations: int = 10
    parallel_workers: int = multiprocessing.cpu_count()
    memory_sampling_interval: float = 0.1  # seconds
    enable_profiling: bool = True
    enable_memory_tracking: bool = True
    enable_visualization: bool = True
    output_dir: Optional[Path] = None
    save_raw_data: bool = True
    
@dataclass
class BenchmarkResult:
    """Result of a performance benchmark."""
    benchmark_name: str
    benchmark_type: BenchmarkType
    timestamp: datetime
    duration: float
    iterations: int
    metrics: Dict[PerformanceMetric, Any] = field(default_factory=dict)
    statistics: Dict[str, float] = field(default_factory=dict)
    optimization_suggestions: List[str] = field(default_factory=list)
    raw_data: Dict[str, List[float]] = field(default_factory=dict)
    metadata: Dict[str, Any] = field(default_factory=dict)

class PerformanceProfiler:
    """
    Performance profiler for detailed analysis of Python bindings.
    """
    
    def __init__(self):
        self.profiler = None
        self.memory_usage = []
        self.start_time = None
        self.monitoring = False
        
    @contextmanager
    def profile(self, enable_memory=True):
        """Context manager for performance profiling."""
        # Start CPU profiling
        self.profiler = cProfile.Profile()
        self.profiler.enable()
        
        # Start memory monitoring
        if enable_memory and PSUTIL_AVAILABLE:
            self.start_time = time.time()
            self.monitoring = True
            monitor_thread = threading.Thread(target=self._monitor_memory, daemon=True)
            monitor_thread.start()
            
        try:
            yield self
        finally:
            # Stop profiling
            self.profiler.disable()
            self.monitoring = False
            
    def _monitor_memory(self):
        """Monitor memory usage during profiling."""
        process = psutil.Process()
        
        while self.monitoring:
            try:
                memory_info = process.memory_info()
                self.memory_usage.append({
                    'timestamp': time.time() - self.start_time,
                    'rss_mb': memory_info.rss / 1024 / 1024,
                    'vms_mb': memory_info.vms / 1024 / 1024
                })
                time.sleep(0.1)
            except Exception:
                break
                
    def get_stats(self) -> Dict[str, Any]:
        """Get profiling statistics."""
        if not self.profiler:
            return {}
            
        # Get CPU profiling stats
        stats_stream = io.StringIO()
        stats = pstats.Stats(self.profiler, stream=stats_stream)
        stats.sort_stats('cumulative')
        stats.print_stats(20)  # Top 20 functions
        
        cpu_stats = stats_stream.getvalue()
        
        # Get memory stats
        memory_stats = {}
        if self.memory_usage:
            rss_values = [m['rss_mb'] for m in self.memory_usage]
            memory_stats = {
                'peak_memory_mb': max(rss_values),
                'average_memory_mb': statistics.mean(rss_values),
                'memory_growth_mb': rss_values[-1] - rss_values[0] if len(rss_values) > 1 else 0
            }
            
        return {
            'cpu_profile': cpu_stats,
            'memory_stats': memory_stats,
            'memory_timeline': self.memory_usage
        }

class PythonBindingsPerformanceTester:
    """
    Comprehensive performance testing and optimization framework for TrustformeRS Python bindings.
    
    Features:
    - Binding overhead measurement and analysis
    - Memory usage profiling and leak detection
    - Zero-copy operation validation
    - Parallel processing performance evaluation
    - Framework integration benchmarks
    - Automated optimization recommendations
    """
    
    def __init__(self, config: BenchmarkConfig):
        self.config = config
        self.project_root = Path(__file__).parent
        self.output_dir = config.output_dir or (self.project_root / "performance_results")
        self.artifacts_dir = self.output_dir / "artifacts"
        
        # Create directories
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.artifacts_dir.mkdir(parents=True, exist_ok=True)
        
        # Setup logging
        self.logger = self._setup_logging()
        
        # Performance data storage
        self.benchmark_results: List[BenchmarkResult] = []
        self.baseline_results: Dict[str, BenchmarkResult] = {}
        
        # System information
        self.system_info = self._gather_system_info()
        
    def _setup_logging(self) -> logging.Logger:
        """Setup logging for performance testing."""
        logger = logging.getLogger("PythonBindingsPerformanceTester")
        logger.setLevel(logging.INFO)
        
        # Clear existing handlers
        logger.handlers.clear()
        
        # Console handler
        console_handler = logging.StreamHandler()
        console_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
        console_handler.setFormatter(console_formatter)
        logger.addHandler(console_handler)
        
        # File handler
        log_file = self.output_dir / f"performance_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        file_handler = logging.FileHandler(log_file)
        file_formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        file_handler.setFormatter(file_formatter)
        logger.addHandler(file_handler)
        
        return logger
        
    def _gather_system_info(self) -> Dict[str, Any]:
        """Gather system information for benchmarking context."""
        info = {
            "python_version": sys.version,
            "platform": sys.platform,
            "cpu_count": multiprocessing.cpu_count(),
            "timestamp": datetime.now().isoformat()
        }
        
        if PSUTIL_AVAILABLE:
            info.update({
                "memory_total_gb": psutil.virtual_memory().total / (1024**3),
                "cpu_freq_mhz": psutil.cpu_freq().current if psutil.cpu_freq() else None,
                "disk_usage_gb": psutil.disk_usage('/').total / (1024**3)
            })
            
        return info
        
    def benchmark(self, benchmark_type: BenchmarkType, name: str = None):
        """Decorator for benchmark functions."""
        def decorator(func):
            benchmark_name = name or func.__name__
            
            @wraps(func)
            def wrapper(*args, **kwargs):
                return self._run_benchmark(func, benchmark_name, benchmark_type, *args, **kwargs)
            return wrapper
        return decorator
        
    def _run_benchmark(self, 
                      func: Callable, 
                      name: str, 
                      benchmark_type: BenchmarkType, 
                      *args, **kwargs) -> BenchmarkResult:
        """Run a single benchmark with comprehensive measurement."""
        self.logger.info(f"Running benchmark: {name} ({benchmark_type.value})")
        
        # Warmup
        for _ in range(self.config.warmup_iterations):
            try:
                func(*args, **kwargs)
            except Exception as e:
                self.logger.warning(f"Warmup iteration failed: {e}")
                
        # Force garbage collection before measurement
        gc.collect()
        
        # Measurements storage
        execution_times = []
        memory_measurements = []
        cpu_measurements = []
        
        # Detailed profiling
        profiler = PerformanceProfiler()
        
        start_time = datetime.now()
        
        with profiler.profile(enable_memory=self.config.enable_memory_tracking):
            for i in range(self.config.iterations):
                # Measure execution time
                iter_start = time.perf_counter()
                
                try:
                    result = func(*args, **kwargs)
                except Exception as e:
                    self.logger.error(f"Benchmark iteration {i} failed: {e}")
                    continue
                    
                iter_end = time.perf_counter()
                execution_times.append(iter_end - iter_start)
                
                # Measure memory and CPU usage
                if PSUTIL_AVAILABLE:
                    process = psutil.Process()
                    memory_info = process.memory_info()
                    memory_measurements.append(memory_info.rss / 1024 / 1024)  # MB
                    cpu_measurements.append(process.cpu_percent())
                    
        end_time = datetime.now()
        total_duration = (end_time - start_time).total_seconds()
        
        # Calculate statistics
        if execution_times:
            stats = {
                "mean_time": statistics.mean(execution_times),
                "median_time": statistics.median(execution_times),
                "std_time": statistics.stdev(execution_times) if len(execution_times) > 1 else 0,
                "min_time": min(execution_times),
                "max_time": max(execution_times),
                "p95_time": np.percentile(execution_times, 95) if NUMPY_AVAILABLE else 0,
                "p99_time": np.percentile(execution_times, 99) if NUMPY_AVAILABLE else 0,
                "throughput": len(execution_times) / sum(execution_times)
            }
        else:
            stats = {}
            
        # Gather performance metrics
        metrics = {
            PerformanceMetric.EXECUTION_TIME: stats.get("mean_time", 0),
            PerformanceMetric.THROUGHPUT: stats.get("throughput", 0)
        }
        
        if memory_measurements:
            metrics[PerformanceMetric.MEMORY_USAGE] = {
                "peak_mb": max(memory_measurements),
                "average_mb": statistics.mean(memory_measurements),
                "growth_mb": memory_measurements[-1] - memory_measurements[0] if len(memory_measurements) > 1 else 0
            }
            
        if cpu_measurements:
            metrics[PerformanceMetric.CPU_USAGE] = {
                "average_percent": statistics.mean(cpu_measurements),
                "peak_percent": max(cpu_measurements)
            }
            
        # Get profiling data
        profiling_stats = profiler.get_stats()
        
        # Generate optimization suggestions
        suggestions = self._generate_optimization_suggestions(stats, metrics, profiling_stats)
        
        # Create result
        result = BenchmarkResult(
            benchmark_name=name,
            benchmark_type=benchmark_type,
            timestamp=start_time,
            duration=total_duration,
            iterations=len(execution_times),
            metrics=metrics,
            statistics=stats,
            optimization_suggestions=suggestions,
            raw_data={
                "execution_times": execution_times,
                "memory_usage": memory_measurements,
                "cpu_usage": cpu_measurements
            },
            metadata={
                "profiling_stats": profiling_stats,
                "system_info": self.system_info,
                "config": {
                    "optimization_level": self.config.optimization_level.value,
                    "iterations": self.config.iterations
                }
            }
        )
        
        self.benchmark_results.append(result)
        
        self.logger.info(f"Benchmark completed: {name} - {stats.get('mean_time', 0):.4f}s avg")
        
        return result
        
    def _generate_optimization_suggestions(self, 
                                         stats: Dict[str, float], 
                                         metrics: Dict[PerformanceMetric, Any],
                                         profiling_stats: Dict[str, Any]) -> List[str]:
        """Generate optimization suggestions based on benchmark results."""
        suggestions = []
        
        # Execution time analysis
        mean_time = stats.get("mean_time", 0)
        std_time = stats.get("std_time", 0)
        
        if mean_time > 0.1:  # > 100ms
            suggestions.append("Consider using batch operations to reduce per-operation overhead")
            
        if std_time / mean_time > 0.3:  # High variance
            suggestions.append("High execution time variance detected - consider warmup or load balancing")
            
        # Memory usage analysis
        memory_info = metrics.get(PerformanceMetric.MEMORY_USAGE, {})
        if isinstance(memory_info, dict):
            growth_mb = memory_info.get("growth_mb", 0)
            peak_mb = memory_info.get("peak_mb", 0)
            
            if growth_mb > 10:  # > 10MB growth
                suggestions.append("Memory growth detected - check for memory leaks or excessive allocations")
                
            if peak_mb > 500:  # > 500MB peak
                suggestions.append("High memory usage - consider using zero-copy operations or memory pooling")
                
        # CPU usage analysis
        cpu_info = metrics.get(PerformanceMetric.CPU_USAGE, {})
        if isinstance(cpu_info, dict):
            avg_cpu = cpu_info.get("average_percent", 0)
            
            if avg_cpu < 30:  # Low CPU usage
                suggestions.append("Low CPU utilization - consider parallel processing or optimization")
            elif avg_cpu > 90:  # High CPU usage
                suggestions.append("High CPU usage - consider algorithmic optimizations or caching")
                
        # Profiling-based suggestions
        if "memory_stats" in profiling_stats:
            memory_stats = profiling_stats["memory_stats"]
            if memory_stats.get("memory_growth_mb", 0) > 5:
                suggestions.append("Memory growth during execution - review object lifecycle management")
                
        return suggestions
        
    def benchmark_binding_overhead(self) -> BenchmarkResult:
        """Benchmark Python/Rust binding overhead."""
        
        @self.benchmark(BenchmarkType.BINDING_OVERHEAD, "python_rust_binding_overhead")
        def measure_binding_overhead():
            try:
                import trustformers
                
                # Simple operation that crosses Python/Rust boundary
                tensor = trustformers.Tensor([1.0, 2.0, 3.0])
                shape = tensor.shape
                return shape
            except ImportError:
                # Fallback to dummy operation
                return (3,)
                
        return measure_binding_overhead()
        
    def benchmark_tensor_operations(self) -> List[BenchmarkResult]:
        """Benchmark tensor operations performance."""
        results = []
        
        @self.benchmark(BenchmarkType.TENSOR_OPERATIONS, "tensor_creation")
        def measure_tensor_creation():
            try:
                import trustformers
                data = list(range(1000))
                tensor = trustformers.Tensor(data)
                return tensor
            except ImportError:
                return list(range(1000))
                
        @self.benchmark(BenchmarkType.TENSOR_OPERATIONS, "tensor_arithmetic")
        def measure_tensor_arithmetic():
            try:
                import trustformers
                tensor1 = trustformers.Tensor([1.0] * 1000)
                tensor2 = trustformers.Tensor([2.0] * 1000)
                result = tensor1 + tensor2
                return result
            except ImportError:
                return [3.0] * 1000
                
        @self.benchmark(BenchmarkType.TENSOR_OPERATIONS, "tensor_numpy_conversion")
        def measure_numpy_conversion():
            try:
                import trustformers
                import numpy as np
                
                # Create tensor and convert to numpy
                tensor = trustformers.Tensor([1.0] * 1000)
                np_array = tensor.numpy()
                
                # Convert back
                tensor2 = trustformers.Tensor(np_array)
                return tensor2
            except ImportError:
                if NUMPY_AVAILABLE:
                    return np.array([1.0] * 1000)
                return [1.0] * 1000
                
        results.extend([
            measure_tensor_creation(),
            measure_tensor_arithmetic(),
            measure_numpy_conversion()
        ])
        
        return results
        
    def benchmark_zero_copy_operations(self) -> List[BenchmarkResult]:
        """Benchmark zero-copy operation performance."""
        results = []
        
        @self.benchmark(BenchmarkType.ZERO_COPY, "zero_copy_tensor_view")
        def measure_zero_copy():
            try:
                import trustformers
                import numpy as np
                
                # Create large numpy array
                np_array = np.random.rand(10000).astype(np.float32)
                
                # Create tensor view (should be zero-copy)
                tensor = trustformers.Tensor.from_numpy_zero_copy(np_array)
                
                # Access data (should not copy)
                view = tensor.numpy_view()
                
                return view.shape
            except (ImportError, AttributeError):
                if NUMPY_AVAILABLE:
                    return np.random.rand(10000).shape
                return (10000,)
                
        @self.benchmark(BenchmarkType.ZERO_COPY, "memory_copy_comparison")
        def measure_copy_vs_zero_copy():
            try:
                import trustformers
                import numpy as np
                
                np_array = np.random.rand(10000).astype(np.float32)
                
                # Regular copy operation
                tensor_copy = trustformers.Tensor(np_array)
                
                # Zero-copy operation
                tensor_zero_copy = trustformers.Tensor.from_numpy_zero_copy(np_array)
                
                return tensor_copy.shape, tensor_zero_copy.shape
            except (ImportError, AttributeError):
                if NUMPY_AVAILABLE:
                    arr = np.random.rand(10000)
                    return arr.shape, arr.shape
                return (10000,), (10000,)
                
        results.extend([
            measure_zero_copy(),
            measure_copy_vs_zero_copy()
        ])
        
        return results
        
    def benchmark_model_operations(self) -> List[BenchmarkResult]:
        """Benchmark model loading and inference performance."""
        results = []
        
        @self.benchmark(BenchmarkType.MODEL_LOADING, "model_loading_time")
        def measure_model_loading():
            try:
                import trustformers
                model = trustformers.BertModel.from_pretrained("bert-tiny")
                return model
            except ImportError:
                return "mock_model"
                
        @self.benchmark(BenchmarkType.INFERENCE_SPEED, "model_inference")
        def measure_inference():
            try:
                import trustformers
                
                model = trustformers.BertModel.from_pretrained("bert-tiny")
                tokenizer = trustformers.AutoTokenizer.from_pretrained("bert-tiny")
                
                text = "This is a test input for inference."
                inputs = tokenizer(text, return_tensors="pt")
                outputs = model(**inputs)
                
                return outputs
            except ImportError:
                return "mock_output"
                
        results.extend([
            measure_model_loading(),
            measure_inference()
        ])
        
        return results
        
    def benchmark_parallel_processing(self) -> List[BenchmarkResult]:
        """Benchmark parallel processing performance."""
        results = []
        
        @self.benchmark(BenchmarkType.PARALLEL_PROCESSING, "parallel_tensor_operations")
        def measure_parallel_tensors():
            try:
                import trustformers
                import concurrent.futures
                
                def create_and_process_tensor(size):
                    tensor = trustformers.Tensor([1.0] * size)
                    return tensor.numpy().sum()
                    
                # Process multiple tensors in parallel
                with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
                    futures = [executor.submit(create_and_process_tensor, 1000) for _ in range(10)]
                    results = [future.result() for future in futures]
                    
                return sum(results)
            except ImportError:
                return 10000.0
                
        @self.benchmark(BenchmarkType.PARALLEL_PROCESSING, "parallel_inference")
        def measure_parallel_inference():
            try:
                import trustformers
                import concurrent.futures
                
                model = trustformers.BertModel.from_pretrained("bert-tiny")
                tokenizer = trustformers.AutoTokenizer.from_pretrained("bert-tiny")
                
                def run_inference(text):
                    inputs = tokenizer(text, return_tensors="pt")
                    outputs = model(**inputs)
                    return outputs.last_hidden_state.shape
                    
                texts = [f"Test input {i}" for i in range(5)]
                
                with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
                    futures = [executor.submit(run_inference, text) for text in texts]
                    results = [future.result() for future in futures]
                    
                return results
            except ImportError:
                return [(1, 10, 768)] * 5
                
        results.extend([
            measure_parallel_tensors(),
            measure_parallel_inference()
        ])
        
        return results
        
    def benchmark_framework_integration(self) -> List[BenchmarkResult]:
        """Benchmark framework integration performance."""
        results = []
        
        @self.benchmark(BenchmarkType.FRAMEWORK_INTEGRATION, "pytorch_conversion")
        def measure_pytorch_conversion():
            try:
                import trustformers
                import torch
                
                # TrustformeRS to PyTorch
                tf_tensor = trustformers.Tensor([1.0] * 1000)
                torch_tensor = tf_tensor.to_torch()
                
                # PyTorch to TrustformeRS
                tf_tensor2 = trustformers.Tensor.from_torch(torch_tensor)
                
                return tf_tensor2.shape
            except ImportError:
                return (1000,)
                
        @self.benchmark(BenchmarkType.FRAMEWORK_INTEGRATION, "numpy_conversion")
        def measure_numpy_conversion():
            try:
                import trustformers
                import numpy as np
                
                # NumPy to TrustformeRS
                np_array = np.random.rand(1000).astype(np.float32)
                tf_tensor = trustformers.Tensor(np_array)
                
                # TrustformeRS to NumPy
                np_array2 = tf_tensor.numpy()
                
                return np_array2.shape
            except ImportError:
                if NUMPY_AVAILABLE:
                    return np.random.rand(1000).shape
                return (1000,)
                
        results.extend([
            measure_pytorch_conversion(),
            measure_numpy_conversion()
        ])
        
        return results
        
    def run_comprehensive_benchmarks(self) -> Dict[str, Any]:
        """Run comprehensive performance benchmarks."""
        self.logger.info("Starting comprehensive performance benchmarks")
        
        start_time = datetime.now()
        
        # Run all benchmark categories
        benchmark_functions = [
            ("Binding Overhead", self.benchmark_binding_overhead),
            ("Tensor Operations", self.benchmark_tensor_operations),
            ("Zero-Copy Operations", self.benchmark_zero_copy_operations),
            ("Model Operations", self.benchmark_model_operations),
            ("Parallel Processing", self.benchmark_parallel_processing),
            ("Framework Integration", self.benchmark_framework_integration)
        ]
        
        for category_name, benchmark_func in benchmark_functions:
            self.logger.info(f"Running {category_name} benchmarks...")
            
            try:
                result = benchmark_func()
                if isinstance(result, list):
                    self.logger.info(f"Completed {len(result)} benchmarks in {category_name}")
                else:
                    self.logger.info(f"Completed benchmark in {category_name}")
            except Exception as e:
                self.logger.error(f"Benchmark category {category_name} failed: {e}")
                
        end_time = datetime.now()
        total_duration = (end_time - start_time).total_seconds()
        
        # Generate comprehensive report
        report = self.generate_performance_report()
        report["total_duration"] = total_duration
        report["timestamp"] = start_time.isoformat()
        
        self.logger.info(f"Performance benchmarks completed in {total_duration:.2f}s")
        self.logger.info(f"Generated {len(self.benchmark_results)} benchmark results")
        
        return report
        
    def generate_performance_report(self) -> Dict[str, Any]:
        """Generate comprehensive performance report."""
        if not self.benchmark_results:
            return {"error": "No benchmark results available"}
            
        # Group results by type
        results_by_type = {}
        for result in self.benchmark_results:
            benchmark_type = result.benchmark_type.value
            if benchmark_type not in results_by_type:
                results_by_type[benchmark_type] = []
            results_by_type[benchmark_type].append(result)
            
        # Calculate summary statistics
        summary = {
            "total_benchmarks": len(self.benchmark_results),
            "benchmark_types": list(results_by_type.keys()),
            "system_info": self.system_info
        }
        
        # Detailed results per type
        detailed_results = {}
        for benchmark_type, results in results_by_type.items():
            type_summary = {
                "count": len(results),
                "benchmarks": []
            }
            
            for result in results:
                benchmark_info = {
                    "name": result.benchmark_name,
                    "duration": result.duration,
                    "iterations": result.iterations,
                    "statistics": result.statistics,
                    "optimization_suggestions": result.optimization_suggestions
                }
                
                # Add key metrics
                if PerformanceMetric.EXECUTION_TIME in result.metrics:
                    benchmark_info["avg_execution_time"] = result.metrics[PerformanceMetric.EXECUTION_TIME]
                    
                if PerformanceMetric.MEMORY_USAGE in result.metrics:
                    memory_info = result.metrics[PerformanceMetric.MEMORY_USAGE]
                    if isinstance(memory_info, dict):
                        benchmark_info["peak_memory_mb"] = memory_info.get("peak_mb", 0)
                        
                type_summary["benchmarks"].append(benchmark_info)
                
            detailed_results[benchmark_type] = type_summary
            
        # Performance recommendations
        recommendations = self._generate_performance_recommendations(results_by_type)
        
        report = {
            "summary": summary,
            "detailed_results": detailed_results,
            "recommendations": recommendations,
            "configuration": {
                "optimization_level": self.config.optimization_level.value,
                "iterations": self.config.iterations,
                "parallel_workers": self.config.parallel_workers
            }
        }
        
        # Save report
        self._save_performance_report(report)
        
        # Generate visualizations if enabled
        if self.config.enable_visualization and MATPLOTLIB_AVAILABLE:
            self._generate_performance_visualizations(results_by_type)
            
        return report
        
    def _generate_performance_recommendations(self, results_by_type: Dict[str, List[BenchmarkResult]]) -> List[str]:
        """Generate performance recommendations based on all results."""
        recommendations = []
        
        # Collect all optimization suggestions
        all_suggestions = []
        for results in results_by_type.values():
            for result in results:
                all_suggestions.extend(result.optimization_suggestions)
                
        # Remove duplicates and sort by frequency
        suggestion_counts = {}
        for suggestion in all_suggestions:
            suggestion_counts[suggestion] = suggestion_counts.get(suggestion, 0) + 1
            
        # Add most common suggestions as recommendations
        sorted_suggestions = sorted(suggestion_counts.items(), key=lambda x: x[1], reverse=True)
        recommendations.extend([suggestion for suggestion, count in sorted_suggestions[:5]])
        
        # Add specific recommendations based on performance patterns
        
        # Check tensor operation performance
        tensor_results = results_by_type.get("tensor_operations", [])
        if tensor_results:
            avg_times = [r.statistics.get("mean_time", 0) for r in tensor_results]
            if avg_times and statistics.mean(avg_times) > 0.01:
                recommendations.append("Consider using batch operations for tensor computations")
                
        # Check memory usage patterns
        memory_growth_detected = False
        for results in results_by_type.values():
            for result in results:
                if PerformanceMetric.MEMORY_USAGE in result.metrics:
                    memory_info = result.metrics[PerformanceMetric.MEMORY_USAGE]
                    if isinstance(memory_info, dict) and memory_info.get("growth_mb", 0) > 5:
                        memory_growth_detected = True
                        break
                        
        if memory_growth_detected:
            recommendations.append("Memory growth detected across multiple benchmarks - review memory management")
            
        # Check parallel processing efficiency
        parallel_results = results_by_type.get("parallel_processing", [])
        if parallel_results:
            # Compare with sequential performance (if available)
            recommendations.append("Consider optimizing parallel processing implementation")
            
        return recommendations
        
    def _save_performance_report(self, report: Dict[str, Any]):
        """Save performance report to files."""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Save JSON report
        json_path = self.output_dir / f"performance_report_{timestamp}.json"
        with open(json_path, 'w') as f:
            json.dump(report, f, indent=2, default=str)
            
        # Save detailed CSV data
        csv_path = self.output_dir / f"performance_data_{timestamp}.csv"
        self._export_to_csv(csv_path)
        
        self.logger.info(f"Performance report saved to {json_path}")
        
    def _export_to_csv(self, csv_path: Path):
        """Export benchmark results to CSV format."""
        try:
            import csv
            
            with open(csv_path, 'w', newline='') as csvfile:
                fieldnames = [
                    'benchmark_name', 'benchmark_type', 'timestamp', 'duration',
                    'iterations', 'mean_time', 'median_time', 'std_time',
                    'min_time', 'max_time', 'throughput', 'peak_memory_mb'
                ]
                
                writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
                writer.writeheader()
                
                for result in self.benchmark_results:
                    row = {
                        'benchmark_name': result.benchmark_name,
                        'benchmark_type': result.benchmark_type.value,
                        'timestamp': result.timestamp.isoformat(),
                        'duration': result.duration,
                        'iterations': result.iterations,
                        'mean_time': result.statistics.get('mean_time', ''),
                        'median_time': result.statistics.get('median_time', ''),
                        'std_time': result.statistics.get('std_time', ''),
                        'min_time': result.statistics.get('min_time', ''),
                        'max_time': result.statistics.get('max_time', ''),
                        'throughput': result.statistics.get('throughput', ''),
                        'peak_memory_mb': ''
                    }
                    
                    # Add memory information if available
                    if PerformanceMetric.MEMORY_USAGE in result.metrics:
                        memory_info = result.metrics[PerformanceMetric.MEMORY_USAGE]
                        if isinstance(memory_info, dict):
                            row['peak_memory_mb'] = memory_info.get('peak_mb', '')
                            
                    writer.writerow(row)
                    
        except Exception as e:
            self.logger.warning(f"Failed to export CSV: {e}")
            
    def _generate_performance_visualizations(self, results_by_type: Dict[str, List[BenchmarkResult]]):
        """Generate performance visualization charts."""
        if not MATPLOTLIB_AVAILABLE:
            return
            
        try:
            # Create performance comparison chart
            fig, axes = plt.subplots(2, 2, figsize=(15, 12))
            fig.suptitle('TrustformeRS Python Bindings Performance Analysis', fontsize=16)
            
            # Execution time comparison
            ax1 = axes[0, 0]
            benchmark_names = []
            execution_times = []
            
            for results in results_by_type.values():
                for result in results:
                    benchmark_names.append(result.benchmark_name)
                    execution_times.append(result.statistics.get('mean_time', 0))
                    
            if benchmark_names and execution_times:
                ax1.bar(range(len(benchmark_names)), execution_times)
                ax1.set_title('Average Execution Time by Benchmark')
                ax1.set_ylabel('Time (seconds)')
                ax1.set_xticks(range(len(benchmark_names)))
                ax1.set_xticklabels(benchmark_names, rotation=45, ha='right')
                
            # Memory usage chart
            ax2 = axes[0, 1]
            memory_usage = []
            memory_names = []
            
            for results in results_by_type.values():
                for result in results:
                    if PerformanceMetric.MEMORY_USAGE in result.metrics:
                        memory_info = result.metrics[PerformanceMetric.MEMORY_USAGE]
                        if isinstance(memory_info, dict):
                            memory_usage.append(memory_info.get('peak_mb', 0))
                            memory_names.append(result.benchmark_name)
                            
            if memory_names and memory_usage:
                ax2.bar(range(len(memory_names)), memory_usage)
                ax2.set_title('Peak Memory Usage by Benchmark')
                ax2.set_ylabel('Memory (MB)')
                ax2.set_xticks(range(len(memory_names)))
                ax2.set_xticklabels(memory_names, rotation=45, ha='right')
                
            # Throughput comparison
            ax3 = axes[1, 0]
            throughput_values = []
            throughput_names = []
            
            for results in results_by_type.values():
                for result in results:
                    throughput = result.statistics.get('throughput', 0)
                    if throughput > 0:
                        throughput_values.append(throughput)
                        throughput_names.append(result.benchmark_name)
                        
            if throughput_names and throughput_values:
                ax3.bar(range(len(throughput_names)), throughput_values)
                ax3.set_title('Throughput by Benchmark')
                ax3.set_ylabel('Operations/Second')
                ax3.set_xticks(range(len(throughput_names)))
                ax3.set_xticklabels(throughput_names, rotation=45, ha='right')
                
            # Performance distribution
            ax4 = axes[1, 1]
            all_execution_times = []
            for results in results_by_type.values():
                for result in results:
                    if 'execution_times' in result.raw_data:
                        all_execution_times.extend(result.raw_data['execution_times'])
                        
            if all_execution_times:
                ax4.hist(all_execution_times, bins=30, alpha=0.7)
                ax4.set_title('Execution Time Distribution')
                ax4.set_xlabel('Time (seconds)')
                ax4.set_ylabel('Frequency')
                
            plt.tight_layout()
            
            # Save chart
            chart_path = self.output_dir / f"performance_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png"
            plt.savefig(chart_path, dpi=300, bbox_inches='tight')
            plt.close()
            
            self.logger.info(f"Performance visualization saved to {chart_path}")
            
        except Exception as e:
            self.logger.warning(f"Failed to generate visualizations: {e}")

# Convenience functions for easy performance testing
def run_performance_tests(
    optimization_level: OptimizationLevel = OptimizationLevel.PRODUCTION,
    iterations: int = 100,
    enable_profiling: bool = True,
    output_dir: Optional[Path] = None
) -> Dict[str, Any]:
    """
    Run comprehensive performance tests with specified configuration.
    
    Args:
        optimization_level: Optimization level for testing
        iterations: Number of iterations per benchmark
        enable_profiling: Whether to enable detailed profiling
        output_dir: Output directory for results
        
    Returns:
        Performance test results and recommendations
    """
    config = BenchmarkConfig(
        optimization_level=optimization_level,
        iterations=iterations,
        enable_profiling=enable_profiling,
        output_dir=output_dir
    )
    
    tester = PythonBindingsPerformanceTester(config)
    return tester.run_comprehensive_benchmarks()

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="TrustformeRS Python Bindings Performance Testing")
    parser.add_argument("--optimization", choices=["debug", "development", "production", "maximum"],
                       default="production", help="Optimization level")
    parser.add_argument("--iterations", type=int, default=100,
                       help="Number of iterations per benchmark")
    parser.add_argument("--output-dir", type=Path,
                       help="Output directory for results")
    parser.add_argument("--no-profiling", action="store_true",
                       help="Disable detailed profiling")
    parser.add_argument("--no-visualization", action="store_true",
                       help="Disable visualization generation")
    
    args = parser.parse_args()
    
    print(f"üöÄ Starting TrustformeRS Python Bindings Performance Testing")
    print(f"Optimization Level: {args.optimization}")
    print(f"Iterations: {args.iterations}")
    
    try:
        config = BenchmarkConfig(
            optimization_level=OptimizationLevel(args.optimization),
            iterations=args.iterations,
            enable_profiling=not args.no_profiling,
            enable_visualization=not args.no_visualization,
            output_dir=args.output_dir
        )
        
        tester = PythonBindingsPerformanceTester(config)
        results = tester.run_comprehensive_benchmarks()
        
        print(f"\nüéâ Performance Testing Completed!")
        print(f"Total Benchmarks: {results['summary']['total_benchmarks']}")
        print(f"Benchmark Types: {', '.join(results['summary']['benchmark_types'])}")
        print(f"Duration: {results['total_duration']:.2f}s")
        
        print(f"\nüìä Key Recommendations:")
        for i, recommendation in enumerate(results['recommendations'][:5], 1):
            print(f"  {i}. {recommendation}")
            
        if 'report_path' in results:
            print(f"\nüìÑ Detailed report saved to: {results['report_path']}")
            
    except Exception as e:
        print(f"‚ùå Performance testing failed: {e}")
        traceback.print_exc()
        sys.exit(1)