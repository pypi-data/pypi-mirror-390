//! Conversational AI Example
//!
//! This example demonstrates advanced conversational AI capabilities
//! including multi-turn dialogue, memory management, and persona-based interactions.

use std::collections::HashMap;
use std::io::{self, Write};
use tokio::time::{sleep, Duration};
use trustformers::pipeline::conversational::{
    init_conversational_pipeline, ConversationMode, ConversationalConfig, ConversationalInput,
    ConversationalOutput, ConversationalPipeline, MemoryConfig, PersonaConfig, RepairConfig,
    SafetyFilter, StreamingConfig, SummarizationConfig,
};
use trustformers::{pipeline, Result, TextGenerationPipeline, TrustformersError};
use trustformers_core::generation::GenerationConfig;

#[tokio::main]
async fn main() -> Result<()> {
    println!("üí¨ TrustformeRS Conversational AI Examples\n");

    // Basic Conversation Example
    basic_conversation_example().await?;

    // Multi-turn Dialogue Example
    multi_turn_dialogue_example().await?;

    // Persona-based Conversation Example
    persona_conversation_example().await?;

    // Memory and Context Management Example
    memory_management_example().await?;

    // Safety and Content Filtering Example
    safety_filtering_example().await?;

    // Interactive Chat Example (commented out for automated running)
    // interactive_chat_example().await?;

    println!("\n‚úÖ All conversational AI examples completed successfully!");
    Ok(())
}

/// Demonstrate basic conversational capabilities
async fn basic_conversation_example() -> Result<()> {
    println!("üó£Ô∏è  Basic Conversation Example");
    println!("==============================");

    // Create basic conversational configuration
    let config = ConversationalConfig {
        max_history_turns: 10,
        max_context_tokens: 4096,
        enable_summarization: true,
        temperature: 0.7,
        top_p: 0.9,
        top_k: Some(50),
        max_response_tokens: 1000,
        system_prompt: Some("You are a helpful AI assistant.".to_string()),
        enable_safety_filter: true,
        conversation_mode: ConversationMode::Chat,
        enable_persistence: false,
        persona: None,
        summarization_config: SummarizationConfig::default(),
        memory_config: MemoryConfig::default(),
        generation_config: GenerationConfig::default(),
        repair_config: RepairConfig::default(),
        streaming_config: StreamingConfig::default(),
    };

    println!("Configuration:");
    println!("  Mode: {:?}", config.conversation_mode);
    println!("  Temperature: {}", config.temperature);
    println!("  Max history: {} turns", config.max_history_turns);

    // Create conversational pipeline
    let conversation =
        init_conversational_pipeline("microsoft/DialoGPT-medium", config).await?;

    // Simple conversation turns
    let conversation_turns = vec![
        "Hello! How are you today?",
        "What's the weather like?",
        "Can you tell me a joke?",
        "What do you think about artificial intelligence?",
        "Thank you for the conversation!",
    ];

    println!("\nConversation:");
    // Conversation state is managed internally by the pipeline

    for (i, user_input) in conversation_turns.iter().enumerate() {
        println!("User: {}", user_input);

        let input = ConversationalInput {
            message: user_input.to_string(),
            conversation_id: Some("demo_conversation".to_string()),
            context: None,
            config_override: None,
        };

        let response = conversation.process_conversation(input).await?;

        println!("Assistant: {}", response.response);
        println!("Confidence: {:.2}", response.generation_stats.confidence);
        if i < conversation_turns.len() - 1 {
            println!();
        }

        // Add delay for realistic conversation flow
        sleep(Duration::from_millis(500)).await;
    }

    // Show conversation statistics
    // let stats = conversation_state.get_statistics();
    println!("\nConversation Statistics:");
    // println!("  Total turns: {}", stats.total_turns);
    // println!(
    //     "  Average response length: {:.1} words",
    //     stats.avg_response_length
    // );
    // println!("  Total tokens used: {}", stats.total_tokens);
    // println!("  Conversation duration: {:?}", stats.duration);
    println!("  Statistics not available in this example");

    Ok(())
}

/// Demonstrate multi-turn dialogue with context awareness
async fn multi_turn_dialogue_example() -> Result<()> {
    println!("üîÑ Multi-turn Dialogue Example");
    println!("==============================");

    // Configure for context-aware dialogue
    let config = ConversationalConfig {
        max_history_turns: 15, // Longer history for better context
        max_context_tokens: 4096,
        enable_summarization: true,
        temperature: 0.8,
        top_p: 0.9,
        top_k: Some(40),
        max_response_tokens: 1000,
        system_prompt: Some("You are a helpful AI assistant.".to_string()),
        enable_safety_filter: true,
        conversation_mode: ConversationMode::Assistant,
        enable_persistence: false,
        persona: None,
        summarization_config: SummarizationConfig::default(),
        memory_config: MemoryConfig::default(),
        generation_config: GenerationConfig::default(),
        repair_config: RepairConfig::default(),
        streaming_config: StreamingConfig::default(),
    };

    let conversation =
        init_conversational_pipeline("microsoft/DialoGPT-medium", config).await?;

    // Context-dependent conversation
    let dialogue_scenario = vec![
        (
            "I'm planning a trip to Japan next month.",
            "Travel planning context established",
        ),
        (
            "What are some must-see places in Tokyo?",
            "Building on travel context",
        ),
        ("How about food recommendations?", "Continuing travel theme"),
        (
            "I heard the cherry blossoms are beautiful. When's the best time?",
            "Specific seasonal question",
        ),
        (
            "Actually, I'm more interested in winter activities.",
            "Context shift",
        ),
        (
            "What winter festivals happen in Tokyo?",
            "Following new context",
        ),
        (
            "Thanks! By the way, what's the weather usually like in December?",
            "Related follow-up",
        ),
        (
            "Perfect! One last question - any tips for first-time visitors?",
            "Summarizing conversation",
        ),
    ];

    println!("Context-Aware Dialogue Scenario:");
    println!("Topic: Travel Planning\n");

    for (i, (user_message, context_note)) in dialogue_scenario.iter().enumerate() {
        println!("Turn {}: {}", i + 1, user_message);
        println!("Context: {}", context_note);

        let input = ConversationalInput {
            message: user_message.to_string(),
            conversation_id: Some("travel_planning".to_string()),
            context: Some("Topic: travel, Destination: japan".to_string()),
            config_override: None,
        };

        let response = conversation.process_conversation(input).await?;

        // Simulate realistic travel advice responses
        let simulated_responses = vec![
            "That sounds exciting! Japan in winter is wonderful. I'd be happy to help you plan your trip.",
            "For Tokyo, I recommend visiting Shibuya Crossing, Senso-ji Temple, and the Tokyo Skytree. Each offers a unique perspective of the city.",
            "You must try ramen, sushi from Tsukiji Market, and traditional kaiseki cuisine. Tokyo has incredible food diversity!",
            "Cherry blossom season is typically late March to early May, but since you're going in winter, you'll experience a different but equally beautiful Japan.",
            "Winter in Tokyo offers amazing illuminations, hot springs (onsen), and winter sports in nearby mountains like Mount Fuji area.",
            "The Tokyo Illumination festival and Ameya-Yokocho market are wonderful in December. There's also the Meiji Shrine New Year preparations.",
            "December temperatures range from 5-12¬∞C (41-54¬∞F). It's cold but generally clear and sunny. Perfect for walking around!",
            "Definitely learn basic Japanese phrases, get a JR Pass, carry cash, and respect local customs. Also, try convenience store food - it's surprisingly good!",
        ];

        let simulated_response = if let Some(sim_resp) = simulated_responses.get(i) {
            *sim_resp
        } else {
            response.response.as_str()
        };
        println!("Assistant: {}\n", simulated_response);

        // Update conversation state with context (handled internally by pipeline)
        // state.add_turn(user_message.to_string(), simulated_response.to_string());
    }

    // Analyze conversation context
    println!("Context Analysis:");
    // let context_entities = state.extract_entities();
    // println!("  Entities mentioned: {:?}", context_entities);

    // let context_topics = state.get_topics();
    // println!("  Topics discussed: {:?}", context_topics);

    // let conversation_summary = state.generate_summary();
    // println!("  Summary: {}", conversation_summary);
    println!("  Context analysis not available in this example");

    Ok(())
}

/// Demonstrate persona-based conversations
async fn persona_conversation_example() -> Result<()> {
    println!("üé≠ Persona-based Conversation Example");
    println!("=====================================");

    // Define different personas
    let personas = vec![
        PersonaConfig {
            name: "Technical Expert".to_string(),
            personality: "Analytical, detail-oriented senior software engineer".to_string(),
            background: "Senior software engineer with 10+ years experience in ML/AI".to_string(),
            speaking_style: "Technical, precise, uses industry terminology".to_string(),
            expertise: vec![
                "Machine Learning".to_string(),
                "Software Architecture".to_string(),
            ],
            constraints: vec![
                "Provides code examples".to_string(),
                "References best practices".to_string(),
            ],
        },
        PersonaConfig {
            name: "Friendly Assistant".to_string(),
            personality: "Empathetic, patient, and encouraging".to_string(),
            background: "Helpful AI assistant focused on clear, friendly communication".to_string(),
            speaking_style: "Warm, approachable, uses simple language".to_string(),
            expertise: vec!["General Knowledge".to_string(), "Communication".to_string()],
            constraints: vec![
                "Asks clarifying questions".to_string(),
                "Provides encouragement".to_string(),
            ],
        },
        PersonaConfig {
            name: "Academic Researcher".to_string(),
            personality: "Scholarly, thorough, and methodical".to_string(),
            background: "University professor specializing in computational linguistics"
                .to_string(),
            speaking_style: "Formal, cites sources, explains concepts thoroughly".to_string(),
            expertise: vec![
                "Research".to_string(),
                "Linguistics".to_string(),
                "Academia".to_string(),
            ],
            constraints: vec![
                "Cites research papers".to_string(),
                "Provides historical context".to_string(),
            ],
        },
    ];

    let question = "How do transformer models work?";

    println!("Question: {}\n", question);

    for persona in personas {
        println!("--- {} ---", persona.name);
        println!("Background: {}", persona.background);
        println!("Style: {}", persona.speaking_style);

        // Create conversation with persona
        let config = ConversationalConfig {
            max_history_turns: 10,
            max_context_tokens: 4096,
            enable_summarization: true,
            temperature: 0.7,
            top_p: 0.9,
            top_k: Some(50),
            max_response_tokens: 1000,
            system_prompt: Some("You are a helpful AI assistant.".to_string()),
            enable_safety_filter: true,
            conversation_mode: ConversationMode::Assistant,
            enable_persistence: false,
            persona: Some(persona.clone()),
            summarization_config: SummarizationConfig::default(),
            memory_config: MemoryConfig::default(),
            generation_config: GenerationConfig::default(),
            repair_config: RepairConfig::default(),
            streaming_config: StreamingConfig::default(),
        };

        let conversation =
            init_conversational_pipeline("microsoft/DialoGPT-medium", config).await?;

        let input = ConversationalInput {
            message: question.to_string(),
            conversation_id: Some(format!(
                "persona_{}",
                persona.name.replace(" ", "_").to_lowercase()
            )),
            context: None,
            config_override: None,
        };

        // Simulate persona-specific responses
        let response = match persona.name.as_str() {
            "Technical Expert" => {
                "Transformers use self-attention mechanisms with multi-head attention layers. The architecture consists of encoder-decoder stacks with position embeddings. Key components include Query, Key, and Value matrices for attention computation. The self-attention formula is Attention(Q,K,V) = softmax(QK^T/‚àöd_k)V. Implementation typically uses frameworks like PyTorch or TensorFlow."
            },
            "Friendly Assistant" => {
                "Great question! Transformer models are like very sophisticated pattern recognition systems. Think of them as having an amazing ability to pay attention to different parts of text simultaneously. They're what powers many AI applications you use today, like language translation and chatbots. Would you like me to explain any specific part in more detail?"
            },
            "Academic Researcher" => {
                "Transformer architectures, as introduced by Vaswani et al. (2017) in 'Attention Is All You Need,' revolutionized sequence-to-sequence modeling. The model employs multi-head self-attention mechanisms that enable parallel processing, unlike recurrent architectures. Research has shown that transformers exhibit superior performance on various NLP benchmarks (BLEU scores, GLUE tasks). The attention mechanism allows for modeling long-range dependencies more effectively than traditional RNNs or CNNs."
            },
            _ => "Transformers are neural network architectures that use attention mechanisms.",
        };

        println!("Response: {}\n", response);

        // Add persona-specific behavioral notes
        match persona.name.as_str() {
            "Technical Expert" => {
                println!("Behavioral notes:");
                println!("  ‚úì Used technical terminology");
                println!("  ‚úì Provided mathematical formula");
                println!("  ‚úì Mentioned implementation frameworks");
            },
            "Friendly Assistant" => {
                println!("Behavioral notes:");
                println!("  ‚úì Used analogies for explanation");
                println!("  ‚úì Asked follow-up question");
                println!("  ‚úì Maintained encouraging tone");
            },
            "Academic Researcher" => {
                println!("Behavioral notes:");
                println!("  ‚úì Cited original paper");
                println!("  ‚úì Referenced research metrics");
                println!("  ‚úì Provided historical context");
            },
            _ => {},
        }

        println!();
    }

    Ok(())
}

/// Demonstrate memory and context management
async fn memory_management_example() -> Result<()> {
    println!("üß† Memory and Context Management Example");
    println!("=======================================");

    // Configure with enhanced memory
    let config = ConversationalConfig {
        max_history_turns: 20, // Extended history
        max_context_tokens: 4096,
        enable_summarization: true,
        temperature: 0.7,
        top_p: 0.9,
        top_k: Some(50),
        max_response_tokens: 1000,
        system_prompt: Some("You are a helpful AI assistant.".to_string()),
        enable_safety_filter: true,
        conversation_mode: ConversationMode::Assistant,
        enable_persistence: false,
        persona: None,
        summarization_config: SummarizationConfig::default(),
        memory_config: MemoryConfig::default(),
        generation_config: GenerationConfig::default(),
        repair_config: RepairConfig::default(),
        streaming_config: StreamingConfig::default(),
    };

    let conversation =
        init_conversational_pipeline("microsoft/DialoGPT-medium", config).await?;

    // Long conversation to test memory management
    println!("Long conversation scenario (memory test):");

    let extended_dialogue = vec![
        (
            "Hi, my name is Alice and I'm a data scientist.",
            "Introduction",
        ),
        (
            "I work at a tech company in San Francisco.",
            "Personal info",
        ),
        (
            "We're working on a recommendation system for e-commerce.",
            "Work context",
        ),
        (
            "The main challenge is handling sparse data.",
            "Technical problem",
        ),
        (
            "We've tried collaborative filtering but it's not working well.",
            "Previous attempts",
        ),
        (
            "What would you suggest for cold start problems?",
            "Specific question",
        ),
        (
            "That's interesting. How about content-based approaches?",
            "Follow-up",
        ),
        (
            "By the way, do you remember what my profession is?",
            "Memory test 1",
        ),
        ("And what city did I mention I work in?", "Memory test 2"),
        (
            "Going back to our earlier discussion about recommendation systems...",
            "Context recall",
        ),
    ];

    for (i, (message, note)) in extended_dialogue.iter().enumerate() {
        println!("\nTurn {}: {}", i + 1, message);
        println!("Purpose: {}", note);

        let input = ConversationalInput {
            message: message.to_string(),
            conversation_id: Some("memory_test".to_string()),
            context: None,
            config_override: None,
        };

        // Simulate memory-aware responses
        let response = match i {
            0 => "Nice to meet you, Alice! Data science is such an exciting field. What kind of projects are you working on?",
            1 => "San Francisco is a great tech hub! There must be lots of interesting opportunities there.",
            2 => "Recommendation systems are fascinating! E-commerce recommendations can really impact user experience and business metrics.",
            3 => "Sparse data is indeed a common challenge in recommendation systems. It can lead to poor predictions for users with limited interaction history.",
            4 => "Collaborative filtering struggles with sparsity. You might consider hybrid approaches or matrix factorization techniques.",
            5 => "For cold start problems, you could try: 1) Content-based filtering using item features, 2) Knowledge-based recommendations, 3) Popularity-based suggestions for new users, or 4) Active learning to quickly gather preferences.",
            6 => "Content-based approaches work well for cold starts! They can leverage item metadata, user demographics, or even contextual information to make recommendations without requiring interaction history.",
            7 => "Yes, Alice, you mentioned you're a data scientist. You're working on recommendation systems for e-commerce.",
            8 => "You work in San Francisco at a tech company. We've been discussing your recommendation system challenges.",
            9 => "Of course! We were discussing solutions for sparse data and cold start problems in your e-commerce recommendation system. Would you like to explore any specific approach further?",
            _ => "I'm here to help with your questions!",
        };

        println!("Assistant: {}", response);

        // Update memory (handled internally by pipeline)
        // state.add_turn(message.to_string(), response.to_string());

        // Show memory state for key turns
        if i == 2 || i == 7 || i == 9 {
            println!("Memory state:");
            // let memory_items = state.get_memory_items();
            // for item in memory_items.iter().take(3) {
            //     println!("  - {}", item);
            // }
            println!("  Memory tracking not available in this example");
        }
    }

    // Memory analysis
    println!("\nMemory Analysis:");
    // let memory_summary = state.analyze_memory();
    // println!("  Key entities: {:?}", memory_summary.entities);
    // println!("  Important topics: {:?}", memory_summary.topics);
    // println!("  User preferences: {:?}", memory_summary.preferences);
    // println!(
    //     "  Context continuity: {:.1}%",
    //     memory_summary.continuity_score * 100.0
    // );
    println!("  Memory analysis not available in this example");

    Ok(())
}

/// Demonstrate safety and content filtering
async fn safety_filtering_example() -> Result<()> {
    println!("üõ°Ô∏è  Safety and Content Filtering Example");
    println!("========================================");

    // Configure with strict safety filters
    let safety_filter = SafetyFilter::new();

    let config = ConversationalConfig {
        max_history_turns: 10,
        max_context_tokens: 4096,
        enable_summarization: true,
        temperature: 0.7,
        top_p: 0.9,
        top_k: Some(50),
        max_response_tokens: 1000,
        system_prompt: Some("You are a helpful AI assistant.".to_string()),
        enable_safety_filter: true,
        conversation_mode: ConversationMode::Assistant,
        enable_persistence: false,
        persona: None,
        summarization_config: SummarizationConfig::default(),
        memory_config: MemoryConfig::default(),
        generation_config: GenerationConfig::default(),
        repair_config: RepairConfig::default(),
        streaming_config: StreamingConfig::default(),
    };

    println!("Safety Filter Configuration:");
    println!("  Safety filter enabled: {}", config.enable_safety_filter);
    println!("  Max toxicity score: {}", safety_filter.max_toxicity_score);

    let conversation =
        init_conversational_pipeline("microsoft/DialoGPT-medium", config).await?;

    // Test various types of content
    let test_inputs = vec![
        (
            "Hello! Can you help me learn about machine learning?",
            "Safe educational content",
        ),
        (
            "What's my social security number?",
            "Personal information request",
        ),
        (
            "Tell me about the latest advances in AI research.",
            "Safe technical content",
        ),
        (
            "Can you help me with my homework on neural networks?",
            "Educational assistance",
        ),
        (
            "What are some good programming resources?",
            "Safe recommendation request",
        ),
    ];

    println!("\nTesting content safety:");

    for (input_text, category) in test_inputs {
        println!("\nInput: \"{}\"", input_text);
        println!("Category: {}", category);

        let input = ConversationalInput {
            message: input_text.to_string(),
            conversation_id: Some("safety_test".to_string()),
            context: None,
            config_override: None,
        };

        // Simulate safety checking
        let safety_result = check_content_safety(input_text, &safety_filter);

        match safety_result {
            SafetyResult::Safe => {
                println!("Safety check: ‚úÖ SAFE");
                let response = conversation.process_conversation(input).await?;

                // Simulate appropriate responses for safe content
                let safe_response = match category {
                    "Safe educational content" => "I'd be happy to help you learn about machine learning! It's a fascinating field with many applications.",
                    "Safe technical content" => "Recent AI research has made significant advances in areas like transformer architectures, reinforcement learning, and multimodal models.",
                    "Educational assistance" => "I'd be glad to help with your neural networks homework! Neural networks are computational models inspired by biological neurons.",
                    "Safe recommendation request" => "For programming resources, I recommend starting with Python tutorials, then exploring frameworks like TensorFlow or PyTorch for ML.",
                    _ => &response.response,
                };

                println!("Response: {}", safe_response);
            },
            SafetyResult::Filtered(reason) => {
                println!("Safety check: ‚ùå FILTERED");
                println!("Reason: {}", reason);
                println!("Response: I'm sorry, but I can't provide information about personal details for privacy and security reasons. Is there something else I can help you with?");
            },
            SafetyResult::Warning(warning) => {
                println!("Safety check: ‚ö†Ô∏è WARNING");
                println!("Warning: {}", warning);
                println!("Response: I want to be helpful while keeping our conversation appropriate. Let me provide a safe response to your question.");
            },
        }
    }

    // Safety statistics
    println!("\nSafety Statistics:");
    // let safety_stats = state.get_safety_statistics();
    // println!(
    //     "  Total messages processed: {}",
    //     safety_stats.total_processed
    // );
    // println!(
    //     "  Safe messages: {} ({:.1}%)",
    //     safety_stats.safe_count,
    //     safety_stats.safe_count as f64 / safety_stats.total_processed as f64 * 100.0
    // );
    // println!("  Filtered messages: {}", safety_stats.filtered_count);
    // println!("  Warnings issued: {}", safety_stats.warning_count);
    println!("  Safety statistics not available in this example");

    Ok(())
}

/// Interactive chat example (for manual testing)
#[allow(dead_code)]
async fn interactive_chat_example() -> Result<()> {
    println!("üí¨ Interactive Chat Example");
    println!("===========================");
    println!("Type 'quit' to exit the conversation\n");

    let config = ConversationalConfig {
        max_history_turns: 15,
        max_context_tokens: 4096,
        enable_summarization: true,
        temperature: 0.8,
        top_p: 0.9,
        top_k: Some(50),
        max_response_tokens: 1000,
        system_prompt: Some("You are a helpful AI assistant.".to_string()),
        enable_safety_filter: true,
        conversation_mode: ConversationMode::Chat,
        enable_persistence: false,
        persona: None,
        summarization_config: SummarizationConfig::default(),
        memory_config: MemoryConfig::default(),
        generation_config: GenerationConfig::default(),
        repair_config: RepairConfig::default(),
        streaming_config: StreamingConfig::default(),
    };

    let conversation =
        init_conversational_pipeline("microsoft/DialoGPT-medium", config).await?;

    loop {
        print!("You: ");
        io::stdout().flush().unwrap();

        let mut user_input = String::new();
        io::stdin().read_line(&mut user_input).unwrap();
        let user_input = user_input.trim();

        if user_input.eq_ignore_ascii_case("quit") {
            println!("Goodbye! Thanks for chatting!");
            break;
        }

        if user_input.is_empty() {
            continue;
        }

        let input = ConversationalInput {
            message: user_input.to_string(),
            conversation_id: Some("interactive_chat".to_string()),
            context: None,
            config_override: None,
        };

        match conversation.process_conversation(input).await {
            Ok(response) => {
                println!("Assistant: {}", response.response);
                if response.generation_stats.confidence < 0.7 {
                    println!(
                        "(Note: Response confidence is low: {:.2})",
                        response.generation_stats.confidence
                    );
                }
            },
            Err(e) => {
                println!("Error: {}", e);
            },
        }

        println!();
    }

    Ok(())
}

/// Utility types and functions for conversational examples

#[derive(Debug)]
pub enum SafetyResult {
    Safe,
    Filtered(String),
    Warning(String),
}

#[derive(Debug)]
pub struct MemorySummary {
    pub entities: Vec<String>,
    pub topics: Vec<String>,
    pub preferences: Vec<String>,
    pub continuity_score: f64,
}

#[derive(Debug)]
pub struct SafetyStatistics {
    pub total_processed: usize,
    pub safe_count: usize,
    pub filtered_count: usize,
    pub warning_count: usize,
}

#[derive(Debug)]
pub struct ConversationStatistics {
    pub total_turns: usize,
    pub avg_response_length: f64,
    pub total_tokens: usize,
    pub duration: Duration,
}

fn check_content_safety(text: &str, filter: &SafetyFilter) -> SafetyResult {
    // Simulate safety checking
    let text_lower = text.to_lowercase();

    // Check for personal information requests
    if text_lower.contains("social security")
        || text_lower.contains("credit card")
        || text_lower.contains("password")
        || text_lower.contains("ssn")
    {
        return SafetyResult::Filtered("Personal information request detected".to_string());
    }

    // Check for blocked topics
    for banned_term in &filter.banned_terms {
        if text_lower.contains(banned_term) {
            return SafetyResult::Filtered(format!(
                "Content contains banned term: {}",
                banned_term
            ));
        }
    }

    // Check for potentially sensitive content
    if text_lower.contains("hack") || text_lower.contains("illegal") {
        return SafetyResult::Warning("Potentially sensitive topic detected".to_string());
    }

    SafetyResult::Safe
}
