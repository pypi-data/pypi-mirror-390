{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fec470bf",
   "metadata": {},
   "source": [
    "\n",
    "## Implement the Continuous Bag of Words (CBOW) Model for the given (textual document) using the below steps:\n",
    "    a. Data preparation\n",
    "    b. Generate training data\n",
    "    c. Train model\n",
    "    d. Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc9467d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data =\"\"\"But I must explain to you how all this mistaken idea of denouncing pleasure and praising pain \n",
    "was born and I will give you a complete account of the system, and expound the actual teachings \n",
    "of the great explorer of the truth, the master-builder of human happiness. No one rejects, \n",
    "dislikes, or avoids pleasure itself, because it is pleasure, but because those who do not know \n",
    "how to pursue pleasure rationally encounter consequences that are extremely painful. Nor again \n",
    "is there anyone who loves or pursues or desires to obtain pain of itself, because it is pain, \n",
    "but because occasionally circumstances occur in which toil and pain can procure him some great \n",
    "pleasure. To take a trivial example, which of us ever undertakes laborious physical exercise, \n",
    "except to obtain some advantage from it? But who has any right to find fault with a man who \n",
    "chooses to enjoy a pleasure that has no annoying consequences, or one who avoids a pain that \n",
    "produces no resultant pleasure?\"\"\"\n",
    "\n",
    "data = data.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb9e67e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(data)\n",
    "\n",
    "word2id = tokenizer.word_index\n",
    "word2id['PAD'] = 0\n",
    "\n",
    "id2word = {v:k for k,v in word2id.items()}\n",
    "wids = tokenizer.texts_to_sequences(data)\n",
    "\n",
    "emb_size = 100\n",
    "window_size = 2\n",
    "vocab_size = len(word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "63df0353",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bca027e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cbow_model(corpus,vocab_size, window_size):\n",
    "    context_length = window_size*2\n",
    "    for words in corpus:\n",
    "        sequences_size = len(words)\n",
    "        for index,word in enumerate(words):\n",
    "            context_word = []\n",
    "            label_word = []\n",
    "            start = index - window_size\n",
    "            end = index + window_size + 1\n",
    "            context_word.append([words[i]\n",
    "                               for i in range(start,end)\n",
    "                               if 0<=i <sequences_size\n",
    "                               and i!=index])\n",
    "            label_word.append(word)\n",
    "            \n",
    "            x = pad_sequences(context_word,context_length)\n",
    "            y = to_categorical(label_word,vocab_size)\n",
    "            yield(x,y)\n",
    "            \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f3947598",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Embedding,Lambda\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bce96afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_3 (Embedding)     (None, 4, 100)            10200     \n",
      "                                                                 \n",
      " lambda_3 (Lambda)           (None, 100)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 102)               10302     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20502 (80.09 KB)\n",
      "Trainable params: 20502 (80.09 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cbow = Sequential([\n",
    "    Embedding(vocab_size,emb_size,input_length = window_size*2),\n",
    "    Lambda(lambda x:K.mean(x,axis=1)),\n",
    "    Dense(vocab_size,activation = 'softmax')\n",
    "])\n",
    "\n",
    "cbow.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "cbow.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a4d5e66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-09 23:06:45.484826: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs 0 - Loss -> 776.0845975875854\n",
      "Epochs 1 - Loss -> 762.3211803436279\n",
      "Epochs 2 - Loss -> 751.9022114276886\n",
      "Epochs 3 - Loss -> 743.8112494945526\n",
      "Epochs 4 - Loss -> 739.9156000614166\n"
     ]
    }
   ],
   "source": [
    "for epochs in range(5):\n",
    "    loss  = 0\n",
    "    for x,y in cbow_model(corpus=wids,vocab_size = vocab_size,window_size=window_size):\n",
    "        loss += cbow.train_on_batch(x,y)\n",
    "    print(\"Epochs {} - Loss -> {}\".format(epochs,loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f7b2333a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "weights = cbow.get_weights()[0][:]\n",
    "# pd.DataFrame(weights,index=word2id.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fa517f7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>to</th>\n",
       "      <th>of</th>\n",
       "      <th>pleasure</th>\n",
       "      <th>pain</th>\n",
       "      <th>a</th>\n",
       "      <th>the</th>\n",
       "      <th>who</th>\n",
       "      <th>but</th>\n",
       "      <th>and</th>\n",
       "      <th>or</th>\n",
       "      <th>...</th>\n",
       "      <th>find</th>\n",
       "      <th>fault</th>\n",
       "      <th>with</th>\n",
       "      <th>man</th>\n",
       "      <th>chooses</th>\n",
       "      <th>enjoy</th>\n",
       "      <th>annoying</th>\n",
       "      <th>produces</th>\n",
       "      <th>resultant</th>\n",
       "      <th>PAD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.807442</td>\n",
       "      <td>0.771928</td>\n",
       "      <td>0.793368</td>\n",
       "      <td>0.810595</td>\n",
       "      <td>0.788211</td>\n",
       "      <td>0.822126</td>\n",
       "      <td>0.831010</td>\n",
       "      <td>0.783233</td>\n",
       "      <td>0.807467</td>\n",
       "      <td>...</td>\n",
       "      <td>0.786745</td>\n",
       "      <td>0.806996</td>\n",
       "      <td>0.813065</td>\n",
       "      <td>0.825786</td>\n",
       "      <td>0.842597</td>\n",
       "      <td>0.795304</td>\n",
       "      <td>0.831060</td>\n",
       "      <td>0.838407</td>\n",
       "      <td>0.821537</td>\n",
       "      <td>0.812710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>0.807442</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.371872</td>\n",
       "      <td>0.384705</td>\n",
       "      <td>0.397729</td>\n",
       "      <td>0.372196</td>\n",
       "      <td>0.430555</td>\n",
       "      <td>0.394984</td>\n",
       "      <td>0.383982</td>\n",
       "      <td>0.382689</td>\n",
       "      <td>...</td>\n",
       "      <td>0.423424</td>\n",
       "      <td>0.413547</td>\n",
       "      <td>0.416344</td>\n",
       "      <td>0.426392</td>\n",
       "      <td>0.404667</td>\n",
       "      <td>0.397039</td>\n",
       "      <td>0.404023</td>\n",
       "      <td>0.383512</td>\n",
       "      <td>0.406284</td>\n",
       "      <td>0.354068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pleasure</th>\n",
       "      <td>0.771928</td>\n",
       "      <td>0.371872</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.402437</td>\n",
       "      <td>0.453952</td>\n",
       "      <td>0.387958</td>\n",
       "      <td>0.434515</td>\n",
       "      <td>0.397301</td>\n",
       "      <td>0.405038</td>\n",
       "      <td>0.425939</td>\n",
       "      <td>...</td>\n",
       "      <td>0.415794</td>\n",
       "      <td>0.401628</td>\n",
       "      <td>0.429888</td>\n",
       "      <td>0.423150</td>\n",
       "      <td>0.434478</td>\n",
       "      <td>0.390972</td>\n",
       "      <td>0.412803</td>\n",
       "      <td>0.426508</td>\n",
       "      <td>0.434454</td>\n",
       "      <td>0.375738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pain</th>\n",
       "      <td>0.793368</td>\n",
       "      <td>0.384705</td>\n",
       "      <td>0.402437</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.386233</td>\n",
       "      <td>0.423150</td>\n",
       "      <td>0.408687</td>\n",
       "      <td>0.392622</td>\n",
       "      <td>0.404882</td>\n",
       "      <td>0.423993</td>\n",
       "      <td>...</td>\n",
       "      <td>0.431200</td>\n",
       "      <td>0.418583</td>\n",
       "      <td>0.365058</td>\n",
       "      <td>0.427527</td>\n",
       "      <td>0.366061</td>\n",
       "      <td>0.387408</td>\n",
       "      <td>0.389884</td>\n",
       "      <td>0.429357</td>\n",
       "      <td>0.409372</td>\n",
       "      <td>0.390907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>0.810595</td>\n",
       "      <td>0.397729</td>\n",
       "      <td>0.453952</td>\n",
       "      <td>0.386233</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.436693</td>\n",
       "      <td>0.389560</td>\n",
       "      <td>0.419061</td>\n",
       "      <td>0.371066</td>\n",
       "      <td>0.443719</td>\n",
       "      <td>...</td>\n",
       "      <td>0.454170</td>\n",
       "      <td>0.409596</td>\n",
       "      <td>0.373956</td>\n",
       "      <td>0.427422</td>\n",
       "      <td>0.432662</td>\n",
       "      <td>0.402466</td>\n",
       "      <td>0.431611</td>\n",
       "      <td>0.432856</td>\n",
       "      <td>0.438219</td>\n",
       "      <td>0.416665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>enjoy</th>\n",
       "      <td>0.795304</td>\n",
       "      <td>0.397039</td>\n",
       "      <td>0.390972</td>\n",
       "      <td>0.387408</td>\n",
       "      <td>0.402466</td>\n",
       "      <td>0.364225</td>\n",
       "      <td>0.401218</td>\n",
       "      <td>0.375372</td>\n",
       "      <td>0.364863</td>\n",
       "      <td>0.409672</td>\n",
       "      <td>...</td>\n",
       "      <td>0.389308</td>\n",
       "      <td>0.413979</td>\n",
       "      <td>0.406275</td>\n",
       "      <td>0.357450</td>\n",
       "      <td>0.409172</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.374033</td>\n",
       "      <td>0.387794</td>\n",
       "      <td>0.380821</td>\n",
       "      <td>0.408744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>annoying</th>\n",
       "      <td>0.831060</td>\n",
       "      <td>0.404023</td>\n",
       "      <td>0.412803</td>\n",
       "      <td>0.389884</td>\n",
       "      <td>0.431611</td>\n",
       "      <td>0.406568</td>\n",
       "      <td>0.435757</td>\n",
       "      <td>0.343474</td>\n",
       "      <td>0.374915</td>\n",
       "      <td>0.401215</td>\n",
       "      <td>...</td>\n",
       "      <td>0.426050</td>\n",
       "      <td>0.372907</td>\n",
       "      <td>0.388981</td>\n",
       "      <td>0.414288</td>\n",
       "      <td>0.383504</td>\n",
       "      <td>0.374033</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.404567</td>\n",
       "      <td>0.377813</td>\n",
       "      <td>0.385093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>produces</th>\n",
       "      <td>0.838407</td>\n",
       "      <td>0.383512</td>\n",
       "      <td>0.426508</td>\n",
       "      <td>0.429357</td>\n",
       "      <td>0.432856</td>\n",
       "      <td>0.425198</td>\n",
       "      <td>0.429420</td>\n",
       "      <td>0.428291</td>\n",
       "      <td>0.427918</td>\n",
       "      <td>0.449116</td>\n",
       "      <td>...</td>\n",
       "      <td>0.408207</td>\n",
       "      <td>0.436525</td>\n",
       "      <td>0.410676</td>\n",
       "      <td>0.418588</td>\n",
       "      <td>0.401932</td>\n",
       "      <td>0.387794</td>\n",
       "      <td>0.404567</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.417281</td>\n",
       "      <td>0.409681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>resultant</th>\n",
       "      <td>0.821537</td>\n",
       "      <td>0.406284</td>\n",
       "      <td>0.434454</td>\n",
       "      <td>0.409372</td>\n",
       "      <td>0.438219</td>\n",
       "      <td>0.420402</td>\n",
       "      <td>0.417078</td>\n",
       "      <td>0.416733</td>\n",
       "      <td>0.383024</td>\n",
       "      <td>0.390505</td>\n",
       "      <td>...</td>\n",
       "      <td>0.434330</td>\n",
       "      <td>0.432945</td>\n",
       "      <td>0.393465</td>\n",
       "      <td>0.397161</td>\n",
       "      <td>0.407885</td>\n",
       "      <td>0.380821</td>\n",
       "      <td>0.377813</td>\n",
       "      <td>0.417281</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.398061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAD</th>\n",
       "      <td>0.812710</td>\n",
       "      <td>0.354068</td>\n",
       "      <td>0.375738</td>\n",
       "      <td>0.390907</td>\n",
       "      <td>0.416665</td>\n",
       "      <td>0.396628</td>\n",
       "      <td>0.420379</td>\n",
       "      <td>0.406486</td>\n",
       "      <td>0.385547</td>\n",
       "      <td>0.421629</td>\n",
       "      <td>...</td>\n",
       "      <td>0.450954</td>\n",
       "      <td>0.418699</td>\n",
       "      <td>0.388940</td>\n",
       "      <td>0.382814</td>\n",
       "      <td>0.396468</td>\n",
       "      <td>0.408744</td>\n",
       "      <td>0.385093</td>\n",
       "      <td>0.409681</td>\n",
       "      <td>0.398061</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>102 rows Ã— 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 to        of  pleasure      pain         a       the  \\\n",
       "to         0.000000  0.807442  0.771928  0.793368  0.810595  0.788211   \n",
       "of         0.807442  0.000000  0.371872  0.384705  0.397729  0.372196   \n",
       "pleasure   0.771928  0.371872  0.000000  0.402437  0.453952  0.387958   \n",
       "pain       0.793368  0.384705  0.402437  0.000000  0.386233  0.423150   \n",
       "a          0.810595  0.397729  0.453952  0.386233  0.000000  0.436693   \n",
       "...             ...       ...       ...       ...       ...       ...   \n",
       "enjoy      0.795304  0.397039  0.390972  0.387408  0.402466  0.364225   \n",
       "annoying   0.831060  0.404023  0.412803  0.389884  0.431611  0.406568   \n",
       "produces   0.838407  0.383512  0.426508  0.429357  0.432856  0.425198   \n",
       "resultant  0.821537  0.406284  0.434454  0.409372  0.438219  0.420402   \n",
       "PAD        0.812710  0.354068  0.375738  0.390907  0.416665  0.396628   \n",
       "\n",
       "                who       but       and        or  ...      find     fault  \\\n",
       "to         0.822126  0.831010  0.783233  0.807467  ...  0.786745  0.806996   \n",
       "of         0.430555  0.394984  0.383982  0.382689  ...  0.423424  0.413547   \n",
       "pleasure   0.434515  0.397301  0.405038  0.425939  ...  0.415794  0.401628   \n",
       "pain       0.408687  0.392622  0.404882  0.423993  ...  0.431200  0.418583   \n",
       "a          0.389560  0.419061  0.371066  0.443719  ...  0.454170  0.409596   \n",
       "...             ...       ...       ...       ...  ...       ...       ...   \n",
       "enjoy      0.401218  0.375372  0.364863  0.409672  ...  0.389308  0.413979   \n",
       "annoying   0.435757  0.343474  0.374915  0.401215  ...  0.426050  0.372907   \n",
       "produces   0.429420  0.428291  0.427918  0.449116  ...  0.408207  0.436525   \n",
       "resultant  0.417078  0.416733  0.383024  0.390505  ...  0.434330  0.432945   \n",
       "PAD        0.420379  0.406486  0.385547  0.421629  ...  0.450954  0.418699   \n",
       "\n",
       "               with       man   chooses     enjoy  annoying  produces  \\\n",
       "to         0.813065  0.825786  0.842597  0.795304  0.831060  0.838407   \n",
       "of         0.416344  0.426392  0.404667  0.397039  0.404023  0.383512   \n",
       "pleasure   0.429888  0.423150  0.434478  0.390972  0.412803  0.426508   \n",
       "pain       0.365058  0.427527  0.366061  0.387408  0.389884  0.429357   \n",
       "a          0.373956  0.427422  0.432662  0.402466  0.431611  0.432856   \n",
       "...             ...       ...       ...       ...       ...       ...   \n",
       "enjoy      0.406275  0.357450  0.409172  0.000000  0.374033  0.387794   \n",
       "annoying   0.388981  0.414288  0.383504  0.374033  0.000000  0.404567   \n",
       "produces   0.410676  0.418588  0.401932  0.387794  0.404567  0.000000   \n",
       "resultant  0.393465  0.397161  0.407885  0.380821  0.377813  0.417281   \n",
       "PAD        0.388940  0.382814  0.396468  0.408744  0.385093  0.409681   \n",
       "\n",
       "           resultant       PAD  \n",
       "to          0.821537  0.812710  \n",
       "of          0.406284  0.354068  \n",
       "pleasure    0.434454  0.375738  \n",
       "pain        0.409372  0.390907  \n",
       "a           0.438219  0.416665  \n",
       "...              ...       ...  \n",
       "enjoy       0.380821  0.408744  \n",
       "annoying    0.377813  0.385093  \n",
       "produces    0.417281  0.409681  \n",
       "resultant   0.000000  0.398061  \n",
       "PAD         0.398061  0.000000  \n",
       "\n",
       "[102 rows x 102 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "distance_matrix = euclidean_distances(weights)\n",
    "data = pd.DataFrame(distance_matrix,index=word2id.keys())\n",
    "data.columns = word2id.keys()\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0b9b0aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SearchWord(WordList):\n",
    "    similar_words ={}\n",
    "    for search_term in WordList:\n",
    "        if(search_term in word2id.keys()):\n",
    "            similar_words[search_term]=[id2word[idx] for idx in \n",
    "                                        distance_matrix[word2id[search_term]-1].argsort()[0:5]+1] \n",
    "    return similar_words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ec9bbe76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'enjoy': ['enjoy', 'us', 'laborious', 'all', 'teachings']}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SearchWord(['enjoy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4a1640",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70220550",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
