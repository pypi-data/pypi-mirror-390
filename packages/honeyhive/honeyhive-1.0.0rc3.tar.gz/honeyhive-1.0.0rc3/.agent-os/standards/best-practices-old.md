# Development Best Practices - HoneyHive Python SDK

## Development Environment Setup

### Mandatory Code Quality Process

**‚ö†Ô∏è CRITICAL: Install Pre-commit Hooks**
```bash
# One-time setup (required for all developers)
./scripts/setup-dev.sh
```

**Automatic Quality Enforcement** (only runs when relevant files change):
- **Black formatting**: 88-character lines, applied when Python files change
- **Import sorting**: isort with black profile, applied when Python files change
- **Static analysis**: pylint + mypy type checking when Python files change
- **YAML validation**: yamllint with 120-character lines when YAML files change
- **Documentation checks**: Only when docs/Agent OS files change
- **Tox verification**: Scoped to relevant file types for efficiency

**Before Every Commit**:
1. Pre-commit hooks run automatically (DO NOT bypass)
2. Manual verification: `tox -e format && tox -e lint`
3. **MANDATORY**: All tests must pass - `tox -e unit && tox -e integration`
4. **MANDATORY for AI Assistants**: Update documentation before committing
5. **MANDATORY for AI Assistants**: Use correct dates - `date +"%Y-%m-%d"` command


**Documentation Update Requirements**:
- **Code changes**: CHANGELOG.md must be updated
- **New features**: CHANGELOG.md + docs/reference/index.rst + .agent-os/product/features.md
- **üîÑ CI/CD Workflow Changes (MANDATORY)**: When modifying `.github/workflows/*.yml` files:
  - **ALWAYS update** `docs/development/testing/ci-cd-integration.rst` to reflect current workflow behavior
  - **Required sections**: Triggers, path filters, job structure, environment variables, testing strategies
  - **Cross-reference**: Update `docs/development/workflow-optimization.rst` for path-based detection changes
  - **Validation**: Ensure documentation matches actual workflow implementation
- **Mermaid diagrams**: MUST follow `.agent-os/standards/mermaid-diagrams.md` dual-theme standards
- **Large changesets (>3 files)**: Comprehensive documentation review required
- **AI Assistant commits**: Automatic documentation compliance checking via unified validation script
- **üß™ Test-First Documentation Standards (CRITICAL)**: When implementing fixes or changes that affect documentation standards:
  1. **ALWAYS implement and test the solution FIRST**
  2. **VERIFY the fix works** (build docs, test in browser, confirm behavior)  
  3. **ONLY THEN update standards documentation** to reflect the working solution
- **üìö Dual Changelog Sync Requirement (CRITICAL)**: When updating CHANGELOG.md, docs/changelog.rst MUST also be updated:
  - **CHANGELOG.md**: Detailed technical changes for developers
  - **docs/changelog.rst**: Lightweight, curated highlights for users
  - **Different Content Styles**: Maintain distinct purposes while keeping them in sync
  - **Automatic Enforcement**: Pre-commit hooks prevent commits with only one changelog updated
- **Enhanced Quality Gates**: Pre-commit hooks now validate ALL significant changes including:
  - Documentation restructuring (>5 files requires CHANGELOG)
  - Configuration changes (pyproject.toml, tox.ini)
  - Tooling changes (scripts/, .github/workflows/)
  - Agent OS documentation (.agent-os/ files)
  - Examples and integration guides

## üìã Agent OS Specification Standards (MANDATORY)

**üö® CRITICAL**: All Agent OS specifications MUST follow the consistent file structure defined in the [Agent OS documentation](https://buildermethods.com/agent-os).

### Required Spec File Structure

**EVERY Agent OS spec MUST include these files:**

```bash
.agent-os/specs/YYYY-MM-DD-spec-name/
‚îú‚îÄ‚îÄ srd.md              # Spec Requirements Document (MANDATORY)
‚îú‚îÄ‚îÄ specs.md            # Technical Specifications (MANDATORY)  
‚îú‚îÄ‚îÄ tasks.md            # Tasks Breakdown (MANDATORY)
‚îú‚îÄ‚îÄ README.md           # Overview/Quick Start (RECOMMENDED)
‚îî‚îÄ‚îÄ implementation.md   # Implementation Guide (OPTIONAL)
```

### File Content Requirements

#### 1. **srd.md** - Spec Requirements Document
**Purpose**: Goals, user stories, success criteria
**Required Sections**:
- Goals (Primary and Secondary)
- User Stories (As a [role], I want [goal] so that [benefit])
- Success Criteria (Functional, Quality, User Experience)
- Acceptance Criteria (Must Have, Should Have, Could Have)
- Out of Scope
- Risk Assessment
- Dependencies
- Validation Plan

#### 2. **specs.md** - Technical Specifications  
**Purpose**: API design, database changes, UI requirements
**Required Sections**:
- Problem Statement
- Solution Framework
- Requirements (REQ-XXX-001 format)
- Implementation Components (COMP-XXX format)
- Validation Protocol
- Success Criteria
- Quality Gates
- Testing Protocol

#### 3. **tasks.md** - Tasks Breakdown
**Purpose**: Trackable step-by-step implementation plan
**Required Sections**:
- Task Overview
- Individual Tasks (TASK-001, TASK-002, etc.)
- Each task must include:
  - Status (‚úÖ Completed, üîÑ In Progress, ‚è≥ Pending)
  - Objective
  - Scope
  - Acceptance Criteria
  - Implementation Details
  - Validation Commands
  - Test Results

#### 4. **README.md** - Overview (Recommended)
**Purpose**: Quick start and overview for developers
**Recommended Sections**:
- Overview
- Quick Start
- Problem Solved
- Solution Delivered
- Current Status
- Usage Examples
- Validation Commands

#### 5. **implementation.md** - Implementation Guide (Optional)
**Purpose**: Step-by-step implementation details
**Optional Sections**:
- Pre-Implementation Validation
- Implementation Tasks
- Quality Validation Sequence
- Post-Implementation Checklist
- Troubleshooting

### Spec Creation Protocol

**MANDATORY**: When creating new Agent OS specs, AI assistants MUST:

1. **Get Current Date**:
   ```bash
   CURRENT_DATE=$(date +"%Y-%m-%d")
   echo "Today is: $CURRENT_DATE"
   ```

2. **Create Directory with Proper Naming**:
   ```bash
   SPEC_NAME="your-spec-name"
   SPEC_DIR=".agent-os/specs/${CURRENT_DATE}-${SPEC_NAME}"
   mkdir -p "$SPEC_DIR"
   ```

3. **Create ALL Required Files**:
   ```bash
   # Create mandatory files
   touch "$SPEC_DIR/srd.md"
   touch "$SPEC_DIR/specs.md" 
   touch "$SPEC_DIR/tasks.md"
   
   # Create recommended files
   touch "$SPEC_DIR/README.md"
   
   # Create optional files (if needed)
   touch "$SPEC_DIR/implementation.md"
   ```

4. **Use Proper Headers in Each File**:
   ```markdown
   # Spec Name - File Type
   
   **Date**: 2025-09-05
   **Status**: Draft/Active/Completed
   **Priority**: High/Medium/Low
   ```

### Validation Commands

**Before committing any Agent OS spec:**

```bash
# Verify required files exist
SPEC_DIR=".agent-os/specs/2025-09-05-your-spec-name"
test -f "$SPEC_DIR/srd.md" && echo "‚úÖ srd.md exists" || echo "‚ùå srd.md missing"
test -f "$SPEC_DIR/specs.md" && echo "‚úÖ specs.md exists" || echo "‚ùå specs.md missing"
test -f "$SPEC_DIR/tasks.md" && echo "‚úÖ tasks.md exists" || echo "‚ùå tasks.md missing"

# Verify proper date format in directory name
echo "$SPEC_DIR" | grep -E "202[0-9]-[0-1][0-9]-[0-3][0-9]" && echo "‚úÖ Date format correct" || echo "‚ùå Date format wrong"

# Check file headers have required fields
grep -l "Date.*Status.*Priority" "$SPEC_DIR"/*.md | wc -l
```

### Common Violations to Prevent

**‚ùå WRONG**:
- Creating only `README.md` without `srd.md`, `specs.md`, `tasks.md`
- Using wrong date formats (`01-30-2025`, `Jan 30, 2025`)
- Missing required sections in files
- Inconsistent file naming across specs

**‚úÖ CORRECT**:
- Complete file structure with all mandatory files
- ISO date format (`YYYY-MM-DD`)
- All required sections present
- Consistent headers and formatting

### Integration with Agent OS Ecosystem

This spec structure integrates with the [Agent OS three-layer context system](https://buildermethods.com/agent-os):

- **Layer 1 (Standards)**: `.agent-os/standards/` - How you build
- **Layer 2 (Product)**: `.agent-os/product/` - What you're building  
- **Layer 3 (Specs)**: `.agent-os/specs/YYYY-MM-DD-name/` - What to build next

### Enforcement

- **Pre-commit hooks**: Validate spec structure before commits
- **AI Assistant validation**: Check file structure during spec creation
- **Documentation compliance**: Ensure all specs follow this standard
- **Regular audits**: Monthly review of spec consistency

This ensures every Agent OS specification provides complete context for AI assistants and maintains consistency across the entire codebase.

### Required Tools
```bash
# Core development tools
pip install yamllint>=1.37.0  # YAML validation for workflows
brew install gh               # GitHub CLI for workflow investigation

# Verify installation
yamllint --version
gh --version
```

### Tool Usage Patterns
- **yamllint**: Validate GitHub Actions YAML syntax before commits
- **GitHub CLI**: Investigate workflow failures, view run logs, manage releases
- **Docker**: Required for Lambda testing and container validation

## Git Branching Strategy

### Branch Model
**HoneyHive Python SDK follows a simplified branching model:**

- **`main`**: The only protected branch containing production-ready code
- **All other branches**: Temporary working feature branches (deleted after merge)

### Branch Types
```bash
# Feature branches (temporary)
feature/add-anthropic-support
feature/improve-error-handling
bugfix/fix-span-serialization
docs/update-api-reference
refactor/modernize-architecture

# Current working branches (temporary)
complete-refactor    # Major architecture changes
develop             # Legacy branch (will be removed)
```

### Workflow Rules

**‚úÖ DO:**
- Create feature branches from `main`
- Use descriptive branch names: `feature/`, `bugfix/`, `docs/`, `refactor/`
- Open PRs targeting `main` when ready for review
- Delete feature branches after successful merge
- Rebase feature branches to keep history clean

**‚ùå DON'T:**
- Consider any branch other than `main` as permanent
- Create long-lived development branches
- Merge directly to `main` without PR review
- Push directly to `main` (use PRs for all changes)

### CI/CD Trigger Strategy

**GitHub Actions Workflows:**
```yaml
push:
  branches: [main]  # Only run on pushes to the protected main branch
pull_request:
  # Run on ALL PRs - immediate feedback on feature branch work
```

**Rationale:**
- **No duplicates**: Feature branch pushes only trigger via PR workflows
- **Immediate feedback**: All PRs get tested regardless of target branch
- **Gate keeping**: Direct pushes to `main` get validated (though should be rare)
- **Resource efficient**: Single workflow run per feature branch change

### Branch Lifecycle
1. **Create**: `git checkout -b feature/my-feature main`
2. **Develop**: Regular commits with quality checks on every push
3. **Integrate**: Open PR to `main` when ready
4. **Review**: Automated + manual review process
5. **Merge**: Squash merge to `main` with clean commit message
6. **Cleanup**: Delete feature branch immediately after merge

## Architecture Principles

### Multi-Instance Support
- Each tracer instance is independent
- No global singleton pattern
- Thread-safe initialization
- Support for multiple concurrent tracers
- Clear instance lifecycle management

### Separation of Concerns
```python
# Clear layer separation
src/honeyhive/
‚îú‚îÄ‚îÄ api/           # API client layer
‚îú‚îÄ‚îÄ tracer/        # OpenTelemetry integration
‚îú‚îÄ‚îÄ evaluation/    # Evaluation framework
‚îú‚îÄ‚îÄ models/        # Data models
‚îî‚îÄ‚îÄ utils/         # Shared utilities
```

### Dependency Injection
```python
# Pass dependencies explicitly
tracer = HoneyHiveTracer(
    api_key="key",
    project="project",
    instrumentors=[OpenAIInstrumentor()]  # Inject instrumentors
)

# Use factory methods for complex initialization
tracer = HoneyHiveTracer.init(
    api_key="key",
    server_url="https://custom.honeyhive.ai"
)
```

## SDK Design Patterns

### Graceful Degradation
```python
def create_session(self) -> Optional[str]:
    """Create session with graceful failure."""
    try:
        response = self.api.create_session()
        return response.session_id
    except Exception as e:
        if not self.test_mode:
            logger.warning(f"Session creation failed: {e}")
        # Continue without session - don't crash host app
        return None
```

### Decorator Pattern
```python
# Unified decorator for sync/async
@trace(event_type=EventType.model)
def sync_function():
    pass

@trace(event_type=EventType.model)
async def async_function():
    pass

# Class-level decoration
@trace_class
class MyService:
    def method(self):
        pass  # Automatically traced
```

### Context Management
```python
# Use context managers for resource management
with tracer.start_span("operation") as span:
    # Span automatically closed on exit
    result = perform_operation()
    span.set_attribute("result", result)

# Enrich span context manager
with enrich_span(event_type=EventType.tool):
    # Enrichment applied to current span
    process_data()
```

## Error Handling Strategy

### Exception Hierarchy
```python
class HoneyHiveError(Exception):
    """Base exception for all HoneyHive errors."""

class ConfigurationError(HoneyHiveError):
    """Configuration-related errors."""

class APIError(HoneyHiveError):
    """API communication errors."""
    
class RateLimitError(APIError):
    """Rate limit exceeded."""
    
class AuthenticationError(APIError):
    """Authentication failed."""
```

### Retry Logic
```python
@retry(
    max_attempts=3,
    backoff_factor=2.0,
    exceptions=(httpx.TimeoutException, httpx.NetworkError)
)
async def make_api_call():
    """API call with exponential backoff retry."""
    return await client.post(url, json=data)
```

### Error Logging
```python
# Log at appropriate levels
logger.debug("Detailed trace information")
logger.info("Normal operation status")
logger.warning("Recoverable issues")
logger.error("Errors that need attention")
logger.critical("System failures")

# Include context in error messages
logger.error(
    "API call failed",
    extra={
        "url": url,
        "status_code": response.status_code,
        "project": self.project,
        "trace_id": span.get_span_context().trace_id
    }
)
```

## Comprehensive Testing Strategy

### Multi-Layer Testing Approach

**Lessons Learned from ProxyTracerProvider Bug (2025-09-05)**:
- **Root Cause**: Over-mocking in tests prevented detection of real OpenTelemetry behavior
- **Documentation Impact**: 85+ instances of incorrect patterns in integration docs
- **Prevention**: Multi-layer testing with real environment validation

**üö® MANDATORY Testing Layers**:

#### 1. Unit Tests (Fast, Isolated)
```bash
# Run with: tox -e unit
```
**Purpose**: Test individual function logic in isolation
**Characteristics**:
- Heavy mocking for external dependencies
- Fast execution (< 1s each)
- No external API calls
- Isolated component testing

**Use For**:
- Function logic validation
- Error handling paths
- Configuration validation
- Mock-friendly scenarios

#### 2. Integration Tests (Real Components)
```bash
# Run with: tox -e integration
```
**Purpose**: Test component interaction with real dependencies
**Characteristics**:
- Minimal mocking (only external APIs)
- Real OpenTelemetry components
- Real database/cache interactions
- Moderate execution time

**Use For**:
- Component interaction validation
- Real API integration testing
- TracerProvider scenarios
- Multi-instance behavior

#### 3. Real Environment Tests (Subprocess-Based)
```bash
# Run with: tox -e real_env (when implemented)
```
**Purpose**: Test fresh environment scenarios that catch integration bugs
**Characteristics**:
- No mocking whatsoever
- Subprocess execution for fresh Python environments
- Real instrumentor library behavior
- Slower but comprehensive validation

**Use For**:
- Fresh environment scenarios (ProxyTracerProvider detection)
- Instrumentor integration patterns
- Environment-specific bugs
- User experience validation

**Example Real Environment Test**:
```python
def test_fresh_environment_proxy_tracer_provider_bug(self):
    """Test ProxyTracerProvider handling in fresh environment."""
    test_script = '''
    from opentelemetry import trace
    from honeyhive.tracer.otel_tracer import HoneyHiveTracer
    
    # Verify we start with ProxyTracerProvider (bug condition)
    initial_provider = trace.get_tracer_provider()
    assert "Proxy" in type(initial_provider).__name__
    
    # Initialize HoneyHive - should handle ProxyTracerProvider correctly
    tracer = HoneyHiveTracer(api_key="test", project="test")
    
    # Should now have real TracerProvider
    final_provider = trace.get_tracer_provider()
    assert "Proxy" not in type(final_provider).__name__
    '''
    
    # Run in subprocess for fresh environment
    result = subprocess.run([sys.executable, script_path], ...)
    assert result.returncode == 0
```

#### 4. Documentation Example Testing (MANDATORY)
```bash
# Run with: python docs/utils/test-examples.py
```
**Purpose**: Validate all documentation code examples work as written
**Requirements**:
- Every code block in documentation must be tested
- Examples must use current API patterns
- No hardcoded credentials in examples
- Examples must follow documented best practices

### Testing Quality Gates

**üö® CRITICAL: All Must Pass Before Commit**:

1. **Unit Tests**: `tox -e unit` (100% pass rate)
2. **Integration Tests**: `tox -e integration` (100% pass rate)
3. **Linting**: `tox -e lint` (‚â•8.0/10.0 pylint score)
4. **Formatting**: `tox -e format` (100% compliance)
5. **Documentation Build**: `cd docs && make html` (zero warnings)
6. **Example Testing**: All documentation examples executable

**For New Features (Additional Requirements)**:
- Real environment tests for instrumentor features
- Compatibility matrix tests for new providers
- Performance tests if feature affects performance
- Documentation examples must be tested before docs are written

### Test Coverage Requirements

**Minimum Coverage**: 60% overall (currently achieving 73.22%)
**New Code Coverage**: 80% minimum for new features
**Critical Path Coverage**: 95% for core tracer functionality

**Coverage Exclusions**:
- Test files themselves
- Debug/development utilities
- Compatibility shims for deprecated features

### Testing Anti-Patterns to Avoid

**‚ùå Don't Do This**:
```python
# Over-mocking that hides real behavior
@patch("opentelemetry.trace.get_tracer_provider")
def test_tracer_init(mock_provider):
    mock_provider.return_value = Mock()  # Hides ProxyTracerProvider!
```

**‚úÖ Do This Instead**:
```python
# Test with real OpenTelemetry components
def test_tracer_init_real_environment():
    # Use subprocess to test fresh environment
    # OR use real TracerProvider in integration tests
```

### Preventing Documentation Bugs

**üö® MANDATORY Process**:
1. **Write Code First**: Implement the feature completely
2. **Test Code**: Verify it works with real environment tests
3. **Write Documentation**: Only after code is tested and working
4. **Test Documentation**: Validate all examples work as written
5. **Review Integration**: Ensure examples follow best practices

**Documentation Testing Tools**:
```bash
# Validate all code examples
python docs/utils/test-examples.py

# Check for broken links
python docs/utils/audit-content.py

# Verify Divio compliance
python docs/utils/validate-structure.py
```

## Performance Optimization

### Connection Pooling
```python
# Reuse HTTP connections
connection_pool = ConnectionPool(
    max_connections=config.max_connections,
    max_keepalive_connections=config.max_keepalive_connections,
    keepalive_expiry=config.keepalive_expiry
)

# Share client across requests
self._client = httpx.AsyncClient(
    limits=httpx.Limits(
        max_connections=100,
        max_keepalive_connections=20
    )
)
```

### Batching Operations
```python
# Batch span exports
class BatchSpanProcessor:
    def __init__(self, max_batch_size=512, schedule_delay_millis=5000):
        self.batch = []
        self.max_batch_size = max_batch_size
        
    def on_end(self, span):
        self.batch.append(span)
        if len(self.batch) >= self.max_batch_size:
            self._export_batch()
```

### Async Best Practices
```python
# Use async for I/O operations
async def fetch_data(urls: List[str]):
    """Fetch data concurrently."""
    async with httpx.AsyncClient() as client:
        tasks = [client.get(url) for url in urls]
        responses = await asyncio.gather(*tasks)
    return responses

# Don't block the event loop
# Bad: time.sleep(1)
# Good: await asyncio.sleep(1)
```

## Testing Strategy

### Test Organization
```
tests/
‚îú‚îÄ‚îÄ unit/              # Fast, isolated tests
‚îÇ   ‚îú‚îÄ‚îÄ test_api_client.py
‚îÇ   ‚îú‚îÄ‚îÄ test_tracer_decorators.py
‚îÇ   ‚îî‚îÄ‚îÄ test_utils_config.py
‚îú‚îÄ‚îÄ integration/       # API integration tests
‚îÇ   ‚îú‚îÄ‚îÄ test_openai_integration.py
‚îÇ   ‚îî‚îÄ‚îÄ test_langchain_integration.py
‚îú‚îÄ‚îÄ compatibility_matrix/  # Instrumentor compatibility tests
‚îÇ   ‚îú‚îÄ‚îÄ test_openinference_openai.py
‚îÇ   ‚îú‚îÄ‚îÄ test_traceloop_openai.py
‚îÇ   ‚îî‚îÄ‚îÄ test_<instrumentor>_<provider>.py
‚îî‚îÄ‚îÄ fixtures/         # Shared test fixtures
```

### Compatibility Matrix Test Naming Pattern

**MANDATORY Pattern**: `test_<instrumentor>_<provider>.py`

**Examples**:
- `test_openinference_openai.py` - OpenInference OpenAI integration
- `test_traceloop_anthropic.py` - Traceloop Anthropic integration
- `test_openinference_bedrock.py` - OpenInference AWS Bedrock integration

**Pattern Rules**:
- `<instrumentor>`: The instrumentor library name (e.g., `openinference`, `traceloop`)
- `<provider>`: The provider name matching documentation (e.g., `openai`, `anthropic`, `bedrock`)
- Provider names must match the corresponding `docs/how-to/integrations/<provider>.rst` file
- Use underscores for multi-word providers (e.g., `azure_openai`, `google_ai`)

### Testing Best Practices
```python
# Always use tox for testing
# tox -e unit       # Unit tests
# tox -e integration # Integration tests
# tox -e py311     # Python 3.11 tests

# Mock external dependencies
@patch('honeyhive.api.client.httpx.AsyncClient')
def test_api_call(mock_client):
    """Test API call with mocked client."""
    mock_client.post.return_value = Mock(json={"success": True})

# Use fixtures for common setup
@pytest.fixture
def configured_tracer():
    """Provide configured tracer for tests."""
    return HoneyHiveTracer(
        api_key="test-key",
        test_mode=True
    )
```

## Testing Standards - MANDATORY FOR ALL COMMITS

**üö® CRITICAL RULE: Zero Failing Tests Policy**

**ALL commits with new features, bug fixes, or code changes MUST have 100% passing tests.**

### Pre-Commit Testing Requirements

**MANDATORY Test Execution**:
```bash
# Required before ANY commit
tox -e unit           # Must pass 100%
tox -e integration    # Must pass 100%
tox -e lint          # Must pass 100%
tox -e format        # Must pass 100%

# For Python version compatibility
tox -e py311 -e py312 -e py313  # All must pass
```

**‚ùå NEVER COMMIT if any tests fail**
**‚ùå NEVER SKIP TESTS** - AI assistants must fix failing tests, never skip them
**‚ùå NEVER use `git commit --no-verify` without immediate follow-up fix**
**‚ùå NEVER push failing tests to any branch (including development branches)**

### ‚ùå PROHIBITED: Test Skipping

**AI assistants are STRICTLY FORBIDDEN from skipping failing tests.**

#### Forbidden Patterns
```python
# ‚ùå FORBIDDEN - Never skip tests
@pytest.mark.skip(reason="Temporarily skipped - will fix later")
def test_broken_feature():
    pass

# ‚ùå FORBIDDEN - Never comment out failing tests  
# def test_that_fails():
#     assert broken_function() == expected

# ‚ùå FORBIDDEN - Never disable in tox.ini
# Removing failing test environments from tox.ini
```

#### Required Approach
```python
# ‚úÖ REQUIRED - Fix the underlying issue
def test_that_works():
    # Proper setup and mocking
    with patch("module.dependency") as mock_dep:
        mock_dep.return_value = expected_response
        result = function_under_test()
        assert result == expected_result
```

#### When Tests Fail: Mandatory Steps
1. **Investigate**: Understand the root cause
2. **Debug**: Use print statements, debugger, or logging
3. **Fix**: Address the underlying issue (code or mock setup)
4. **Validate**: Ensure the fix works and doesn't break other tests
5. **Never Skip**: Skipping creates technical debt and hides problems

### New Feature Requirements

**For ANY new feature or functionality:**

1. **Feature Implementation**: Write the feature code
2. **Test Implementation**: Write comprehensive tests that cover:
   - Happy path scenarios
   - Error conditions
   - Edge cases
   - Backward compatibility (if applicable)
3. **Test Verification**: All tests must pass locally
4. **Integration Check**: Feature must not break existing functionality
5. **Documentation**: Update relevant docs and examples

### Test Coverage Requirements

**üéØ Coverage Targets - Updated Standards:**

- **Project-wide coverage**: **Minimum 80%** (enforced in CI/CD)
- **Individual file coverage**: **Minimum 70%** (strongly recommended)
- **New code**: Minimum 80% coverage required
- **Modified code**: Cannot decrease existing coverage below 70%
- **Critical paths**: 100% coverage required (API clients, decorators, core functionality)

**Coverage Configuration:**
- Project threshold: `--cov-fail-under=80` (pytest.ini, tox.ini)
- Individual file goal: 70% (documented standard, not enforced)
- Configuration location: `pyproject.toml [tool.coverage.*]`

**Current Achievement**: 81.14% project coverage (exceeds 80% requirement ‚úÖ)

### Test Count Reporting - MANDATORY

**üö® CRITICAL: Always Report Total Test Count Correctly**

When reporting test results in commit messages, documentation, or communication:

**REQUIRED FORMAT:**
- **Total Tests**: X tests passing (Y unit + Z integration)
- **Examples**: 
  - ‚úÖ "950 tests passing (831 unit + 119 integration)"
  - ‚úÖ "All 950 tests pass (831 unit + 119 integration)"
  - ‚ùå "All 831 tests passing" (missing integration tests)

**Test Counting Rules:**
1. **Always sum all test types**: unit + integration + any other test categories
2. **Specify breakdown**: Show individual counts for transparency
3. **Use consistent language**: "X tests passing" or "All X tests pass"
4. **Verify accuracy**: Run both test suites to confirm actual counts

**Common Test Types:**
- **Unit tests**: `tox -e unit` (isolated, fast tests)
- **Integration tests**: `tox -e integration` (end-to-end, API tests)
- **Performance tests**: `tox -e performance` (when present)
- **Compatibility tests**: `tox -e compatibility` (when present)

**AI Assistant Requirements:**
- AI assistants MUST calculate and report total test counts correctly by summing all test categories, never report partial counts as totals
- AI assistants MUST NEVER use `git commit --no-verify` or skip pre-commit hooks - always fix formatting/linting issues properly

### Development Branch Testing Policy

**Even on development branches:**
- **No exceptions**: All tests must pass before pushing
- **Work-in-progress**: Use local commits, squash before pushing
- **Feature branches**: Must have passing tests for each logical commit
- **Draft PRs**: Can have failing tests, but must be fixed before review

### CI/CD Integration

**Automated enforcement:**
- GitHub Actions will block merges if tests fail
- Pre-commit hooks prevent local commits with obvious issues
- Tox environments ensure consistent testing across all environments
- Coverage reports must meet minimum thresholds

**Failure Response Protocol:**
1. **Immediate action**: Stop all work on new features
2. **Fix tests**: Address failing tests as highest priority
3. **Root cause analysis**: Understand why tests failed
4. **Prevention**: Update practices to prevent similar failures

### Testing Requirements by Change Type

**Bug Fixes:**
- Must include test that reproduces the bug
- Test must fail before fix, pass after fix
- Regression test must be added to prevent reoccurrence

**New Features:**
- Comprehensive test suite covering all functionality
- Integration tests showing feature works with existing code
- Performance tests if feature affects performance
- Documentation examples must be tested

**Refactoring:**
- All existing tests must continue to pass
- No decrease in test coverage
- May require updating test implementation (not removing tests)

**Documentation Changes:**
- **üö® MANDATORY: No New Documentation Without Testing Code First** - All code examples must be tested before documentation is written or updated
- All code examples in docs must be tested and verified to work
- Examples must use current API patterns and follow documented best practices
- Integration with existing documentation must be verified
- **ProxyTracerProvider Lesson**: The 2025-09-05 ProxyTracerProvider bug was caused by 85+ instances of untested documentation examples teaching incorrect patterns

### Emergency Procedures

**If tests fail after commit:**
1. **Immediate revert**: Revert the failing commit
2. **Fix locally**: Address the test failures
3. **Re-commit**: Only after all tests pass
4. **Post-mortem**: Document what went wrong and how to prevent it

**For critical hotfixes:**
- All testing requirements still apply
- No exceptions for "urgent" fixes

### Instrumentor Integration Requirements - MANDATORY

**üö® ALL NEW INSTRUMENTOR INTEGRATIONS MUST INCLUDE**:

**0. Version Validation**: **MANDATORY Package Version Lookup**
- **ALWAYS look up the latest major version** of the instrumentor package before defining version in spec
- Use `pip index versions openinference-instrumentation-[provider]` or check PyPI directly
- **NEVER use outdated or assumed versions** - always verify current latest version
- Document the version lookup process and date in the specification
- Use the most recent major version available at time of spec creation

**1. Compatibility Matrix Test**: `tests/compatibility_matrix/test_<instrumentor>_<provider>.py`
- Complete integration test with actual provider API calls
- Error handling validation (auth errors, rate limits, network failures)
- Performance benchmarking (latency, throughput metrics)
- Multi-configuration testing (different models, parameters)
- State management verification (for agent-based instrumentors)
- Real API credential testing (with proper environment variable setup)

**2. Integration Documentation**: `docs/how-to/integrations/[provider].rst`
- **MANDATORY**: Use automated template generation system (see `.agent-os/standards/documentation-generation.md`)
- **Command**: `python docs/_templates/generate_provider_docs.py --provider [provider]`
- **Structure**: 4-tab interface (Installation | Basic Setup | Advanced Usage | Troubleshooting)
- **Content**: Quick start, working examples, advanced patterns, error handling
- **Environment**: Configuration in troubleshooting section (item 4)
- **Quality**: All generated content must pass Sphinx build with `-W` flag

**3. Example Implementation**: `examples/[instrumentor]_[provider]_example.py`
- Complete working example demonstrating integration
- Proper error handling and graceful degradation
- Environment variable configuration with clear setup instructions
- Type hints and comprehensive docstrings
- Real-world usage patterns (basic usage, advanced features, multi-step workflows)
- Executable standalone script with proper main() function
- Clear success/failure exit codes and user feedback
- Examples of tracing enrichment and custom span attributes

**4. Multi-Provider Documentation Update**: `docs/how-to/integrations/multi-provider.rst`
- Add new instrumentor to provider comparison table
- Include in multi-provider example code
- Update capability matrix with instrumentor features
- Add to provider selection guidance

**5. Integration Index Update**: `docs/how-to/integrations/index.rst`
- Add to provider list with clear description
- Include in integration overview section
- Update provider comparison table
- Add to recommended provider combinations

**6. Feature Catalog Update**: `.agent-os/product/features.md`
- Add instrumentor to compatibility matrix
- Update provider support table
- Document new capabilities enabled
- Add to roadmap if applicable

**7. Compatibility Matrix Environment Documentation**: `tests/compatibility_matrix/README.md`
- Add all required environment variables for the new instrumentor
- Include provider-specific API key requirements
- Document authentication setup steps
- Update the "Complete Environment Setup" section
- Add troubleshooting guidance for provider-specific issues
- Include provider in the supported providers table with status

**8. Examples Directory Documentation**: `examples/README.md`
- Add new example to the appropriate provider integration section
- Include clear description of what the example demonstrates
- Link to the example file with proper formatting
- Update provider integration examples list
- Ensure example follows the documented patterns and conventions

**üîç Pre-Commit Validation Checklist**:
- [ ] **Latest package version verified** - Used `pip index versions` or PyPI to confirm current major version
- [ ] **Version lookup documented** - Specification includes version verification date and process
- [ ] Compatibility test passes with real API credentials
- [ ] Documentation builds without Sphinx warnings
- [ ] All code examples are executable and tested
- [ ] Integration guide follows Divio documentation standards
- [ ] Multi-provider docs include new instrumentor
- [ ] CHANGELOG.md updated with new integration details
- [ ] Feature catalog reflects new capabilities
- [ ] Example code includes proper error handling
- [ ] Performance benchmarks are reasonable (if applicable)
- [ ] Type hints are complete and accurate
- [ ] Compatibility matrix README updated with environment variables
- [ ] Supported providers table includes new instrumentor
- [ ] Environment setup section covers new provider requirements
- [ ] Troubleshooting section addresses provider-specific issues
- [ ] Examples directory README updated with new example
- [ ] Example file is executable and demonstrates key features
- [ ] Example includes proper error handling and environment setup
- [ ] Documentation navigation validation passes locally
- [ ] All new documentation files included in appropriate toctree
- [ ] Cross-references between documentation sections work correctly
- [ ] Integration with main navigation structure verified

**üìã Quality Gates**:
- **Documentation Quality**: Must pass `tox -e docs` with `-W` flag (warnings as errors)
- **Navigation Validation**: Must pass `python docs/utils/validate_navigation.py --local`
- **Code Quality**: Must pass `tox -e lint` (pylint ‚â•8.0/10.0, mypy clean)
- **Test Coverage**: Compatibility test must achieve ‚â•80% code coverage
- **Example Validation**: All examples must be executable via `python examples/[file].py`
- **Integration Testing**: Must pass in CI/CD environment with mock/test credentials
- **Post-Deployment Validation**: Navigation validation must pass against live site
- **Enhanced Pre-Commit Gates**: Automatic validation of changelog and documentation updates
  - Triggers on ALL significant changes (code, docs, config, tooling, Agent OS files)
  - Enforces changelog updates for major documentation restructuring (>5 files)
  - Validates AI assistant compliance with documentation standards
  - Comprehensive file pattern matching prevents bypassing quality requirements

**‚ùå PROHIBITED**:
- Instrumentor integrations without complete documentation
- Tests that require manual credential setup without environment fallbacks
- Documentation that references non-existent features
- Examples that use hardcoded credentials or non-generic configurations
- Compatibility tests that skip functionality due to "complexity"
- Adding new instrumentors without updating compatibility matrix environment documentation
- Missing environment variable documentation for new provider integrations
- Incomplete provider setup instructions in compatibility matrix README

### ‚ùå PROHIBITED: Invalid Tracer Decorator Patterns - MANDATORY

**üö® CRITICAL**: The `@tracer.trace(...)` decorator pattern is NOT supported in HoneyHive SDK**:

**Invalid Patterns (DO NOT USE)**:
```python
# ‚ùå NEVER USE - This pattern does not exist in HoneyHive SDK
@tracer.trace(event_type=EventType.chain, event_name="agent_benchmark")
def my_function():
    pass

# ‚ùå NEVER USE - HoneyHiveTracer has no trace method for decorators
@my_tracer.trace(event_type=EventType.tool)
def process_data():
    pass
```

**Correct Patterns (USE THESE)**:
```python
# ‚úÖ CORRECT: Standalone trace decorator with explicit tracer
from honeyhive import trace, EventType

@trace(tracer=tracer, event_type=EventType.chain, event_name="agent_benchmark")
def my_function():
    pass

# ‚úÖ CORRECT: Standalone trace decorator with automatic discovery
@trace(event_type=EventType.chain, event_name="agent_benchmark")
def my_function():
    pass

# ‚úÖ CORRECT: Context manager for manual span control
with tracer.start_span("operation_name") as span:
    # Manual span management
    pass
```

**Why This Matters**:
- **API Consistency**: HoneyHiveTracer class has no `trace()` method that returns a decorator
- **Documentation Accuracy**: Prevents confusion and incorrect examples
- **Code Quality**: Ensures all tracing code uses supported patterns
- **Backward Compatibility**: Maintains consistency with both main and complete-refactor branches

**üîç Common Sources of Confusion**:
- **Other SDKs**: Some observability SDKs do provide `@tracer.trace()` patterns
- **OpenTelemetry**: Raw OpenTelemetry has different patterns than HoneyHive
- **Intuitive Assumption**: It seems logical that `@tracer.trace()` would exist alongside `tracer.start_span()`

**üìã Validation Checklist**:
- [ ] All examples use `@trace(tracer=tracer, ...)` or `@trace(...)`
- [ ] No instances of `@tracer.trace(...)` or `@*.trace(...)` in documentation
- [ ] Context manager usage correctly uses `tracer.start_span()` or `tracer.trace()` (context manager form)
- [ ] All trace decorators import from `honeyhive` module: `from honeyhive import trace`
- [ ] Run validation: `grep -r "@.*\.trace(" docs/` should return no results
- [ ] All code examples include proper import statements

### Integration Documentation Navigation - MANDATORY

**üö® ALL INTEGRATION PAGES MUST INCLUDE CONSISTENT "SEE ALSO" NAVIGATION**:

**Required "See Also" Section Template**:
```rst
See Also
--------

- :doc:`multi-provider` - Use [Current Provider] with other providers
- :doc:`../troubleshooting` - Common integration issues  
- :doc:`../../tutorials/03-llm-integration` - LLM integration tutorial
```

**Navigation Principles**:
- **Keep it minimal**: Only 3-4 essential links that users actually need
- **Focus on value**: Multi-provider setup, troubleshooting, and learning path
- **Avoid link spam**: Don't list every other integration (users can find those via index)
- **Be consistent**: Same structure and essential links across all integration pages

**‚ùå PROHIBITED Navigation Patterns**:
- Exhaustive lists of all other integrations (creates maintenance burden)
- Links to niche advanced topics (better discovered organically)  
- Complex automation systems that require maintenance scripts
- Dead or outdated cross-references

**‚úÖ REQUIRED Navigation Links**:
1. **Multi-provider**: Always relevant for users wanting to combine providers
2. **Troubleshooting**: Practical when integration issues arise
3. **Tutorial**: Clear learning path for new users

**üìã Integration Page Validation**:
- [ ] Page has "See Also" section with exactly 3 essential links
- [ ] Multi-provider link uses current provider name in description
- [ ] All links are verified working during docs build
- [ ] Navigation follows consistent rst formatting

### Tutorial Integration Coverage - MANDATORY

**üö® ALL NEW LLM INSTRUMENTORS MUST BE ADDED TO TUTORIAL**:

**Required Tutorial Integration**: `docs/tutorials/03-llm-integration.rst`

**Mandatory Tutorial Section Template**:
```rst
[Provider Name] Integration
---------------------------

[Brief description of provider and use cases]

**Step 1: Install [Provider] Instrumentor**

.. code-block:: bash

   pip install openinference-instrumentation-[provider]

**Step 2: Set Up [Provider] Tracing**

.. code-block:: python

   from honeyhive import HoneyHiveTracer
   from openinference.instrumentation.[provider] import [Provider]Instrumentor
   import [provider-package]
   
   # Initialize with [Provider] instrumentor
   tracer = HoneyHiveTracer.init(
       api_key="your-honeyhive-api-key",
       project="[provider]-tutorial",
       instrumentors=[[Provider]Instrumentor()]
   )
   
   # Configure [Provider]
   [provider-specific-setup]
   
   def example_function(prompt: str) -> str:
       """Example function showing [Provider] usage."""
       
       # Working example with actual provider API
       [provider-api-call]
       
       return result

**Step 3: Test Your Integration**

.. code-block:: python

   # Example call that demonstrates tracing
   result = example_function("Example prompt")
   print(f"Result: {result}")
```

**Tutorial Integration Requirements**:

1. **Placement**: Add section between existing providers and "Advanced: Custom Instrumentor"
2. **Working Example**: Include complete, executable code example
3. **Real API Usage**: Demonstrate actual provider API calls, not pseudocode
4. **Environment Setup**: Show how to configure provider credentials
5. **Clear Naming**: Use `[provider]-tutorial` project naming convention
6. **Practical Use Case**: Choose example that showcases provider's strengths

**üìã Tutorial Validation Checklist**:
- [ ] New provider section added to `docs/tutorials/03-llm-integration.rst`
- [ ] Section follows the standard template structure  
- [ ] Code example is complete and executable
- [ ] Proper imports for both HoneyHive and provider
- [ ] Environment variable setup documented
- [ ] Example demonstrates actual provider API usage
- [ ] Tutorial maintains logical flow between providers
- [ ] Prerequisites updated if new provider has unique requirements

**‚ùå PROHIBITED Tutorial Patterns**:
- Adding instrumentor without tutorial coverage
- Pseudocode examples that aren't executable
- Missing import statements in code examples
- Generic examples that don't showcase provider capabilities
- Incomplete setup instructions for provider credentials

**üìö Documentation Standards**:
- All instrumentor docs must follow the Divio Documentation System (Tutorials, How-to, Reference, Explanation)
- **MANDATORY**: ALL new instrumentor integration HOW-TO docs MUST use formal template system (see `.agent-os/standards/documentation-templates.md`)
- **Template**: Use `docs/_templates/multi_instrumentor_integration_formal_template.rst`
- **Variables**: Define provider-specific values using `docs/_templates/template_variables.md`
- **Scope**: Apply to `docs/how-to/integrations/` only, NOT `docs/tutorials/` (use linear structure for learning)
- **Required tabs**: Installation | Basic Setup | Advanced Usage | Troubleshooting
- **Interactive UI**: Top-level instrumentor selector (OpenInference/OpenLLMetry) with 4 sub-tabs each
- **Content Structure**: All instrumentor-specific content in tabs, general content (comparison, environment, migration) outside tabs
- Code examples must use `EventType` enums, never string literals
- All code examples must be copy-paste ready and tested
- CSS/JavaScript for tabbed interface must be included
- All examples must include proper `from honeyhive.models import EventType` imports
- Error handling patterns must be consistent across all instrumentor documentation
- Performance considerations must be documented for resource-intensive instrumentors

### Documentation Navigation Validation - MANDATORY

**üö® ALL NEW DOCUMENTATION MUST PASS NAVIGATION VALIDATION**:

**Automatic Validation Requirements**:
- All new `.rst` files are automatically discovered and validated
- Navigation validation runs on every documentation deployment
- Broken links, missing pages, and navigation errors block deployment
- Cross-references between documentation sections must be valid

**Pre-Commit Documentation Validation**:
```bash
# Local validation before committing
python docs/utils/validate_navigation.py --local

# Validate against production after deployment
python docs/utils/validate_navigation.py --base-url https://honeyhiveai.github.io/python-sdk/
```

**Mandatory Navigation Requirements**:
1. **Toctree Inclusion**: All new documentation files must be included in appropriate toctree
2. **Cross-Reference Validation**: Internal links must resolve correctly
3. **Navigation Structure**: New sections must integrate with existing navigation
4. **Search Integration**: New content must be searchable via Sphinx search
5. **Mobile Compatibility**: Navigation must work on mobile devices

**üîÑ Self-Updating Validation**:
- Documentation validation automatically discovers new files
- No manual updates to validation lists required
- New instrumentor documentation is automatically included
- Navigation validation adapts as documentation grows

**‚ö†Ô∏è Common Navigation Issues to Prevent**:
- New `.rst` files not added to any toctree
- Broken cross-references after restructuring
- Missing index pages for new sections
- Orphaned documentation files
- Invalid internal link targets
- Missing or broken integration with main navigation

**üîç Validation Scope**:
- **All Documentation Pages**: Every `.rst` file that becomes an `.html` page
- **Navigation Links**: All internal navigation and toctree links
- **Cross-References**: Links between different documentation sections
- **Search Functionality**: Sphinx search index and search page
- **Structural Integrity**: Main navigation, breadcrumbs, and section organization

**üìã Automatic Quality Gates**:
- **Post-Deployment Validation**: MANDATORY validation after every documentation deployment
- **GitHub Actions**: Runs automatically on `workflow_run` completion and `push` to main
- **Pre-commit Hooks**: Local validation prevents broken navigation commits
- **Deployment Blocking**: Failed navigation validation = failed deployment
- **Weekly Monitoring**: Scheduled validation catches deployment drift
- **Manual Triggers**: On-demand validation for specific URL testing

**‚ùå PROHIBITED Navigation Patterns**:
- Adding new documentation without toctree inclusion
- Creating orphaned pages that can't be reached via navigation
- Broken internal links that return 404 errors
- Navigation structures that don't follow Divio documentation system
- Instrumentor documentation that isn't linked from integration index
- Cross-references that break after documentation restructuring
- Fast tracking through expedited review, not skipped testing

## Date and Timestamp Standards - MANDATORY FOR AI ASSISTANTS

**üö® CRITICAL ISSUE**: AI Assistants consistently make date errors that create confusion and misaligned documentation.

### Mandatory Date Usage Protocol

**ALWAYS use the system date command before creating dated content:**

```bash
# REQUIRED: Get current date before ANY date-related work
CURRENT_DATE=$(date +"%Y-%m-%d")
echo "Today is: $CURRENT_DATE"

# Use this variable for all date references
echo "Creating spec for date: $CURRENT_DATE"
```

### Date Format Standards

**Standard Format**: `YYYY-MM-DD` (ISO 8601)
- ‚úÖ **Correct**: `2025-09-03`
- ‚ùå **Wrong**: `2025-01-30` (when today is 2025-09-03)
- ‚ùå **Wrong**: `09/03/2025`, `Sep 3, 2025`, `3-9-2025`

### AI Assistant Date Requirements

#### For New Specifications
```bash
# 1. Get current date
CURRENT_DATE=$(date +"%Y-%m-%d")

# 2. Create directory with current date
mkdir -p ".agent-os/specs/${CURRENT_DATE}-spec-name"

# 3. Use date in file headers
echo "**Date**: $CURRENT_DATE" > spec-file.md
```

#### For File Naming
- **Directories**: `.agent-os/specs/YYYY-MM-DD-spec-name/`
- **Files**: `YYYY-MM-DD-feature-name.md`
- **Logs**: `build-YYYY-MM-DD.log`
- **Releases**: `v1.2.3-YYYY-MM-DD`

#### For Documentation Headers
```markdown
# Specification Title

**Date**: 2025-09-03
**Status**: Active
**Last Updated**: 2025-09-03
**Review Date**: 2025-10-03
```

### Common Date Errors to Prevent

#### Error Pattern 1: Using Random Past Dates
‚ùå **Wrong**:
```bash
mkdir .agent-os/specs/2025-01-30-new-spec  # Created in September!
```

‚úÖ **Correct**:
```bash
CURRENT_DATE=$(date +"%Y-%m-%d")
mkdir ".agent-os/specs/${CURRENT_DATE}-new-spec"
```

#### Error Pattern 2: Hardcoded Dates in Content
‚ùå **Wrong**:
```markdown
**Date**: 2025-01-30  <!-- Hardcoded wrong date -->
```

‚úÖ **Correct**:
```bash
CURRENT_DATE=$(date +"%Y-%m-%d")
echo "**Date**: $CURRENT_DATE" >> spec.md
```

#### Error Pattern 3: Inconsistent Date Formats
‚ùå **Wrong**:
- `January 30, 2025`
- `30-01-2025`
- `1/30/2025`

‚úÖ **Correct**:
- `2025-09-03` (always ISO 8601)

### Date Validation Checklist

**Before creating ANY dated content:**

1. **Get Current Date**: `date +"%Y-%m-%d"`
2. **Verify Output**: Confirm the date makes sense
3. **Use Variable**: Store in variable for consistency
4. **Validate Creation**: Check directory/file names match current date
5. **Review Headers**: Ensure all date headers use current date

### Directory Naming Protocol

**For new specifications:**
```bash
# Template
.agent-os/specs/YYYY-MM-DD-specification-name/

# Example (if today is 2025-09-03)
.agent-os/specs/2025-09-03-new-feature-spec/
.agent-os/specs/2025-09-03-ai-quality-framework/
.agent-os/specs/2025-09-03-testing-standards/
```

**NEVER use old or random dates in new directories!**

### Automated Date Injection

**For AI Assistants - use this template:**

```bash
#!/bin/bash
# Date-aware specification creation template

# Get current date
CURRENT_DATE=$(date +"%Y-%m-%d")
SPEC_NAME="$1"  # First argument is spec name

# Create directory
SPEC_DIR=".agent-os/specs/${CURRENT_DATE}-${SPEC_NAME}"
mkdir -p "$SPEC_DIR"

# Create README with correct date
cat > "$SPEC_DIR/README.md" << EOF
# Specification: $SPEC_NAME

**Date**: $CURRENT_DATE
**Status**: Draft
**Last Updated**: $CURRENT_DATE

## Overview
[Specification content here]
EOF

echo "Created specification: $SPEC_DIR"
echo "Date used: $CURRENT_DATE"
```

### Date Review and Maintenance

#### Weekly Reviews
- **Audit existing specs**: Check for date inconsistencies
- **Update "Last Updated"**: Refresh modified specifications
- **Archive old specs**: Move outdated specs to archive directory

#### Monthly Reviews
- **Validate date patterns**: Ensure consistency across all files
- **Update review dates**: Extend review cycles for stable specs
- **Clean up directories**: Remove any incorrectly dated directories

### Emergency Date Correction Protocol

**If wrong dates are discovered:**

1. **Stop all work**: Halt current development
2. **Identify scope**: Find all affected files/directories
3. **Create fix plan**: Plan correction strategy
4. **Execute corrections**: Rename directories, update headers
5. **Validate fixes**: Ensure all dates are now correct
6. **Document lessons**: Update this protocol if needed

### Date Quality Metrics

**Track these metrics to prevent date errors:**
- **Specification Date Accuracy**: % of specs with correct creation dates
- **Directory Naming Consistency**: % of directories following date standards
- **Header Date Validity**: % of files with accurate date headers
- **Review Date Compliance**: % of specs with up-to-date review dates

### Enforcement Mechanisms

#### Pre-commit Hooks
```bash
# Add to pre-commit validation
check_dates() {
    # Validate new spec directories use current date
    CURRENT_DATE=$(date +"%Y-%m-%d")
    
    # Check for directories created today
    NEW_DIRS=$(git diff --cached --name-only | grep "\.agent-os/specs/" | head -1)
    if [[ $NEW_DIRS == *"specs/"* ]] && [[ $NEW_DIRS != *"$CURRENT_DATE"* ]]; then
        echo "ERROR: New spec directory must use current date: $CURRENT_DATE"
        exit 1
    fi
}
```

#### CI/CD Validation
```yaml
# GitHub Actions date validation
- name: Validate Specification Dates
  run: |
    CURRENT_DATE=$(date +"%Y-%m-%d")
    # Check for any new specs with wrong dates
    NEW_SPECS=$(git diff --name-only HEAD~1 HEAD | grep "\.agent-os/specs/")
    for spec in $NEW_SPECS; do
        if [[ $spec == *"specs/"* ]] && [[ $spec != *"$CURRENT_DATE"* ]]; then
            echo "ERROR: Specification uses wrong date: $spec"
            echo "Expected date: $CURRENT_DATE"
            exit 1
        fi
    done
```

## Commit Message Standards - MANDATORY

**üö® CRITICAL ISSUE**: AI Assistants consistently make commit message formatting errors.

### Mandatory Commit Message Protocol

**ALWAYS follow Conventional Commits format:**
- **Title**: `<type>: <description>` (max 50 chars)
- **NO unnecessary quotes**: Don't wrap titles in quotes
- **Match quotes properly**: If you start a quote, close it
- **Types**: feat, fix, docs, style, refactor, perf, test, build, ci, chore

### Common Errors to Prevent

```bash
# ‚ùå WRONG - Missing closing quote
git commit -m "feat: Add feature

# ‚ùå WRONG - Unnecessary quotes
git commit -m "\"feat: Add feature\""

# ‚ùå WRONG - Too long (71 chars)
git commit -m "feat: Add comprehensive documentation quality control system validation"

# ‚úÖ CORRECT
git commit -m "feat: Add documentation quality control"
```

### Validation Requirements

**Before EVERY commit:**
1. Check title length ‚â§ 50 characters
2. Verify conventional commit format
3. Ensure proper quote matching
4. No periods at end of title

## Security Practices

### API Key Management
```python
# Never log API keys
def __init__(self, api_key: str):
    self.api_key = api_key
    logger.info("Client initialized")  # Don't log the key!

# Validate API key format
if not api_key or not api_key.startswith("hh_"):
    raise ValueError("Invalid API key format")

# Support key rotation
def rotate_api_key(self, new_key: str):
    """Update API key without restart."""
    self.api_key = new_key
    self._reinitialize_client()
```

### Data Privacy
```python
# Redact sensitive data
def redact_pii(data: Dict[str, Any]) -> Dict[str, Any]:
    """Redact PII from data."""
    sensitive_keys = ["ssn", "email", "phone", "credit_card"]
    return {
        k: "***REDACTED***" if k in sensitive_keys else v
        for k, v in data.items()
    }

# Configurable data filtering
if config.redact_inputs:
    inputs = redact_pii(inputs)
```

## Configuration Management

### Environment Variable Patterns
```python
# Support multiple prefixes for compatibility
api_key = (
    os.getenv("HH_API_KEY") or
    os.getenv("HONEYHIVE_API_KEY") or
    os.getenv("API_KEY")
)

# Configuration precedence
# 1. Constructor parameters (highest)
# 2. HH_* environment variables
# 3. Standard environment variables
# 4. Default values (lowest)
```

### Configuration Validation
```python
class Config:
    def __init__(self):
        self.api_key = self._validate_api_key()
        self.timeout = self._validate_timeout()
        
    def _validate_timeout(self) -> float:
        """Validate and parse timeout value."""
        timeout = os.getenv("HH_TIMEOUT", "30.0")
        try:
            value = float(timeout)
            if value <= 0:
                raise ValueError("Timeout must be positive")
            return value
        except (ValueError, TypeError):
            logger.warning(f"Invalid timeout: {timeout}, using default")
            return 30.0
```

## Documentation Requirements

### Code Documentation
- Every module needs a docstring
- Every public function needs a docstring
- Complex logic requires inline comments
- Include usage examples in docstrings

### User Documentation - Divio System

**üéØ Following the [Divio Documentation System](https://docs.divio.com/documentation-system/)**

The HoneyHive SDK documentation is organized into four distinct types, each serving different user needs:

#### 1. TUTORIALS (Learning-oriented)
**Purpose**: Help newcomers get started and achieve early success
**User mindset**: "I want to learn by doing"

**Structure**:
```
tutorials/
‚îú‚îÄ‚îÄ 01-quick-start.rst          # 5-minute setup
‚îú‚îÄ‚îÄ 02-basic-tracing.rst        # First traces with @trace decorator
‚îú‚îÄ‚îÄ 03-llm-integration.rst      # OpenAI/Anthropic integration
‚îú‚îÄ‚îÄ 04-evaluation-basics.rst    # First evaluation
‚îî‚îÄ‚îÄ 05-dashboard-tour.rst       # Understanding HoneyHive UI
```

**Content Requirements**:
- Step-by-step instructions
- Clear learning objectives
- Working code examples
- Expected outcomes at each step
- Maximum 15-20 minutes per tutorial
- Test with actual beginners

#### 2. HOW-TO GUIDES (Problem-oriented)
**Purpose**: Solve specific real-world problems
**User mindset**: "I want to solve this specific problem"

**Structure**:
```
how-to/
‚îú‚îÄ‚îÄ troubleshooting.rst         # Common issues and solutions
‚îú‚îÄ‚îÄ deployment/                 # Production deployment guides
‚îú‚îÄ‚îÄ integrations/              # LLM provider integrations
‚îú‚îÄ‚îÄ advanced-tracing/          # Complex tracing scenarios
‚îú‚îÄ‚îÄ evaluation/                # Evaluation workflows
‚îú‚îÄ‚îÄ testing/                   # Testing strategies
‚îî‚îÄ‚îÄ monitoring/                # Performance and error tracking
```

**Content Requirements**:
- Problem-focused titles
- Minimal background explanation
- Clear steps to solution
- Multiple approaches when applicable
- Prerequisites clearly stated
- Links to reference docs

#### 3. REFERENCE (Information-oriented)
**Purpose**: Provide comprehensive technical specifications
**User mindset**: "I need to look up exact details"

**Structure**:
```
reference/
‚îú‚îÄ‚îÄ api/                       # Complete API documentation
‚îú‚îÄ‚îÄ configuration/             # All configuration options
‚îú‚îÄ‚îÄ data-models/              # Data structure specifications
‚îú‚îÄ‚îÄ cli/                      # CLI command reference
‚îî‚îÄ‚îÄ evaluation/               # Evaluator specifications
```

**Content Requirements**:
- Complete API coverage
- Accurate parameter descriptions
- Return value specifications
- Error condition documentation
- Code examples for each function
- Cross-references between related items

#### 4. EXPLANATION (Understanding-oriented)
**Purpose**: Provide context, background, and design decisions
**User mindset**: "I want to understand how this works and why"

**Structure**:
```
explanation/
‚îú‚îÄ‚îÄ architecture/              # SDK design and architecture
‚îú‚îÄ‚îÄ concepts/                  # Core concepts and terminology
‚îú‚îÄ‚îÄ decisions/                 # Design decision rationale
‚îî‚îÄ‚îÄ comparisons/              # Comparisons with alternatives
```

**Content Requirements**:
- Conceptual explanations
- Design decision rationale
- Architecture overviews
- Historical context when relevant
- Comparison with alternatives
- Future direction insights

### Documentation Best Practices

#### Type Safety and Code Examples

**MANDATORY: Proper Type Usage in All Documentation**

All code examples in documentation MUST follow strict type safety guidelines:

```python
# ‚úÖ CORRECT: Proper enum imports and usage
from honeyhive import HoneyHiveTracer, trace, atrace
from honeyhive.models import EventType

@trace(event_type=EventType.model)  # Type-safe enum value
def llm_function():
    """Process LLM requests."""
    pass

@trace(event_type=EventType.tool)   # Individual function/utility
def utility_function():
    """Process individual data operations."""
    pass

@trace(event_type=EventType.chain)  # Multi-step workflow
def workflow_function():
    """Orchestrate multiple operations."""
    pass

# ‚ùå INCORRECT: String literals (deprecated, breaks type safety)
@trace(event_type="model")  # Never use string literals
def bad_function():
    pass
```

**EventType Semantic Guidelines**:
- **EventType.model**: LLM calls, AI model inference, generation
- **EventType.tool**: Individual functions, utilities, data processing, validation
- **EventType.chain**: Workflows, multi-step processes, business logic orchestration  
- **EventType.session**: High-level sessions, complete user interactions

**Validation Requirements**:
1. ‚úÖ **Import Validation**: Every code example includes correct imports
2. ‚úÖ **Type Checking**: All examples pass mypy validation
3. ‚úÖ **Enum Usage**: No string literals for enum values anywhere
4. ‚úÖ **Import Order**: honeyhive imports first, then models
5. ‚úÖ **Consistency**: Same patterns across tutorials, how-to, reference
6. ‚úÖ **Semantic Correctness**: EventType matches actual function purpose

**AI Assistant Requirements**:
When updating documentation, AI assistants MUST:
- Validate all imports are correct and complete
- Replace string literals with proper enum values
- Test code examples for syntax correctness
- Follow EventType semantic mapping guidelines
- Update import statements when adding enum usage
- Maintain consistency across all files

#### Documentation Error Prevention Protocol

**MANDATORY: Pre-generation validation checklist**

1. ‚úÖ **RST Structure Validation**:
   - Title underlines MUST match title length exactly
   - Blank lines MUST separate sections and headers
   - Code blocks MUST have proper indentation (3 spaces)
   - Tables MUST use consistent column formatting

2. ‚úÖ **Type Safety Enforcement**:
   - NO string literals in `event_type` parameters
   - ALL `@trace` decorators MUST use `EventType` enums
   - Complete import statements MUST be included
   - Import validation MUST pass before generation

3. ‚úÖ **Code Example Integrity**:
   - Python syntax MUST be valid (AST parseable)
   - All imports MUST resolve correctly
   - Examples MUST follow project standards
   - No orphaned code fragments allowed

4. ‚úÖ **Structural Compliance**:
   - All files MUST be included in toctrees
   - Cross-references MUST resolve correctly
   - No broken internal links allowed
   - Section hierarchy MUST be logical

**Error Prevention Tools** (See `.agent-os/specs/2025-09-03-documentation-quality-prevention/`):
- Pre-commit validation hooks
- Automated RST quality checking
- Type safety enforcement
- Code example testing
- Structural integrity verification

#### Content Creation Guidelines
```python
# Every tutorial should follow this pattern:
"""
1. Clear objective statement
2. Prerequisites list
3. Step-by-step instructions
4. Code examples with explanations
5. Expected results
6. Next steps recommendations
7. Troubleshooting section
"""

# How-to guides should be problem-focused:
"""
Title: "How to trace custom LLM providers"
Not: "Custom LLM provider documentation"

Structure:
- Problem statement
- Solution overview
- Step-by-step implementation
- Verification steps
- Common pitfalls
"""
```

#### Cross-linking Strategy
- Tutorials link to relevant how-to guides
- How-to guides reference specific API docs
- Reference docs link to conceptual explanations
- Explanations provide context for tutorials

#### Content Maintenance
```bash
# Regular content audits
docs/utils/audit-content.py      # Check for broken links
docs/utils/test-examples.py      # Verify all code examples work
docs/utils/validate-structure.py # Ensure Divio compliance
```

#### User Testing Protocol
1. **Tutorial Testing**: Test with 3+ new users monthly
2. **How-to Validation**: Verify solutions work in real scenarios
3. **Reference Accuracy**: Automated testing of API examples
4. **Explanation Clarity**: Expert review for technical accuracy

### Maintenance Documentation
- Architecture decisions (in `explanation/decisions/`)
- Design patterns used (in `explanation/architecture/`)
- Performance considerations (in `explanation/concepts/`)
- Security implications (in `how-to/deployment/`)
- Known limitations (in `reference/` sections)

## Release Process

### Version Management
```
# Semantic Versioning: MAJOR.MINOR.PATCH
0.1.0 - Initial beta release
0.1.1 - Bug fixes
0.2.0 - New features (backwards compatible)
1.0.0 - First stable release
2.0.0 - Breaking changes
```

### Release Checklist
- [ ] Update version in pyproject.toml
- [ ] Update CHANGELOG.md
- [ ] Run full test suite with tox
- [ ] Build documentation
- [ ] Create git tag
- [ ] Build and publish to PyPI
- [ ] Update GitHub release notes
- [ ] Notify users of breaking changes

### Backwards Compatibility
```python
# Deprecation warnings
def old_method(self):
    """Deprecated method."""
    warnings.warn(
        "old_method is deprecated, use new_method instead",
        DeprecationWarning,
        stacklevel=2
    )
    return self.new_method()

# Support multiple API versions
if api_version == "v1":
    return self._handle_v1_response(response)
else:
    return self._handle_v2_response(response)
```

## Development Workflow

### Code Review Process
1. Create feature branch
2. Implement changes
3. Write/update tests
4. Update documentation
5. Run tox locally
6. Create pull request
7. Address review feedback
8. Merge after approval

### Continuous Integration & CI/CD Best Practices

**Multi-Tier Testing Strategy** (see [CI/CD GitHub Actions Specification](../specs/2025-09-02-cicd-gha-best-practices/specs.md)):

- **Tier 1: Continuous Testing** - Fast feedback on every PR/push (5-10 minutes)
  - Run core tests on all Python versions (3.11, 3.12, 3.13)
  - Check code formatting with black and isort
  - Measure test coverage (minimum 70% requirement)
  - Validate YAML syntax with yamllint
  - Docker simulation testing for AWS Lambda compatibility

- **Tier 2: Daily Scheduled Testing** - Comprehensive validation (30-60 minutes)
  - Performance benchmarking with statistical significance
  - Real AWS Lambda environment testing
  - Security scans and dependency vulnerability checks
  - Build and deploy documentation
  - Performance regression detection

- **Tier 3: Release Candidate Testing** - Complete validation (45-90 minutes)
  - All tier 1 & 2 tests plus integration validation
  - Package building and distribution testing
  - Cross-platform testing (Ubuntu, Windows, macOS)
  - Quality gates for production deployment

**GitHub Actions Workflow Optimization**:

- **Smart Job Organization** - Reduce PR interface clutter through composite jobs
- **Conditional Execution** - Branch and commit message-based test triggering
- **Modern Action Versions** - Use latest stable actions (v4/v5)
- **Artifact Management** - Comprehensive test result preservation
- **Duplicate Prevention** - Optimize triggers to prevent redundant executions

### Developer Experience
- Clear error messages
- Helpful debug output
- Good IDE support
- Quick feedback loops
- Comprehensive examples
- Active community support

### üßπ Temporary File Cleanup Protocol - MANDATORY FOR AI ASSISTANTS

**üö® CRITICAL**: AI assistants MUST clean up temporary analysis files created during specification work.

#### What Constitutes a Temporary File

**Temporary Files** are analysis documents created during specification development that:
- Are **NOT** part of the permanent project structure
- Contain analysis, research, or planning information
- Are created to support specification development but not needed long-term
- Have names indicating temporary/analysis nature

**Common Temporary File Patterns**:
```bash
# Analysis documents
*-analysis.md
*-gap-analysis.md
*-governance-analysis.md

# Planning documents  
*-naming-standard.md
*-implementation-plan.md
*-research-notes.md

# Investigation files
*-investigation.md
*-findings.md
*-comparison.md
```

#### Cleanup Requirements

**1. Mandatory Cleanup Task**
- EVERY specification MUST include a cleanup task as the final implementation step
- Cleanup task MUST be included in requirements (REQ-*-XXX: Temporary File Cleanup)
- Cleanup task MUST be included in implementation components (COMP-CLEANUP)

**2. Integration Verification**
- Confirm ALL analysis findings are integrated into Agent OS specification
- Verify no critical information is lost during cleanup
- Ensure cleanup doesn't affect permanent documentation or code

**3. Automated Validation**
```bash
# Verify temporary files are removed
find . -maxdepth 1 -name "*analysis*.md" -o -name "*governance*.md" -o -name "*naming-standard*.md" -o -name "*investigation*.md" | wc -l | grep -q "^0$" && echo "‚úÖ Project root clean" || echo "‚ùå Temporary files remain"
```

#### Permanent vs Temporary Files

**‚úÖ KEEP (Permanent Files)**:
- Agent OS specifications (`.agent-os/specs/*/`)
- Project documentation (`docs/`)
- Code files (`src/`, `tests/`)
- Configuration files (`pyproject.toml`, `tox.ini`, etc.)
- README files and changelogs

**üóëÔ∏è REMOVE (Temporary Files)**:
- Analysis documents created during spec development
- Research notes and investigation files
- Naming standard documents (content should be in spec)
- Gap analysis documents (findings should be in spec)
- Planning documents (plans should be in tasks.md)

#### Example Cleanup Implementation

**In specs.md**:
```markdown
### REQ-XXX-YYY: Temporary File Cleanup
**Priority**: Medium
**Description**: Clean up temporary analysis files created during specification implementation
**Acceptance Criteria**:
- Remove all temporary analysis documents from project root
- Verify no temporary files remain that could confuse future development
- Confirm all analysis findings are properly integrated into Agent OS specification
```

**In tasks.md**:
```markdown
- [ ] **Cleanup Temporary Analysis Files** ‚è±Ô∏è 20 minutes
  - Remove `*-analysis.md` files
  - Remove `*-naming-standard.md` files  
  - Remove `*-investigation.md` files
  - Verify project root cleanliness
  - Confirm all findings integrated into spec
```

#### Why This Matters

- **Professional Delivery**: Clean project structure without development artifacts
- **Future Clarity**: No confusion from leftover temporary documents
- **Maintainability**: Clear separation between permanent and temporary content
- **Standards Compliance**: Consistent approach across all Agent OS specifications

## Common Pitfalls to Avoid

### Anti-Patterns
- ‚ùå Global state
- ‚ùå Mutable default arguments
- ‚ùå Bare except clauses
- ‚ùå Hardcoded values
- ‚ùå Synchronous I/O in async code
- ‚ùå Memory leaks from circular references

### Best Practices
- ‚úÖ Explicit is better than implicit
- ‚úÖ Fail fast with clear errors
- ‚úÖ Log liberally but carefully
- ‚úÖ Test edge cases
- ‚úÖ Document assumptions
- ‚úÖ Keep it simple

## Performance Guidelines

### Profiling
```python
# Profile before optimizing
import cProfile
import pstats

profiler = cProfile.Profile()
profiler.enable()
# Code to profile
profiler.disable()
stats = pstats.Stats(profiler)
stats.sort_stats('cumulative')
stats.print_stats(10)
```

### Memory Management
- Use generators for large datasets
- Clear caches periodically
- Avoid circular references
- Use weak references where appropriate
- Monitor memory usage in tests

### Optimization Priorities
1. Correctness first
2. Readability second
3. Performance third
4. Measure before optimizing
5. Document optimizations

## AI Assistant Development Process Requirements

### ü§ñ Mandatory AI Assistant Validation Process

**‚ö†Ô∏è CRITICAL**: AI assistants must follow strict validation protocols to prevent codebase drift and outdated reference errors.

#### Pre-Generation Validation (MANDATORY)

Before generating ANY code that integrates with the codebase:

1. **üìã Current API Validation**:
   ```bash
   # ALWAYS check current exports first
   read_file src/honeyhive/__init__.py
   grep -r "class.*:" src/honeyhive/api/
   ```

2. **üîç Import Pattern Verification**:
   ```bash
   # Check current import patterns in examples
   grep -r "from honeyhive import" examples/
   grep -r "import honeyhive" tests/
   ```

3. **üìö Current Usage Pattern Analysis**:
   - Read at least 2-3 current example files
   - Check recent test files for current API usage
   - Verify class names and method signatures

#### Workflow/CI Generation Rules (MANDATORY)

**üö® Never generate CI/CD workflows without codebase validation**:

1. **Current API Check**: Read `__init__.py` and `__all__` exports
2. **Test Pattern Review**: Check `tests/` for current import patterns  
3. **Example Validation**: Verify against `examples/` directory
4. **Documentation Cross-Check**: Ensure consistency with current docs

#### Case Study: HoneyHiveClient Failure (2025-09-02)

**‚ùå What Happened**: AI assistant generated workflow using `HoneyHiveClient` (deprecated Aug 28) instead of `HoneyHive` (current API)

**üîç Root Cause**: Generated code from memory/assumptions instead of current codebase validation

**‚úÖ Prevention**: Mandatory pre-generation codebase validation prevents this failure mode

#### AI Assistant Commit Requirements

**All AI assistant commits MUST**:
1. **Validate current API** before generating integration code
2. **Test generated code** against current codebase
3. **Update documentation** to reflect any changes
4. **Include validation evidence** in commit messages

**Example compliant commit message**:
```
feat: add release candidate workflow

VALIDATION EVIDENCE:
- Checked src/honeyhive/__init__.py exports: HoneyHive, HoneyHiveTracer
- Verified examples/basic_usage.py import patterns
- Tested against current API surface
- All imports validated against __all__ exports
```

#### Emergency Override Process

**Only in genuine emergencies**:
1. Document why validation was skipped
2. Add TODO for immediate post-emergency validation
3. Schedule validation within 24 hours
4. Update Agent OS with lessons learned

### üîÑ Continuous Validation Requirements

**For Long-Running Development Sessions**:
- Re-validate API every 50+ file changes
- Check for deprecation warnings before major code generation
- Refresh codebase understanding if session > 2 hours
- Always validate before final commits

## üõë Commit Review Protocol - MANDATORY FOR AI ASSISTANTS

**üö® CRITICAL**: AI assistants must pause for user review before ANY commit actions, especially when CHANGELOG updates are involved.

### Mandatory Review Checkpoints

**ALWAYS prompt user before committing when:**

1. **CHANGELOG.md updates are required** - Give user time to review entry accuracy
2. **Any commit action is planned** - Allow user to choose new commit vs amend
3. **Significant changes are staged** - Verify scope and impact with user
4. **Documentation updates are included** - Ensure accuracy of what was done vs what needs implementation

### Commit Decision Protocol

**Before executing any `git commit` command, AI assistants MUST ask:**

```
üõë COMMIT REVIEW CHECKPOINT

I need to commit these changes:
- [List of files and changes]
- [CHANGELOG entry if applicable]

CHANGELOG Entry Review:
[Show proposed CHANGELOG text]

Questions:
1. Does this CHANGELOG entry accurately reflect what was DONE vs what needs IMPLEMENTATION?
2. Should this be a NEW COMMIT or AMEND the previous commit?
3. Are there any other changes that should be included?
4. Is the commit message appropriate?

Please review and advise how to proceed.
```

### Rapid Iteration Exceptions

**AI assistants MAY iterate quickly WITHOUT review for:**
- **Pre-commit hook fixes** (formatting, linting corrections)
- **Technical corrections** that don't affect CHANGELOG
- **Build/test fixes** during development iteration

**But MUST pause for review when:**
- CHANGELOG updates are identified as needed
- Commit represents completion of a task/feature
- Multiple files are being committed together
- User explicitly requests review protocol

### CHANGELOG Review Focus Areas

**User should verify CHANGELOG entries for:**

1. **Accuracy**: Does entry reflect what was actually done?
2. **Language**: Uses "created/defined/planned" vs "implemented/completed" appropriately
3. **Scope**: Covers all significant changes without overstating
4. **Implementation Status**: Clear about specification vs implementation phase

### Git History Management

**Commit Strategy Decision Tree:**

- **New Commit**: When adding genuinely new functionality or fixes
- **Amend Commit**: When correcting errors in the previous commit (typos, CHANGELOG corrections, missed files)
- **Squash Later**: When multiple related commits should be combined before PR

### Example Review Interaction

```
AI: "I need to update CHANGELOG.md for the integration testing spec.

Proposed entry:
- Created integration testing consolidation specification
- Updated MDC files with Agent OS standards
- IMPLEMENTATION REQUIRED: Specification ready, code changes pending

Should I:
1. Create new commit with this CHANGELOG update?
2. Amend previous commit to include corrected CHANGELOG?
3. Modify the CHANGELOG entry before committing?

Please review and let me know how to proceed."

User: "The entry looks good, but this should amend the previous commit since it's just fixing the CHANGELOG accuracy."

AI: "Understood. I'll amend the previous commit with the corrected CHANGELOG entry."
```

### Benefits of This Protocol

- **Prevents incorrect CHANGELOG entries** that misrepresent work status
- **Maintains clean git history** through appropriate use of amend vs new commits
- **Gives user control** over commit strategy and messaging
- **Reduces technical debt** from hasty or inaccurate commits
- **Ensures documentation accuracy** matches actual implementation status

### Enforcement

This protocol is **MANDATORY** for all AI assistants working on the HoneyHive Python SDK project. Violations should be corrected immediately and the protocol reinforced.

## üìö Documentation Quality Prevention System

### Overview

The HoneyHive Python SDK implements a **prevention-first** documentation quality system. Issues are caught and fixed **before commit** via pre-commit hooks, with CI serving as a safety net to block merges if anything slips through.

### üèóÔ∏è Prevention Architecture

```
Developer writes docs ‚Üí Pre-commit hooks ‚Üí Auto-fix ‚Üí Validation ‚Üí Commit allowed/blocked
                                    ‚Üì
                              CI Safety Net (backup validation)
```

**Philosophy**: Prevention at commit time is 100x better than reactive monitoring.

### üõ°Ô∏è Consolidated Documentation Quality Mini-App

#### **Documentation Quality Controller** (`scripts/docs-quality.py`)
**Purpose**: Unified documentation quality control system that replaces individual validation scripts
**Usage**: 
```bash
# Check all documentation
python scripts/docs-quality.py check --path docs

# Auto-fix all issues
python scripts/docs-quality.py fix --path docs

# Generate comprehensive report
python scripts/docs-quality.py report --path docs --json

# Validate specific aspects only
python scripts/docs-quality.py check --path docs --only eventtype rst_quality

# Check specific files
python scripts/docs-quality.py check --path docs/tutorials/01-quick-start.rst
```

**Integrated Validators**:
- **EventType Validator**: Prevents string literal violations (`event_type="model"` ‚Üí `EventType.model`)
- **RST Quality Validator**: Checks formatting, structure, title underlines, blank lines
- **Code Example Validator**: Validates Python syntax, imports, detects hardcoded credentials
- **Navigation Validator**: Validates `:doc:` and `:ref:` links, auto-fixes cross-tree references

**What it catches** (comprehensive detection):
- ‚ùå `event_type="model"` ‚Üí ‚úÖ `event_type=EventType.model`
- ‚ùå Missing `from honeyhive.models import EventType`
- ‚ùå Title underline length mismatches
- ‚ùå Missing blank lines around directives
- ‚ùå Malformed code block indentation
- ‚ùå Syntax errors in code blocks
- ‚ùå Missing import statements
- ‚ùå Hardcoded API keys (security issue)
- ‚ùå Broken `:doc:` and `:ref:` references
- ‚ùå Cross-tree references that should be HTML links

**Performance Benefits**:
- **Single-pass processing**: All validations run simultaneously for better performance
- **Shared state**: Coordinated fixing across different validation types
- **Enhanced detection**: Finds more issues than individual scripts
- **Unified reporting**: Consistent output format and comprehensive summaries

#### **Documentation Coverage Checker** (`scripts/check-doc-coverage.py`)
**Purpose**: Ensures comprehensive documentation coverage of all APIs and features
**Usage**:
```bash
python scripts/check-doc-coverage.py                      # Full coverage analysis
python scripts/check-doc-coverage.py --fix                # Auto-generate doc stubs
python scripts/check-doc-coverage.py --json               # JSON output for CI/CD
```

**Coverage Analysis**:
- **API Coverage**: Verifies all public classes, functions, and methods are documented
- **Feature Coverage**: Ensures all Agent OS features have documentation
- **Cross-References**: Validates proper linking between documentation sections
- **Orphaned Files**: Identifies documentation files not referenced anywhere
- **Example Coverage**: Checks if APIs have usage examples

**Metrics Tracked**:
- API coverage percentage (currently ~11.5%)
- Feature coverage percentage (currently 0.0%)
- Orphaned file count (currently 51)
- Broken reference count (currently 99)
- Missing example count (currently 5)

#### **Performance Benchmarking** (`scripts/benchmark-docs-performance.py`)
**Purpose**: Measures documentation validation performance and identifies optimization opportunities
**Usage**:
```bash
python scripts/benchmark-docs-performance.py                  # Standard benchmark
python scripts/benchmark-docs-performance.py --iterations 5   # More iterations
python scripts/benchmark-docs-performance.py --json           # JSON output
```

**Performance Metrics**:
- **Quality Validation**: ~0.29s (very fast)
- **Coverage Analysis**: ~0.36s (fast)
- **Navigation Validation**: ~0.17s (fastest)
- **Sphinx Build**: Variable (depends on documentation quality)
- **Parallel Execution**: May be slower for small tasks due to overhead

**CI/CD Optimizations Implemented**:
- **Enhanced Caching**: Python dependencies, Sphinx builds, system packages
- **Single Python Version**: Reduced matrix overhead for documentation validation
- **Parallel Task Execution**: Background processing of validation tasks
- **Conditional Validation**: Link validation only on main branches
- **Shallow Clones**: Faster repository checkout
- **Optimized Dependencies**: Only essential packages for documentation tasks

#### **Quality Dashboard** (`scripts/docs-quality-dashboard.py`)
**Purpose**: Web-based real-time monitoring and visualization of documentation quality metrics
**Usage**:
```bash
python scripts/docs-quality-dashboard.py                     # Start dashboard on port 8000
python scripts/docs-quality-dashboard.py --port 8080        # Custom port
python scripts/docs-quality-dashboard.py --collect-only     # Data collection only
```

**Dashboard Features**:
- **Real-time Metrics**: Live quality overview, coverage statistics, build status
- **Historical Trends**: Interactive charts showing quality evolution over time
- **Performance Monitoring**: Validation timing and performance benchmarks
- **Auto-refresh**: Configurable automatic data updates
- **Responsive Design**: Modern web interface with mobile support
- **Data Persistence**: SQLite database for historical trend analysis

**Metrics Visualized**:
- **Quality Overview**: Total issues, errors, warnings breakdown with trend indicators
- **Coverage Metrics**: API coverage percentage, feature coverage, orphaned files
- **Build Status**: Sphinx warnings/errors, build success rate, build timing
- **Performance**: Validation speed, coverage analysis time, build duration
- **Historical Trends**: Multi-metric time series with configurable date ranges

**Technical Architecture**:
- **Backend**: Flask web server with REST API endpoints
- **Database**: SQLite for metrics storage and historical data
- **Frontend**: Modern HTML5/CSS3/JavaScript with Chart.js for visualizations
- **Data Collection**: Automated integration with existing validation tools
- **Caching**: Intelligent data collection with configurable refresh intervals

#### **Legacy Navigation Validator** (`docs/utils/validate_navigation.py`)
**Purpose**: Validates documentation links and cross-references (specialized use cases)
**Usage**:
```bash
python docs/utils/validate_navigation.py --source-only    # Check RST links
python docs/utils/validate_navigation.py --local          # Check with server
python docs/utils/validate_navigation.py --fix            # Auto-fix references
```

**What it catches**:
- ‚ùå Broken `:doc:` references
- ‚ùå Cross-tree reference issues
- ‚ùå Missing toctree entries
- ‚ùå Orphaned documentation files

### üîí Pre-commit Integration

All validation scripts are integrated into pre-commit hooks to prevent quality issues:

```yaml
# .pre-commit-config.yaml
- id: eventtype-validation
  name: EventType Enum Validation (Prevent String Literals)
  entry: python scripts/check-doc-types.py

- id: rst-quality-validation  
  name: RST Quality Validation (Formatting & Structure)
  entry: python scripts/check-rst-quality.py

- id: code-example-validation
  name: Code Example Validation (Syntax & Imports)
  entry: python scripts/test-doc-examples.py

- id: docs-navigation-validation
  name: Documentation Navigation Validation
  entry: scripts/validate-docs-navigation.sh
```

### üöÄ GitHub Actions Integration

The `.github/workflows/documentation-quality.yml` workflow runs comprehensive validation on all PRs:

- **Multi-Python testing** (3.11, 3.12, 3.13)
- **Zero-warning Sphinx builds** (fails on any warnings)
- **Comprehensive validation** (all scripts run)
- **Quality reports** generated and uploaded as artifacts
- **Link validation** on main branch

### üìã Quality Standards

#### **EventType Usage**
```python
# ‚úÖ CORRECT - Always use enums
from honeyhive.models import EventType

@trace(event_type=EventType.model)    # LLM calls
@trace(event_type=EventType.tool)     # Individual functions
@trace(event_type=EventType.chain)    # Multi-step workflows
@trace(event_type=EventType.session)  # User interactions

# ‚ùå WRONG - Never use string literals
@trace(event_type="model")  # Breaks type safety
```

#### **Code Examples**
```python
# ‚úÖ CORRECT - Complete, working examples
from honeyhive import HoneyHiveTracer, trace
from honeyhive.models import EventType
from os import getenv

tracer = HoneyHiveTracer.init(api_key=getenv("HH_API_KEY"))

@trace(event_type=EventType.model)
def my_function():
    return "Hello, World!"

# ‚ùå WRONG - Missing imports, hardcoded keys
@trace(event_type="model")  # Missing import and enum
def my_function():
    tracer = HoneyHiveTracer.init(api_key="sk-1234...")  # Hardcoded key
```

#### **RST Formatting**
```rst
‚úÖ CORRECT - Proper title underlines
Section Title
=============

Subsection Title
----------------

.. code-block:: python

   # Properly indented (3 spaces minimum)
   print("Hello, World!")

‚ùå WRONG - Mismatched underlines
Section Title
===========  # Too short!

.. code-block:: python

 # Wrong indentation (only 1 space)
 print("Hello, World!")
```

### üéØ Success Metrics

The system maintains these quality standards:

- **0 Sphinx build warnings** (enforced in CI)
- **0 EventType string literals** (enforced by pre-commit)
- **100% working code examples** (syntax validated)
- **0 broken internal links** (navigation validated)

### üîß Developer Workflow

#### **Before Committing**
```bash
# Run all validations locally
python scripts/check-doc-types.py
python scripts/check-rst-quality.py  
python scripts/test-doc-examples.py
python docs/utils/validate_navigation.py --source-only

# Or let pre-commit handle it
pre-commit run --all-files
```

#### **Fixing Issues**
```bash
# Auto-fix what's possible
python scripts/check-doc-types.py --fix
python scripts/check-rst-quality.py --fix
python scripts/test-doc-examples.py --fix
python docs/utils/validate_navigation.py --source-only --fix

# Manual fixes for complex issues
# - Syntax errors in code blocks
# - Complex table formatting
# - Security issues (hardcoded keys)
```

#### **CI/CD Integration**
- **Pull Requests**: All validation runs automatically
- **Main Branch**: Additional link validation with live server
- **Quality Reports**: Generated for each Python version
- **Artifacts**: Documentation builds and reports uploaded

### üö® Error Prevention

This system prevents the types of errors that previously caused:
- **99 EventType violations** ‚Üí Now caught by pre-commit
- **236 Sphinx warnings** ‚Üí Now enforced as zero-tolerance
- **Broken navigation links** ‚Üí Now validated automatically
- **Syntax errors in examples** ‚Üí Now caught before commit

### üìñ References

- **Specification**: `.agent-os/specs/2025-09-03-documentation-quality-prevention/`
- **Validation Scripts**: `scripts/check-*.py`, `scripts/test-*.py`
- **GitHub Workflow**: `.github/workflows/documentation-quality.yml`
- **Pre-commit Config**: `.pre-commit-config.yaml`
