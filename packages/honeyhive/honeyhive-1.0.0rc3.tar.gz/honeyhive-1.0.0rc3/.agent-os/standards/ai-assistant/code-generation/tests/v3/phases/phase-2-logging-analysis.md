# Phase 2: Logging Analysis - Comprehensive Strategy Restoration

## üéØ **CRITICAL PHASE: LOGGING STRATEGY FOR TEST SUCCESS**

**Purpose**: Comprehensive logging analysis to prevent test failures through proper mocking/real logging strategy  
**Archive Success**: Detailed logging mock strategy prevented assertion failures  
**V2 Failure**: Generic logging search missed conditional patterns and mock requirements  
**V3 Restoration**: Archive depth + path-specific logging strategies  

---

## üö® **MANDATORY LOGGING ANALYSIS COMMANDS**

### **1. Complete Logging Call Detection**
```bash
# Find ALL logging calls (critical for mock strategy)
grep -n "log\." [PRODUCTION_FILE]
# Expected: All logger.debug, logger.info, logger.warning, logger.error calls

# Alternative patterns for comprehensive detection
grep -n "\.log(" [PRODUCTION_FILE]
grep -n "logging\." [PRODUCTION_FILE]
```

### **2. Logging Infrastructure Analysis**
```bash
# Identify logging imports and setup (mock targets)
grep -n "import.*log\|from.*log\|getLogger\|basicConfig" [PRODUCTION_FILE]
# Expected: How logging is configured and imported

# Find logging configuration patterns
grep -n "Logger\|handler\|formatter" [PRODUCTION_FILE]
```

### **3. Project-Specific Logging Detection**
```bash
# Find safe_log usage (HoneyHive's centralized logging utility)
grep -n "safe_log" [PRODUCTION_FILE]
# Expected: Usage of centralized logging utility with parameters

# Count safe_log occurrences for mock planning
grep -c "safe_log" [PRODUCTION_FILE]
```

### **4. Conditional Logging Pattern Analysis**
```bash
# Find conditional logging (critical for branch testing)
grep -B2 -A2 "if.*log\|log.*if" [PRODUCTION_FILE]
# Expected: Conditional logging patterns that need branch testing

# Find logging in exception handlers
grep -B3 -A3 "except.*log\|log.*except" [PRODUCTION_FILE]
```

---

## üìä **COMPREHENSIVE LOGGING REQUIREMENTS**

### **üîç MUST DOCUMENT FOR TEST STRATEGY**

**LOGGING CALL INVENTORY:**
- **All safe_log calls** with parameters (tracer, level, message, context)
- **All standard logging calls** (logger.debug, logger.info, etc.)
- **Logging levels used** (debug, info, warning, error, critical)
- **Conditional logging patterns** (if verbose, if error, etc.)
- **Exception logging** (error handling with logging)

**LOGGING CONTEXT ANALYSIS:**
- **Message patterns** (static vs dynamic messages)
- **Context data** (additional parameters passed to logging)
- **Logging frequency** (how often each log call is made)
- **Error path logging** (logging in exception scenarios)

---

## üõ§Ô∏è **PATH-SPECIFIC LOGGING STRATEGIES**

### **üß™ UNIT TEST PATH: COMPREHENSIVE LOGGING MOCKS**

**Mock Strategy Requirements:**
```python
# Mock ALL logging infrastructure
@patch('honeyhive.tracer.instrumentation.initialization.safe_log')
@patch('honeyhive.utils.logger.get_tracer_logger')
def test_function_with_logging(mock_safe_log, mock_get_logger):
    """Test with comprehensive logging mocks."""
    
    # Configure logger mock
    mock_logger = Mock()
    mock_get_logger.return_value = mock_logger
    
    # Execute function
    result = function_under_test(mock_tracer)
    
    # Verify logging calls with exact parameters
    mock_safe_log.assert_called_with(
        mock_tracer,           # tracer instance
        'info',                # log level
        'Expected message',    # message
        {'key': 'value'}       # context data
    )
    
    # Verify call count for multiple logs
    assert mock_safe_log.call_count == 3
```

**Conditional Logging Mock Patterns:**
```python
def test_conditional_logging_verbose_enabled():
    """Test logging when verbose mode enabled."""
    mock_tracer.verbose = True
    
    function_under_test(mock_tracer)
    
    # Verify verbose logging occurred
    debug_calls = [call for call in mock_safe_log.call_args_list 
                   if call[0][1] == 'debug']
    assert len(debug_calls) >= 1

def test_conditional_logging_verbose_disabled():
    """Test logging when verbose mode disabled."""
    mock_tracer.verbose = False
    
    function_under_test(mock_tracer)
    
    # Verify no debug logging occurred
    debug_calls = [call for call in mock_safe_log.call_args_list 
                   if call[0][1] == 'debug']
    assert len(debug_calls) == 0
```

**Error Path Logging Verification:**
```python
@patch('external_dependency', side_effect=Exception("Test error"))
def test_error_logging(mock_external, mock_safe_log):
    """Test error logging in exception scenarios."""
    
    result = function_under_test(mock_tracer)
    
    # Verify error was logged
    error_calls = [call for call in mock_safe_log.call_args_list 
                   if call[0][1] == 'error']
    assert len(error_calls) == 1
    assert 'Test error' in error_calls[0][0][2]  # Error message
```

### **üîó INTEGRATION TEST PATH: REAL LOGGING VALIDATION**

**Real Logging Strategy Requirements:**
```python
import logging
from unittest import TestCase

class TestRealLogging(TestCase):
    """Integration tests with real logging validation."""
    
    def test_real_logging_output(self):
        """Test actual logging output in integration scenario."""
        # Configure real logging for test
        with self.assertLogs('honeyhive', level='INFO') as log:
            # Execute function with real logging
            tracer = HoneyHiveTracer(
                api_key=os.getenv("HH_TEST_API_KEY"),
                project="integration-test",
                verbose=True
            )
            
            # Real function execution
            initialize_tracer_instance(tracer)
            
            # Verify real log messages appeared
            self.assertIn('Tracer initialization started', log.output[0])
            self.assertIn('OTEL components initialized', log.output[1])
```

**Real Error Logging Validation:**
```python
def test_real_error_logging_integration(self):
    """Test real error logging with invalid configuration."""
    # Use invalid configuration to trigger real errors
    tracer = HoneyHiveTracer(
        api_key="invalid-key",
        project="test-project"
    )
    
    # Capture real logging output
    with self.assertLogs('honeyhive', level='ERROR') as log:
        try:
            # This should fail and log real errors
            tracer.create_event(event_name="test")
        except Exception:
            pass  # Expected failure
        
        # Verify real error messages
        self.assertTrue(any('authentication' in msg.lower() 
                          for msg in log.output))
```

---

## üö® **CRITICAL LOGGING FAILURE PREVENTION**

### **V2 Failures That V3 MUST Prevent:**

1. **Wrong Logging Assertions** (caused test failures):
   ```python
   # V2 FAILED: Expected specific log calls that weren't made
   mock_safe_log.assert_called_with('Expected message')
   
   # V3 PREVENTS: Analyze actual logging patterns first
   # Use assert_any_call for specific messages
   # Use call_count for frequency verification
   ```

2. **Missing Conditional Logging** (missed test branches):
   ```python
   # V2 FAILED: Didn't test verbose vs non-verbose logging
   # V3 PREVENTS: Analyze conditional patterns and test both paths
   ```

3. **Wrong Log Levels** (incorrect expectations):
   ```python
   # V2 FAILED: Expected 'warning' level, actual was 'debug'
   # V3 PREVENTS: Analyze actual log levels used in production
   ```

---

## üìã **MANDATORY PHASE 2 COMPLETION EVIDENCE**

### **üõë CRITICAL: Progress Table Update Required**

**You MUST update the progress table in chat window before proceeding to Phase 3.**

**Required Evidence Format:**
```
| Phase | Status | Evidence | Commands | Validation | Gate |
|-------|--------|----------|----------|------------|------|
| 2: Logging Analysis | ‚úÖ | Found X safe_log calls (Y debug, Z info, A error), B conditional branches analyzed, C error paths identified | 4/4 | Manual | ‚úÖ |
```

### **üö® ENFORCEMENT PATTERNS**

**‚ùå VIOLATION INDICATORS:**
- "Phase 2 complete" without showing updated table
- "Moving to Phase 3" without table update
- "Logging analysis finished" without specific evidence
- "Found logging calls" without counts and conditional analysis

**üõë VIOLATION RESPONSE:**
"STOP - You completed Phase 2 but didn't update the progress table. Show me the updated table in the chat window with Phase 2 marked as ‚úÖ and evidence documented (safe_log count, conditional branches, error paths) before proceeding to Phase 3."

---

## üéØ **LOGGING ANALYSIS SUCCESS CRITERIA**

**Phase 2 is complete ONLY when:**
1. ‚úÖ All safe_log calls identified with parameters
2. ‚úÖ All standard logging calls documented
3. ‚úÖ Conditional logging patterns analyzed
4. ‚úÖ Error path logging identified
5. ‚úÖ Path-specific strategy determined (mock vs real)
6. ‚úÖ Progress table updated with specific evidence
7. ‚úÖ Mock strategy planned for unit tests OR real logging strategy for integration

**Failure to complete Phase 2 properly WILL cause logging assertion failures in generated tests.**

---

## üîÑ **INTEGRATION WITH OTHER PHASES**

### **Phase 1 ‚Üí Phase 2 Integration**
- Use function analysis to identify which functions perform logging
- Cross-reference with attribute analysis for logging configuration access

### **Phase 2 ‚Üí Phase 3 Integration**
- Logging infrastructure becomes part of dependency mocking strategy
- safe_log utility identified as critical internal dependency

### **Phase 2 ‚Üí Test Generation Integration**
- Unit tests: Mock all logging with verification
- Integration tests: Use real logging with output validation
- Error tests: Verify logging in exception scenarios

---

## ‚úÖ **PHASE 2 SUCCESS VALIDATION**

**Logging analysis is successful when:**
1. ‚úÖ Complete inventory of all logging calls
2. ‚úÖ Conditional logging patterns identified
3. ‚úÖ Error path logging documented
4. ‚úÖ Path-specific strategy determined
5. ‚úÖ Mock requirements documented (unit) OR real logging plan (integration)
6. ‚úÖ Progress table shows Phase 2 ‚úÖ with evidence

**Next Phase**: Only proceed to [Phase 3: Dependency Analysis](phase-3-dependency-analysis.md) after progress table update with evidence.
