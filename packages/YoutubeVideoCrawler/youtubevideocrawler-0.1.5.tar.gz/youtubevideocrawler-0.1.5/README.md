# ğŸ¥ YoutubeVideoCrawler

A **Python module for scalable YouTube data collection** using the YouTube Data API v3.  
It supports keyword-based crawling, comment extraction, intelligent rate limiting, and structured storage (JSON + CSV).

---

## ğŸš€ Features

- ğŸ” Crawl YouTube videos by keywords
- ğŸ§  Supports **multi-keyword** and **incremental crawling**
- ğŸ’¬ Optionally fetch video comments
- ğŸ§¾ Stores all video metadata in JSON + indexed CSV
- ğŸ§° Automatically skips duplicate videos
- ğŸ’¤ Rate-limit friendly (`safe_sleep`)
- ğŸ“œ Flexible logger: print to console, file, or both

---

## ğŸ“¦ Installation

```bash
pip install YoutubeVideoCrawler
````

*(If installing locally from source after modification:)*

```bash
pip install .
```

---

## ğŸ”‘ Requirements

Before running, you **must have a YouTube Data API key**:

1. Visit [Google Cloud Console](https://console.cloud.google.com/apis/credentials)
2. Create a project â†’ Enable the **YouTube Data API v3**
3. Generate an **API key**

---

## ğŸ§© Basic Usage

```python
from YoutubeVideoCrawler.crawler import YouTubeCrawler

# Initialize the crawler
crawler = YouTubeCrawler(api_key="YOUR_YOUTUBE_API_KEY")

# Start crawling
crawler.run(
    keywords=["software engineering", "machine learning", "distributed systems"],
    fetch_comments=False,
    max_per_keyword=100
)
```

This will:

* Crawl up to 100 videos for each keyword
* Store video metadata in `videos.json`
* Maintain an index in `video_index.csv`

---

## ğŸ“‚ Storage Output

After running, the module creates the following files:

| File              | Description                                            |
| ----------------- | ------------------------------------------------------ |
| `videos.json`     | Full JSON dump of all fetched video metadata           |
| `video_index.csv` | Lightweight index (videoId, title) for quick reference |
| `token.csv`       | Tracks how many videos have been fetched per keyword   |
| `app.log`         | Logger file recording crawl progress                   |

---

## âš™ï¸ Advanced Usage

### ğŸ§  Load keywords from file

Create a file `keywords.txt`:

```
software engineering
deep learning
data visualization
```

Then run:

```python
crawler.run(keyword_file="keywords.txt", max_per_keyword=50)
```

---

### ğŸª£ Multi-keyword mode

Search all keywords in a single query (logical OR):

```python
crawler.run(
    keywords=["AI", "ML", "DL"],
    search_mode="multi",
    max_per_keyword=200
)
```

---

### ğŸ’¬ Fetch comments

Enable fetching comments for each video:

```python
crawler.run(
    keywords=["open source software"],
    fetch_comments=True,
    max_per_keyword=20
)
```

Comments are stored within the corresponding video entry in `videos.json`.

---

### ğŸ•’ Resume partially completed crawl

The crawler automatically:

* Detects existing videos in `video_index.csv`
* Skips duplicates
* Continues from where it left off

---

## ğŸ§± Module Structure

```
YoutubeVideoCrawler/
â”‚
â”œâ”€â”€ crawler.py      # Core crawling logic using YouTube API
â”œâ”€â”€ storage.py      # Persistent storage manager (JSON + CSV)
â””â”€â”€ utils.py        # Logging, sleep, and helper functions
```

---

## ğŸ§° Utility Functions

### `get_logger(name="YouTubeCrawler", log_file="app.log")`

Creates a flexible logger that can print to console, file, or both:

```python
logger.info("This message prints to console and file.")
logger.info("This prints only to file.", print_to="file")
```

### `safe_sleep(seconds, logger)`

Sleep while logging to respect API rate limits.

### `keyword_loader(file_path)`

Loads keyword list from file, skipping lines starting with `#`.

---

## ğŸ“Š Example Output

Example `video_index.csv`:

```csv
videoId,title
XyZ12345,Introduction to Software Engineering
AbC67890,Machine Learning Basics
```

Example JSON entry:

```json
{
  "id": "XyZ12345",
  "snippet": {
    "title": "Introduction to Software Engineering",
    "publishedAt": "2025-01-01T12:00:00Z"
  },
  "statistics": {
    "viewCount": "15342",
    "likeCount": "876"
  },
  "keyword": "software engineering",
  "comments": []
}
```

---

## ğŸ§© Command-Line Example (optional script)

You can create a small script `crawl_youtube.py`:

```python
from yt_video_crawler import YouTubeCrawler

if __name__ == "__main__":
    crawler = YouTubeCrawler(api_key="YOUR_API_KEY")
    crawler.run(keyword_file="keywords.txt", fetch_comments=False, max_per_keyword=50)
```

Run it:

```bash
python crawl_youtube.py
```

---

## ğŸ§‘â€ğŸ’» Developer Notes

* Respects YouTube API quota and sleep intervals.
* Modular design for easy integration with data pipelines.
* Extend `Storage` class for custom database backends (e.g., MongoDB, SQLite).
* Log verbosity can be controlled using `print_to` argument.

---

## ğŸ“œ License

MIT License Â© 2025 [Md. Masud Mazumder](https://github.com/masudmm)

---

## ğŸ§  Acknowledgments

* [Google API Python Client](https://github.com/googleapis/google-api-python-client)
* [tqdm](https://github.com/tqdm/tqdm) for progress bars
* [pandas](https://pandas.pydata.org/) for data handling

---

## ğŸ¤ Contributing

Pull requests are welcome!