import torch
import torch.nn as nn
from monai.networks.schedulers import Scheduler

from cocolit.utils import unpad


class WeightedImageSpaceLoss:
    """
    Implements the Weighted Image Space Loss (WISL) as defined in Eq. (4) of the CoCoLIT paper.

    This loss term is designed to improve image synthesis quality during ControlNet training
    by fine-tuning the VAE decoder. It computes a weighted L1 difference between the ground-truth
    image and a decoded prediction.

    The prediction is generated by first estimating the fully denoised latent, z_0_hat, from the
    noisy latent z_t and the model's predicted noise eps_pred. This z_0_hat is then passed
    through the VAE decoder.

    A linear weighting schedule, lambda_t = (T-t)/T, prioritizes reconstruction quality for timesteps t
    that are closer to 0, placing more importance on samples with less noise.

    Attributes:
        target_vae (nn.Module): The VAE model used as the decoder D.
        noise_scheduler (Scheduler): The scheduler providing noise parameters like alpha_bar_t.
        vae_space_shape (tuple): The spatial dimensions of the VAE's latent space.
    """

    def __init__(
        self,
        target_vae: nn.Module,
        noise_scheduler: Scheduler,
        vae_space_shape=(4, 32, 38, 32)
    ):
        self.target_vae = target_vae
        self.noise_scheduler = noise_scheduler
        self.vae_space_shape = vae_space_shape
        self.to_batched_vae_space = lambda b: (b,) + self.vae_space_shape
        self.n_timesteps = self.noise_scheduler.num_train_timesteps

    def __call__(
        self,
        x_0: torch.Tensor,
        z_t: torch.Tensor,
        eps_pred: torch.Tensor,
        timesteps: torch.Tensor,
        device: str
    ) -> torch.Tensor:
        """
        Calculates the WISL value for a given batch based on Eq. (4).

        The formula is: L_WISL := E[ lambda_t * |y - D(z_0_hat)|_1 ]

        Args:
            x_0 (torch.Tensor): The ground-truth image y.
            z_t (torch.Tensor): The noised latent tensor z_t at timestep t.
            eps_pred (torch.Tensor): The predicted noise eps_pred for the given z_t.
            timesteps (torch.Tensor): The corresponding timestep t for each sample in the batch.
            device (str): The torch device on which to perform calculations.

        Returns:
            torch.Tensor: A scalar tensor representing the final computed loss.
        """

        batch_size = z_t.shape[0]

        # Get cumulative alpha product for the current timestep t, which is alpha_bar_t
        alpha_prod_t = self.noise_scheduler.alphas_cumprod[timesteps.cpu()].to(device)
        sqrt_alpha_prod_t = (alpha_prod_t ** .5).view(-1, 1, 1, 1, 1)

        # Get 1 - alpha_bar_t
        beta_prod_t = 1 - alpha_prod_t
        sqrt_beta_prod_t = (beta_prod_t ** .5).view(-1, 1, 1, 1, 1)

        # Predict the original latent z_0 using the formula for z_0_hat
        # z_0_hat = (z_t - sqrt(1 - alpha_bar_t) * eps_pred) / sqrt(alpha_bar_t)
        z_0_pred = (z_t - sqrt_beta_prod_t * eps_pred) / sqrt_alpha_prod_t

        # Unpad the latent to bring it to the original VAE latent space shape
        z_0_pred = unpad(z_0_pred, self.to_batched_vae_space(batch_size))

        # Decode the inferred z_0 to get the predicted image D(z_0_hat)
        x_0_pred = self.target_vae.decode_stage_2_outputs(z_0_pred)

        # Compute the linear weighting lambda_t = (T - t) / T
        lambda_t = ((self.n_timesteps - timesteps) / self.n_timesteps).view(-1, 1, 1, 1, 1)

        # Finally compute the weighted L1 loss in image space
        loss = (lambda_t * torch.abs(x_0_pred - x_0)).mean()
        return loss