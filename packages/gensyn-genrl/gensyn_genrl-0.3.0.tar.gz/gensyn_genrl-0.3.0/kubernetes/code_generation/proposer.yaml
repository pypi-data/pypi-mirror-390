apiVersion: v1
kind: Service
metadata:
  name: proposer-1-1
  namespace: research
spec:
  clusterIP: None
  selector:
    job-name: proposer-1-1
  ports:
    - name: nccl
      port: 29500
      targetPort: 29500
---
apiVersion: batch/v1
kind: Job
metadata:
  name: proposer-1-1
  namespace: research
  annotations:
    kueue.x-k8s.io/queue-name: research-queue
spec:
  suspend: true # Required for the queue to work
  completionMode: Indexed

  # Set to the number of parallel workers you'd like
  completions: 2
  parallelism: 2

  template:
    spec:
      nodeSelector:
        research-reserved: "true"

      runtimeClassName: nvidia
      tolerations:
        - key: "research-volume"
          operator: "Exists"
          effect: "NoSchedule"

        - key: "nvidia.com/gpu"
          operator: "Exists"
          effect: "NoSchedule"

      imagePullSecrets:
        - name: gcp-repo-creds

      restartPolicy: Never
      subdomain: proposer-1-1

      volumes:
      - emptyDir:
          medium: Memory
          sizeLimit: 16Gi
        name: shm

      - name: shared-volume
        persistentVolumeClaim:
          claimName: research-volume

      containers:
        # Change the below line to the image you want the job to use.
        - image: europe-docker.pkg.dev/gensyn-main/registry/diloco-fp8-enabled:latest
          name: job
          securityContext:
            privileged: true

          # These should be self explanatory, but these change based on the job
          workingDir: /home/gensyn/shared/
          command:
            - bash
            - -c
            - |
              # Install Ollama
              curl -fsSL https://ollama.com/install.sh | sh
              
              # Start Ollama server in background
              nohup ollama serve > /tmp/ollama.log 2>&1 &

              # Wait for Ollama to initialize
              sleep 5 &&

              # Verify Ollama installation and pull model
              ollama --version &&
              ollama pull qwen2.5-coder:1.5b-instruct &&
              ollama list || true &&

              # Setup Deno and PATH
              curl -fsSL https://deno.land/install.sh | sh
              export PATH="$HOME/.deno/bin:$PATH"

              # CREATE A PRIVATE, TEMPORARY BUILD FOLDER
              BUILD_DIR=$(mktemp -d -t genrl-build-XXXX) &&

              # COPY THE SOURCE CODE TO THE PRIVATE FOLDER
              cp -r /home/gensyn/shared/semih/codeswarm/genrl-private "$BUILD_DIR" &&

              # CD INTO THE PRIVATE FOLDER
              cd "$BUILD_DIR/genrl-private" &&
              
              # Install Python deps (including Ollama package)
              PIP_CACHE_DIR=/tmp/pip-cache pip install .[examples] &&
              pip install ollama &&

              # Install other dependencies
              pip install vllm &&
              pip install langchain-sandbox &&
              pip install hivemind@git+https://github.com/gensyn-ai/hivemind@639c964a8019de63135a2594663b5bec8e5356dd &&

              # Run training script
              . scripts/train_hivemind.sh code_gen code-gen-hivemind.yaml

          env:
          - name: MASTER_ADDR
            # Every node will be discoverable at <job_name>-<index>.<subdomain>.research.svc.cluster.local
            value: proposer-1-1-0.proposer-1-1.research.svc.cluster.local 

          - name: MASTER_PORT
            value: '29500'

          - name: LAMBDA
            value: '1' 

          - name: HIVEMIND_WORLD_SIZE
            value: '1'


          # Feel free to add more ports below.
          ports:
          - containerPort: 29500
            name: nccl

          # These are per-pod
          resources:
            requests:
              memory: 600G
              cpu: 100
            limits:
              nvidia.com/gpu: '1'
              memory: 600G

          volumeMounts:
            - mountPath: /dev/shm
              name: shm

            - name: shared-volume
              mountPath: /home/gensyn/shared