# è¿™ä¸ªæ˜¯åŸå§‹ç‰ˆæœ¬ï¼ˆæµ‹è¯•ä½¿ç”¨ï¼Œå¹¶æ²¡æœ‰ä½¿ç”¨ï¼‰
import sys,os
sys.path.append(os.getcwd())

import logging
import time
import os
import tempfile
from typing import Dict, List, Any, Optional,Union
import asyncio
import pandas as pd


from pgmanage.asyncpg.connect import AsyncPgPool
from pdcleaner import DataSet

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)




# ä½¿ç”¨copyæ–¹æ³•è®²æ•°æ®å†™å…¥åˆ°PostgreSQLæ•°æ®åº“
class CopyToPGSQL():
    """
    ä½¿ç”¨copyæ–¹æ³•å°†æ•°æ®å†™å…¥åˆ°PostgreSQLæ•°æ®åº“
    
    """

    def __init__(self,conn, cur: str = None):
        """
        åˆå§‹åŒ–æ•°æ®åº“è¿æ¥é…ç½®ã€‚
        
        :param conn_str: æ•°æ®åº“è¿æ¥å­—ç¬¦ä¸²
        """
        self.conn = conn
        self.cur = cur

    # æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
    def _check_table(self, schema: str, table: str) -> bool:
        """
        æ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨ã€‚
        
        :param schema: æ¨¡å¼å
        :param table: è¡¨å
        :return: å¦‚æœå­˜åœ¨è¿”å›Trueï¼Œå¦åˆ™False
        """
        querystr = f"""
        SELECT EXISTS (SELECT FROM information_schema.tables WHERE table_schema = '{schema}' AND table_name = '{table}');
        """
        self.cur.execute(querystr)
        exists = cur.fetchone()[0]
        return exists

    # æ ¹æ®dfè·å–pgsqlæ•°æ®ç±»å‹æ˜ å°„å…³ç³»
    def _get_pgsql_type_mapping(self, df: pd.DataFrame):
        """
        æ ¹æ®dfè·å–pgsqlæ•°æ®ç±»å‹æ˜ å°„å…³ç³»
        :return: æ•°æ®ç±»å‹æ˜ å°„å…³ç³»
        """
        def infer_pgsql_type(col):
            # ä½¿ç”¨ pandas çš„å†…éƒ¨ç±»å‹æ¨æ–­
            inferred_type = pd.api.types.infer_dtype(col, skipna=True)

            # æ›´å…¨é¢çš„ç±»å‹æ˜ å°„
            type_mapping = {
                'integer': 'INTEGER',
                'floating': 'FLOAT',
                'boolean': 'BOOLEAN',
                'datetime': 'TIMESTAMP',
                'date': 'DATE',
                'timedelta': 'INTERVAL',
                'string': 'TEXT',
                'empty': 'TEXT',
                'categorical': 'TEXT',
                'list': 'JSON',
                'mixed-list': 'JSON',
                'dict': 'JSON',
                'mixed': 'TEXT',
                'bytes': 'BYTEA',
                'decimal': 'NUMERIC',
                'complex': 'TEXT',
                'interval': 'INTERVAL',
                'path': 'TEXT',
                'url': 'TEXT',
                'email': 'TEXT',
                'ip': 'INET',
                'uuid': 'UUID',
                'geometry': 'GEOMETRY',
            }

            return type_mapping.get(inferred_type, 'TEXT')
        return {col: infer_pgsql_type(df[col]) for col in df.columns}
    # åˆ›å»ºæ•°æ®åº“è¡¨
    def _create_table(self, schema: str, table: str, columns: Dict[str, str], primary_key: Optional[str] = None) -> None:
        """
        åˆ›å»ºæ•°æ®åº“è¡¨
            :param schema: æ¨¡å¼å
            :param table: è¡¨å
            :param columns: åˆ—å®šä¹‰çš„å­—å…¸ï¼Œé”®ä¸ºåˆ—åï¼Œå€¼ä¸ºåˆ—ç±»å‹
            :param primary_key: ä¸»é”®åˆ—åï¼Œé»˜è®¤ä¸ºNone
        """
        cols_sql = ', '.join([f'"{col}" {ctype}' for col, ctype in columns.items()])
        pk_sql = f', PRIMARY KEY ("{primary_key}")' if primary_key else ""
        create_sql = f'CREATE TABLE "{schema}"."{table}" ({cols_sql}{pk_sql});'

        self.cur.execute(create_sql)
        self.conn.commit()
        logger.info(f"åˆ›å»ºè¡¨ '{schema}.{table}' æˆåŠŸ")

    # è·å–æ•°æ®åº“ä¸­è¡¨çš„åˆ—åå’Œç±»å‹
    def _get_db_columns_types(self,schema:str,table:str):
        """
        è·å–æ•°æ®åº“ä¸­è¡¨çš„åˆ—åå’Œç±»å‹
        """
        querystr = f"""
            SELECT
                a.attname AS column_name,
                format_type(a.atttypid, a.atttypmod) AS data_type
            FROM pg_attribute a
            JOIN pg_class t ON a.attrelid = t.oid
            JOIN pg_namespace s ON t.relnamespace = s.oid
            WHERE t.relname = '{table}' AND s.nspname ='{schema}' AND a.attnum > 0 AND NOT a.attisdropped
            ORDER BY a.attnum
        """

        self.cur.execute(querystr)
        result = self.cur.fetchall()

        # æŠŠå…ƒç»„ç»„åˆæˆå­—å…¸
        target_col_types = {row[0]: row[1] for row in result}
        target_columns = list(target_col_types.keys())
        logger.debug(f"ğŸ“Š ç›®æ ‡è¡¨å­—æ®µåŠå…¶ç±»å‹: {target_col_types}")
        return target_col_types
    
    # å‘æ•°æ®åº“è¡¨ä¸­æ·»åŠ åˆ—ã€‚
    def _add_columns(self, schema: str, table: str, columns: dict) -> None:
        """
        å‘æ•°æ®åº“è¡¨ä¸­æ·»åŠ å¤šåˆ—
            :param schema: æ¨¡å¼å
            :param table: è¡¨å
            :param columns: å­—å…¸ï¼Œé”®æ˜¯æ–°åˆ—åï¼Œå€¼æ˜¯æ–°åˆ—çš„æ•°æ®ç±»å‹
        """
        # åˆå§‹åŒ–ä¸€ä¸ªåˆ—è¡¨æ¥å­˜å‚¨æ‰€æœ‰çš„ALTER TABLEè¯­å¥
        alter_table_statements = []

        # éå†ä¼ å…¥çš„columnså­—å…¸ï¼Œä¸ºæ¯ä¸€åˆ—æ„å»ºALTER TABLEè¯­å¥
        for column, column_type in columns.items():
            alter_table_statement = f'ADD COLUMN "{column}" {column_type}'
            alter_table_statements.append(alter_table_statement)

        # å°†æ‰€æœ‰å•ç‹¬çš„ALTER TABLEè¯­å¥ç”¨é€—å·è¿æ¥èµ·æ¥ï¼Œå¹¶é™„åŠ åˆ°ALTER TABLEå‘½ä»¤ä¸Š
        add_cols_sql = f'ALTER TABLE "{schema}"."{table}" {", ".join(alter_table_statements)};'
        
        # æ‰§è¡ŒSQLè¯­å¥å¹¶æäº¤æ›´æ”¹
        self.cur.execute(add_cols_sql)
        self.conn.commit()

        # è®°å½•ä¿¡æ¯
        for column, column_type in columns.items():
            logger.info(f"å‘è¡¨ '{schema}.{table}' æ·»åŠ åˆ— '{column}', ç±»å‹ '{column_type}' æˆåŠŸ")

    # è·å–ç»™å®šè¡¨çš„ä¸»é”®åˆ—è¡¨
    def _get_primary_keys(self, schema: str, table: str) -> List[str]:
        """
        è·å–ç»™å®šè¡¨çš„ä¸»é”®åˆ—è¡¨ã€‚
        
        :param schema: æ¨¡å¼å
        :param table: è¡¨å
        :return: ä¸»é”®åˆ—è¡¨
        """
        query = """
        SELECT a.attname
        FROM   pg_index i
        JOIN   pg_attribute a ON a.attnum = ANY(i.indkey) AND a.attrelid = i.indrelid
        WHERE  i.indrelid = %s::regclass
        AND    i.indisprimary;
        """
        self.cur.execute(query, (f"{schema}.{table}",))
        primary_keys = [row[0] for row in cur.fetchall()]
        if not primary_keys:
            raise ValueError(f"è¡¨ '{schema}.{table}' æ²¡æœ‰ä¸»é”®")
        return primary_keys

    def copyrun(self,
        schema: str, 
        table: str, 
        data: pd.DataFrame, 
        update: Union[bool, List[str]] = False,
        create: bool = False, 
        add_columns: bool = False,
        db_only:bool=False
        ) -> int:
        """
        å°†æ•°æ®ä½¿ç”¨copyæ–¹æ³•å†™å…¥pgsqlæ•°æ®åº“
            :param schema: æ•°æ®åº“æ¨¡å¼
            :param table: æ•°æ®åº“è¡¨å
            :param data: æ•°æ®
            :param update: æ˜¯å¦æ›´æ–°æ•°æ® bool or list
            :param create: æ˜¯å¦åˆ›å»ºè¡¨
            :param add_columns: æ˜¯å¦æ·»åŠ åˆ—
            :param db_only: æ˜¯å¦åªå†™å…¥æ•°æ®åº“ä¸­å­˜åœ¨çš„åˆ—
        """
        start_time = time.time()

        file_headers = list(data.columns)
        db_columns_type = self._get_db_columns_types(schema, table)

        # åˆ›å»ºæ•°æ®åº“è¡¨
        if not db_columns_type:
            if create:
                columns =  self._get_pgsql_type_mapping(data)
                self._create_table(schema, table,columns)
                db_columns_type = self._get_db_columns_types(schema, table)
            else:
                raise Exception(f"âš ï¸ æ•°æ®åº“è¡¨{schema}.{table} ä¸å­˜åœ¨ï¼Œè¯·å…ˆåˆ›å»ºè¡¨")

        db_columns = db_columns_type.keys()

        # æ‰“å°ä¸ä¸€è‡´åˆ—
        missing_in_db = list(set(file_headers) - set(db_columns))
        if missing_in_db:
            logger.info(f"âš ï¸ æ–‡ä»¶ä¸­å­˜åœ¨ä½†æ•°æ®åº“{schema}.{table}ä¸­ä¸å­˜åœ¨çš„åˆ—: {missing_in_db}")
        missing_in_file = list(set(db_columns) - set(file_headers))
        if missing_in_file:
            logger.info(f"âš ï¸ æ•°æ®åº“{schema}.{table}ä¸­å­˜åœ¨ä½†æ–‡ä»¶ä¸å­˜åœ¨çš„åˆ—: {missing_in_file}")

        # æ–°å¢æ•°æ®åº“åˆ—
        if add_columns and missing_in_db:
            addcolumns = self._get_pgsql_type_mapping(data[[missing_in_file]])
            self._add_columns(schema, table, addcolumns)

            # é‡æ–°è·å–åˆ—
            db_columns_type = self.get_db_columns(schema, table)
            db_columns = db_columns_type.keys()

        # åˆ¤æ–­æ˜¯å¦ä»…å†™å…¥æ•°æ®åº“ä¸­å­˜åœ¨çš„åˆ—
        if not add_columns and not db_only and  missing_in_db:
            raise Exception(f"æ•°æ®åº“{schema}.{table}ä¸­ä¸å­˜åœ¨åˆ—: {missing_in_db}")

        # Step 3: ç­›é€‰ CSV ä¸­åªä¿ç•™ç›®æ ‡è¡¨ä¸­å­˜åœ¨çš„åˆ—
        valid_headers = [col for col in file_headers if col in db_columns]

        # ä¿å­˜ä¸´æ—¶æ–‡ä»¶ç”¨äº COPY
        with tempfile.NamedTemporaryFile(mode='w+', delete=False, suffix='.csv', newline='', encoding='utf-8') as tmpfile:
            data[valid_headers].to_csv(tmpfile, index=False, header=True)
            tmpfile_path = tmpfile.name

        # Step 5: åˆ›å»ºä¸´æ—¶è¡¨
        temp_table = "temp_import_table"
        columns_sql = sql.SQL(', ').join(
            sql.SQL("{} {}").format(sql.Identifier(col), sql.SQL(db_columns_type[col]))
            for col in valid_headers
        )
        self.cur.execute(sql.SQL("""
            CREATE TEMP TABLE {temp_table} (
                {columns}
            ) ON COMMIT DROP;
        """).format(temp_table=sql.Identifier(temp_table), columns=columns_sql))

        # COPY æ•°æ®
        with open(tmpfile_path, 'r', encoding='utf-8') as f:
            with self.cur.copy(
                sql.SQL("COPY {table} ({cols}) FROM STDIN WITH (FORMAT csv, HEADER true)").format(
                    table=sql.Identifier(temp_table),
                    cols=sql.SQL(', ').join(sql.Identifier(c) for c in valid_headers)
                )
            ) as copy:
                copy.write(f.read())
                # self.conn.commit()

                # æ‰§è¡Œ COPY åæ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
                error_message = self.conn.info.error_message
                if error_message:  # å¦‚æœæœ‰é”™è¯¯æ¶ˆæ¯
                    raise ValueError(error_message)
                
                # ä½¿ç”¨è¿æ¥çŠ¶æ€æ£€æŸ¥ï¼ˆæ›¿ä»£noticesï¼‰
                if self.conn.info.transaction_status != 0:  # 0 = idle
                    logger.info(f"âœ… æ•°æ®åº“copyäº‹åŠ¡æˆåŠŸï¼Œè€—æ—¶ï¼š{time.time() - start_time:.2f}ç§’")


        os.remove(tmpfile_path)   

        # è·å–ä¸»é”®
        primary_keys = self._get_primary_keys(schema, table)
        if not primary_keys:
            raise ValueError(f"âŒ æ•°æ®åº“è¡¨{schema}.{table} æ²¡æœ‰ä¸»é”®")
        
        # Step 8: æ„å»º INSERT è¯­å¥ï¼Œç¡®ä¿åˆ—é¡ºåºä¸ç›®æ ‡è¡¨å®Œå…¨ä¸€è‡´
        insert_cols = [col for col in db_columns if col in valid_headers]
        select_cols = insert_cols
        full_table_name = sql.SQL("{}.{}").format(sql.Identifier(schema), sql.Identifier(table))
        # æ„å»º INSERT è¯­å¥
        insert_sql = sql.SQL("""
            INSERT INTO {target_table} ({insert_cols})
            SELECT {select_cols} FROM {temp_table}
        """).format(
            target_table=full_table_name,
            insert_cols=sql.SQL(', ').join(sql.Identifier(c) for c in insert_cols),
            select_cols=sql.SQL(', ').join(sql.Identifier(c) for c in select_cols),
            temp_table=sql.Identifier(temp_table),
        )

        # å¦‚æœéœ€è¦å¤„ç†ä¸»é”®å†²çª
        if update is False:
            insert_sql += sql.SQL(" ON CONFLICT ({pk_cols}) DO NOTHING").format(
                pk_cols=sql.SQL(', ').join(sql.Identifier(c) for c in primary_keys)
            )

        elif update is True:
            # æ›´æ–°æ‰€æœ‰éä¸»é”®åˆ—
            update_cols = [c for c in insert_cols if c not in primary_keys]
            insert_sql += sql.SQL("""
                ON CONFLICT ({pk_cols}) DO UPDATE SET
                    {update_cols}
            """).format(
                pk_cols=sql.SQL(', ').join(sql.Identifier(c) for c in primary_keys),
                update_cols=sql.SQL(', ').join(
                    sql.SQL("{col} = EXCLUDED.{col}").format(col=sql.Identifier(c))
                    for c in update_cols
                )
            )
        elif isinstance(update, list):
            # åªæ›´æ–°æŒ‡å®šåˆ—
            update_cols = [c for c in update if c in insert_cols and c not in primary_keys]
            if not update_cols:
                raise ValueError("âŒ update ä¸ºåˆ—è¡¨æ—¶ï¼Œå¿…é¡»åŒ…å«éä¸»é”®çš„å¯æ›´æ–°åˆ—")

            insert_sql += sql.SQL("""
                ON CONFLICT ({pk_cols}) DO UPDATE SET
                    {update_cols}
            """).format(
                pk_cols=sql.SQL(', ').join(sql.Identifier(c) for c in primary_keys),
                update_cols=sql.SQL(', ').join(
                    sql.SQL("{col} = EXCLUDED.{col}").format(col=sql.Identifier(c))
                    for c in update_cols
                )
            )
        else:
            raise ValueError("âŒ update å‚æ•°å¿…é¡»ä¸º bool æˆ– list ç±»å‹")
        
        self.cur.execute(insert_sql)
        self.conn.commit()
        result_count = self.cur.rowcount

        end_time = time.time() 
        logger.info(f"âœ… æ•°æ®å†™å…¥å®Œæˆï¼Œè€—æ—¶ï¼š{end_time - start_time:.2f}ç§’, å†™å…¥è¡Œæ•°ï¼š{result_count}")


if __name__ == '__main__':
    import psycopg
    from psycopg import sql
    # from psycopg import sql, extensions

    PGLINK_URL = 'dbname =shop_data host = 127.0.0.1 port = 5432 user = postgres password = manji1688'
    conn = psycopg.connect(PGLINK_URL)
    cur = conn.cursor()
    
    schema = 'douyin_shop'
    table = 'dy_dp_dd_ddgl_bzbb_copy12'
    data = pd.read_csv(r"C:\Users\manji\Downloads\æŠ–éŸ³è®¢å•\1752550967_6e03f011910e5e4414e47f8f6dc317c8OtkVGLHs.csv", dtype=str)
    data = DataSet.clean_data(data,add_time=True)
    print(f"æ•°æ®æ¸…æ´—å®Œæˆï¼Œæ•°æ®è¡Œæ•°: {len(data)}")
    
    obj =  CopyToPGSQL(conn,cur)
    # obj._get_db_columns_types(table,schema)
    # obj._check_table(schema,table)
    obj.copyrun(schema,table,data,update=True,create=True,db_only=True)

    cur.close()
    conn.close()