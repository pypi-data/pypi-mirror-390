# Copyright 2025 - Pruna AI GmbH. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
from collections.abc import Iterable
from typing import Any

from pruna.algorithms.base.pruna_base import PrunaAlgorithmBase
from pruna.algorithms.base.tags import AlgorithmTag as tags
from pruna.config.smash_config import SmashConfigPrefixWrapper
from pruna.engine.model_checks import has_fused_attention_processor, is_diffusers_model
from pruna.engine.save import SAVE_FUNCTIONS
from pruna.engine.utils import ModelContext


class QKVFusing(PrunaAlgorithmBase):
    """
    Implement QKV factorizing for diffusers models.

    QKV factorizing fuses the QKV matrices of the denoiser model into a single matrix,
    reducing the number of operations. In the attention layer, we can compute the q, k, v signals
    all at once: the matrix multiplication involve a larger matrix but we compute one operation instead of three.
    """

    algorithm_name: str = "qkv_diffusers"
    group_tags: list[str] = [tags.FACTORIZER]
    references: dict[str, str] = {
        "BFL": "https://github.com/black-forest-labs/flux/",
        "Github": "https://github.com/huggingface/diffusers/pull/9185",
    }
    save_fn: SAVE_FUNCTIONS = SAVE_FUNCTIONS.save_before_apply
    tokenizer_required: bool = False
    processor_required: bool = False
    runs_on: list[str] = ["cpu", "cuda", "accelerate"]
    dataset_required: bool = False
    compatible_after: Iterable[str] = [
        "diffusers_int8",
        "hqq_diffusers",
        "quanto",
        "torchao",
        "deepcache",
        "fora",
        "torch_compile",
    ]

    def model_check_fn(self, model: Any) -> bool:
        """
        Check if the model is a pipeline with unet/transformer denoiser compatible with a fused attention processor.

        Parameters
        ----------
        model : Any
            The model to check.

        Returns
        -------
        bool
            True if the model has a fused attention processor, False otherwise.
        """
        if is_diffusers_model(model):
            return has_fused_attention_processor(model)
        return False

    def _apply(self, model: Any, smash_config: SmashConfigPrefixWrapper) -> Any:
        """
        Fuse the QKV matrices of the denoiser model.

        Parameters
        ----------
        model : Any
            The model to fuse.
        smash_config : SmashConfigPrefixWrapper
            The configuration for the fusion.

        Returns
        -------
        Any
            The fused model.
        """
        # Use context manager to handle the model vs working_model.
        with ModelContext(model) as (mc, working_model):
            # only this line thanks to https://github.com/huggingface/diffusers/pull/9185
            working_model.fuse_qkv_projections()
            # adding a single attention processor instance for all layers for torch compile compatibility
            # Get a random processor instance to initialize the new single processor
            random_key = next(iter(working_model.attn_processors.keys()))
            processor = working_model.attn_processors[random_key]
            working_model.set_attn_processor(processor)

            mc.update_working_model(working_model)

        return mc.get_updated_model()
