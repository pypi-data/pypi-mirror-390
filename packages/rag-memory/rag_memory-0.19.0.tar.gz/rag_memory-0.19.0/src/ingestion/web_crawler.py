"""Web crawler for documentation ingestion using Crawl4AI."""

import logging
import os
import sys
import uuid
from contextlib import contextmanager
from datetime import datetime, timezone
from typing import Dict, List, Optional, Set
from urllib.parse import urlparse, urljoin

from crawl4ai import (
    AsyncWebCrawler,
    BFSDeepCrawlStrategy,
    BrowserConfig,
    CacheMode,
    CrawlerRunConfig,
)
from crawl4ai.content_filter_strategy import PruningContentFilter
from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator

from src.ingestion.models import CrawlError, CrawlResult

logger = logging.getLogger(__name__)


@contextmanager
def suppress_crawl4ai_stdout():
    """
    Context manager to suppress Crawl4AI's stdout logging.

    Crawl4AI writes progress messages like [FETCH], [SCRAPE], [COMPLETE] directly
    to stdout, which interferes with MCP's JSON-RPC protocol over stdio transport.

    This redirects stdout to stderr temporarily during crawl operations.
    """
    original_stdout = sys.stdout
    try:
        # Redirect stdout to stderr (or to devnull if you want to suppress completely)
        sys.stdout = sys.stderr
        yield
    finally:
        # Restore original stdout
        sys.stdout = original_stdout


class WebCrawler:
    """Crawls web pages for documentation ingestion."""

    def __init__(self, headless: bool = True, verbose: bool = False):
        """
        Initialize web crawler.

        Args:
            headless: Run browser in headless mode (default: True)
            verbose: Enable verbose logging (default: False)
        """
        self.headless = headless
        self.verbose = verbose
        self.visited_urls: Set[str] = set()  # Track visited URLs to prevent duplicates

        # Browser configuration
        # Extra args to prevent Playwright multi-process deadlocks in Docker
        # See: https://github.com/microsoft/playwright/issues/4761
        self.browser_config = BrowserConfig(
            headless=headless,
            verbose=verbose,
            extra_args=[
                "--disable-dev-shm-usage",     # Don't use /dev/shm (shared memory)
                "--no-sandbox",                 # Disable Chrome sandbox (required in Docker)
                "--single-process",             # CRITICAL - force single process to prevent deadlock
                "--no-zygote",                  # CRITICAL - prevent process forking
                "--disable-features=IsolateOrigins,site-per-process",  # Disable multi-process features
            ],
        )

        # Content filter to remove navigation noise and reduce document-to-document relationships
        # in knowledge graph extraction. Uses algorithmic text density + link density scoring.
        # Issue: Web pages contain navigation elements (links, breadcrumbs, sidebars) that confuse
        # Graphiti's LLM-based entity extraction, causing extraction of document structure instead of
        # semantic relationships. PruningContentFilter removes ~50-55% of navigation clutter while
        # preserving 100% of valuable documentation content.
        # See: Issue 10 in TASKS_AND_ISSUES.md for detailed analysis
        content_filter = PruningContentFilter(
            threshold=0.40,           # Lower threshold (0.35-0.40) more permissive for documentation sites
            threshold_type="fixed",   # Fixed mode is more predictable than dynamic
            min_word_threshold=5      # Keep small but meaningful content blocks
        )

        # Wrap filter in markdown generator (filter must be passed through generator, not directly)
        self.markdown_generator = DefaultMarkdownGenerator(
            content_filter=content_filter
        )

        # Crawler run configuration (for single-page crawls)
        self.crawler_config = CrawlerRunConfig(
            markdown_generator=self.markdown_generator,  # Pass generator with filter to clean content
            cache_mode=CacheMode.BYPASS,  # Always fetch fresh content
            word_count_threshold=10,  # Minimum words to consider valid content
            excluded_tags=[
                "nav", "footer", "header", "aside",  # Remove navigation
                "form", "iframe", "script", "style",  # Remove interactive/styling elements
                "noscript", "meta", "link"  # Remove non-content elements
            ],
            remove_overlay_elements=True,  # Remove popups/modals
        )

        logger.info(f"WebCrawler initialized (headless={headless}, verbose={verbose})")
        logger.info(f"Content filtering enabled: PruningContentFilter (threshold=0.40, fixed mode)")

    async def crawl_page(self, url: str, crawl_root_url: Optional[str] = None) -> CrawlResult:
        """
        Crawl a single web page.

        Args:
            url: URL to crawl
            crawl_root_url: Root URL for the crawl session (defaults to url)

        Returns:
            CrawlResult with page content and metadata
        """
        if not crawl_root_url:
            crawl_root_url = url

        crawl_timestamp = datetime.now(timezone.utc)
        crawl_session_id = str(uuid.uuid4())

        logger.info(f"Crawling page: {url}")

        try:
            with suppress_crawl4ai_stdout():
                async with AsyncWebCrawler(config=self.browser_config) as crawler:
                    result = await crawler.arun(
                        url=url,
                        config=self.crawler_config,
                    )

                    if not result.success:
                        error = CrawlError(
                            url=url,
                            error_type="crawl_failed",
                            error_message=result.error_message or "Unknown error",
                            timestamp=crawl_timestamp,
                            status_code=result.status_code,
                        )
                        logger.error(f"Failed to crawl {url}: {error.error_message}")
                        return CrawlResult(
                            url=url,
                            content="",
                            metadata={},
                            success=False,
                            error=error,
                        )

                    # Extract metadata
                    metadata = self._build_metadata(
                        url=url,
                        crawl_root_url=crawl_root_url,
                        crawl_timestamp=crawl_timestamp,
                        crawl_session_id=crawl_session_id,
                        crawl_depth=0,  # Single page = depth 0
                        result=result,
                    )

                    # Use filtered markdown output to reduce navigation noise
                    # fit_markdown is ONLY populated when PruningContentFilter is used
                    # Falls back to markdown_with_citations if filtered version not available
                    content = result.markdown.fit_markdown or result.markdown.markdown_with_citations

                    logger.info(
                        f"Successfully crawled {url} ({len(content)} chars, "
                        f"status={result.status_code})"
                    )

                    return CrawlResult(
                        url=url,
                        content=content,
                        metadata=metadata,
                        success=True,
                        links_found=result.links.get("internal", []) if result.links else [],
                    )

        except Exception as e:
            error = CrawlError(
                url=url,
                error_type=type(e).__name__,
                error_message=str(e),
                timestamp=crawl_timestamp,
            )
            logger.exception(f"Exception while crawling {url}")
            return CrawlResult(
                url=url,
                content="",
                metadata={},
                success=False,
                error=error,
            )

    def _build_metadata(
        self,
        url: str,
        crawl_root_url: str,
        crawl_timestamp: datetime,
        crawl_session_id: str,
        crawl_depth: int,
        result,
        parent_url: Optional[str] = None,
    ) -> Dict:
        """
        Build metadata dictionary for a crawled page.

        Args:
            url: Page URL
            crawl_root_url: Root URL of the crawl
            crawl_timestamp: Timestamp of the crawl
            crawl_session_id: Unique session ID
            crawl_depth: Depth level in the crawl tree
            result: Crawl4AI result object
            parent_url: Optional parent page URL

        Returns:
            Metadata dictionary
        """
        parsed = urlparse(url)

        metadata = {
            # PAGE IDENTITY
            "source": url,
            "content_type": "web_page",
            # CRAWL CONTEXT (for re-crawl management - CRITICAL)
            "crawl_root_url": crawl_root_url,
            "crawl_timestamp": crawl_timestamp.isoformat(),
            "crawl_session_id": crawl_session_id,
            "crawl_depth": crawl_depth,
            # PAGE METADATA
            "title": result.metadata.get("title", ""),
            "description": result.metadata.get("description", ""),
            "domain": parsed.netloc,
            # OPTIONAL BUT USEFUL
            "language": result.metadata.get("language", "en"),
            "status_code": result.status_code,
            "content_length": len(result.markdown.raw_markdown),
            "crawler_version": "crawl4ai-0.7.4",
        }

        if parent_url:
            metadata["parent_url"] = parent_url

        return metadata

    async def crawl_with_depth(
        self,
        url: str,
        max_depth: int = 1,
        max_pages: int = float('inf'),
        crawl_root_url: Optional[str] = None,
    ) -> List[CrawlResult]:
        """
        Crawl a website following links up to max_depth and max_pages.

        Uses BFSDeepCrawlStrategy for breadth-first traversal.

        Args:
            url: Starting URL
            max_depth: Maximum depth to crawl (0 = only starting page, 1 = starting + direct links, etc.)
            max_pages: Maximum number of pages to crawl (default: unlimited)
            crawl_root_url: Root URL for the crawl session (defaults to url)

        Returns:
            List of CrawlResult objects, one per page crawled (limited to max_pages)
        """
        if not crawl_root_url:
            crawl_root_url = url

        crawl_timestamp = datetime.now(timezone.utc)
        crawl_session_id = str(uuid.uuid4())

        logger.info(
            f"Starting deep crawl from {url} (max_depth={max_depth}, max_pages={max_pages}, session={crawl_session_id})"
        )

        results: List[CrawlResult] = []
        self.visited_urls.clear()  # Reset visited tracking for this crawl session

        # Configure BFSDeepCrawlStrategy with max_pages limit
        crawl_strategy = BFSDeepCrawlStrategy(max_depth=max_depth, max_pages=max_pages)

        # Multi-page crawler config
        # stream=True processes pages one-at-a-time to prevent browser context leaks in Docker
        # session_id ensures clean browser session management
        deep_crawler_config = CrawlerRunConfig(
            markdown_generator=self.markdown_generator,  # Use PruningContentFilter to remove navigation noise
            cache_mode=CacheMode.BYPASS,
            word_count_threshold=10,
            excluded_tags=["nav", "footer", "header", "aside"],
            remove_overlay_elements=True,
            deep_crawl_strategy=crawl_strategy,
            session_id=crawl_session_id,  # Force new session for proper cleanup
            stream=True,  # Process pages individually (fixes Docker browser leak in v0.6.0)
        )

        try:
            with suppress_crawl4ai_stdout():
                async with AsyncWebCrawler(config=self.browser_config) as crawler:
                    # Start crawling
                    # stream=True returns async generator, otherwise returns list
                    crawl_results = await crawler.arun(
                        url=url,
                        config=deep_crawler_config,
                    )

                    # Process each crawled page
                    # stream=True returns async generator, we need to iterate with async for
                    depth = 0
                    async for crawl_result in crawl_results:
                        page_url = crawl_result.url
                        self.visited_urls.add(page_url)

                        if crawl_result.success:
                            # Determine parent URL (first page has no parent)
                            parent_url = None
                            if depth > 0:
                                # Parent is the starting URL for depth 1, and could be any previously crawled page
                                # For simplicity, we'll use the starting URL as parent for all depth=1 pages
                                parent_url = url

                            metadata = self._build_metadata(
                                url=page_url,
                                crawl_root_url=crawl_root_url,
                                crawl_timestamp=crawl_timestamp,
                                crawl_session_id=crawl_session_id,
                                crawl_depth=depth,
                                result=crawl_result,
                                parent_url=parent_url,
                            )
                            results.append(
                                CrawlResult(
                                    url=page_url,
                                    content=crawl_result.markdown.fit_markdown or crawl_result.markdown.raw_markdown,  # Use filtered if available, fallback to raw
                                    metadata=metadata,
                                    success=True,
                                    links_found=(
                                        crawl_result.links.get("internal", [])
                                        if crawl_result.links
                                        else []
                                    ),
                                )
                            )
                            logger.info(
                                f"Successfully crawled page {page_url} (depth={depth}, {len(crawl_result.markdown.raw_markdown)} chars)"
                            )
                        else:
                            error = CrawlError(
                                url=page_url,
                                error_type="crawl_failed",
                                error_message=crawl_result.error_message or "Unknown error",
                                timestamp=crawl_timestamp,
                                status_code=crawl_result.status_code,
                            )
                            results.append(
                                CrawlResult(
                                    url=page_url,
                                    content="",
                                    metadata={},
                                    success=False,
                                    error=error,
                                )
                            )
                            logger.warning(f"Failed to crawl {page_url}: {error.error_message}")

                        # Increment depth for next page
                        depth += 1

                    logger.info(
                        f"Deep crawl completed: {len(results)} pages crawled, "
                        f"{sum(1 for r in results if r.success)} successful"
                    )
                    return results

        except Exception as e:
            error = CrawlError(
                url=url,
                error_type=type(e).__name__,
                error_message=str(e),
                timestamp=crawl_timestamp,
            )
            logger.exception(f"Exception during deep crawl from {url}")
            # Return whatever we managed to crawl plus the error
            if not results:
                results.append(
                    CrawlResult(
                        url=url,
                        content="",
                        metadata={},
                        success=False,
                        error=error,
                    )
                )
            return results


async def crawl_single_page(url: str, headless: bool = True, verbose: bool = False) -> CrawlResult:
    """
    Convenience function to crawl a single page.

    Args:
        url: URL to crawl
        headless: Run browser in headless mode
        verbose: Enable verbose logging

    Returns:
        CrawlResult with page content and metadata
    """
    crawler = WebCrawler(headless=headless, verbose=verbose)
    return await crawler.crawl_page(url)
