import os
import platform
import shutil
from abc import ABC, abstractmethod
from collections.abc import Callable
from pathlib import Path
from typing import Literal, override
from uuid import uuid4

import boto3
import pandas as pd
from botocore.config import Config
from tenacity import retry, stop_after_attempt, wait_fixed
from tqdm import tqdm

from .dataframes import concat_on_index
from .p_tqdm import p_imap


class RemoteStore(ABC):
    @abstractmethod
    def upload_folder(self, local_folder: Path | str, bucket_name: str) -> None:
        raise NotImplementedError

    @abstractmethod
    def upload_file(
        self,
        file_path: Path,
        bucket_name: str,
        output_file_name: str | None = None,
    ) -> None:
        raise NotImplementedError

    @abstractmethod
    def download_folder(
        self,
        local_folder: str | Path,
        bucket_name: str,
        predicate: Callable | None = None,
    ) -> None:
        raise NotImplementedError

    @abstractmethod
    def delete_all_files(self, bucket_name: str) -> None:
        raise NotImplementedError

    @abstractmethod
    def list_objects(
        self, bucket_name: str, predicate: Callable | None = None
    ) -> list[tuple[str, str]]:
        """List objects in a bucket and return tuples of (object_key, presigned_url).

        Args:
            bucket_name: Name of the S3 bucket
            predicate: Optional callable to filter objects. Should accept (obj_key, url) tuple.

        Returns:
            List of tuples containing (object_key, presigned_url)
        """
        raise NotImplementedError

    def upload_dataframe(self, df: pd.DataFrame, filename: str, bucket_name: str):
        local_folder = Path(f".temp/{uuid4()}")
        if local_folder.exists():
            shutil.rmtree(local_folder)
        local_folder.mkdir(parents=True)
        full_path = local_folder.joinpath(filename)
        if filename.endswith(".csv"):
            df.to_csv(full_path)
        elif filename.endswith(".json"):
            df.to_json(full_path)
        elif filename.endswith(".parquet"):
            df.to_parquet(full_path)
        self.upload_file(full_path, bucket_name)
        shutil.rmtree(local_folder)

    def read_remote_folder_as_dataframe(
        self,
        bucket_name: str,
        predicate: Callable | None = None,
        extension: Literal["csv", "json", "parquet"] = "csv",
        add_filename_as_column: bool = False,
    ) -> pd.DataFrame:
        local_folder = Path(
            r"C:\Windows\Temp\{uuid4()}"
            if platform.system() == "Windows"
            else f".temp/{uuid4()}"  # Assumes iOS / Linux
        )
        if local_folder.exists():
            shutil.rmtree(local_folder)
        self.download_folder(
            local_folder=local_folder, bucket_name=bucket_name, predicate=predicate
        )

        df = read_folder_as_dataframe(local_folder, extension, add_filename_as_column)
        shutil.rmtree(local_folder)

        return df


class S3RemoteStore(RemoteStore):
    url: str
    key_id: str
    secret: str

    def __init__(self, url: str, key_id: str, secret: str) -> None:
        self.url = url
        self.key_id = key_id
        self.secret = secret

        self.api = boto3.client(
            "s3",
            endpoint_url=url,
            aws_access_key_id=key_id,
            aws_secret_access_key=secret,
            config=Config(signature_version="s3v4", region_name="auto"),
        )
        self.resource = boto3.resource(
            "s3",
            endpoint_url=url,
            aws_access_key_id=key_id,
            aws_secret_access_key=secret,
            config=Config(signature_version="s3v4", region_name="auto"),
        )

    @override
    def upload_folder(
        self,
        local_folder: Path | str,
        bucket_name: str,
        destination_folder: str | None = None,
    ) -> None:
        local_folder = Path(local_folder)
        for file in tqdm(Path(local_folder).iterdir()):
            target_file_name = local_folder.joinpath(file)
            self.api.upload_file(
                target_file_name.as_posix(),
                bucket_name,
                file.name
                if destination_folder is None
                else f"{destination_folder}/{file.name}",
            )

    @override
    def upload_file(
        self,
        file_path: Path,
        bucket_name: str,
        output_file_name: str | None = None,
    ) -> None:
        if output_file_name is None:
            output_file_name = file_path.name
        self.api.upload_file(file_path.as_posix(), bucket_name, output_file_name)

    def download_file(
        self,
        filename: str,
        bucket_name: str,
        output_path: Path | None = None,
    ) -> None:
        output_path.parent.mkdir(parents=True, exist_ok=True)
        self.api.download_file(bucket_name, filename, output_path.as_posix())

    def download_folder(
        self,
        local_folder: str | Path,
        bucket_name: str,
        predicate: Callable | None = None,
    ) -> None:
        local_folder = Path(local_folder)
        local_folder.mkdir(parents=True, exist_ok=True)

        bucket = self.resource.Bucket(bucket_name)

        urls = [
            (
                obj.key,
                bucket.meta.client.generate_presigned_url(
                    "get_object",
                    Params={"Bucket": bucket_name, "Key": obj.key},
                ),
            )
            for obj in bucket.objects.all()
        ]
        if predicate is not None:
            urls = list(filter(predicate, urls))

        def download(tup):
            @retry(stop=stop_after_attempt(3), wait=wait_fixed(10))
            def execute():
                obj_key, url = tup
                local_file_path = local_folder.joinpath(obj_key)
                local_file_path.parent.mkdir(parents=True, exist_ok=True)

                if platform.system() == "Windows":
                    wget_cmd = f'wget "{url}" -O "{local_file_path}.tmp" -q'
                    move_cmd = f'move "{local_file_path}.tmp" "{local_file_path}"'
                else:  # Assumes iOS or Linux
                    wget_cmd = f"wget '{url}' -O '{local_file_path}.tmp' -q"
                    move_cmd = f"mv '{local_file_path}.tmp' '{local_file_path}'"

                return_code = os.system(wget_cmd)
                os.system(move_cmd)

                if return_code != 0:
                    raise RuntimeError(f"Failed to download {url}")

            return execute()

        return_codes = p_imap(download, urls)

        if any(return_codes):
            raise RuntimeError(f"Failed to download files from {bucket_name}")

    def delete_all_files(self, bucket_name: str) -> None:
        bucket = self.resource.Bucket(bucket_name)
        bucket.objects.all().delete()

    def list_objects(
        self, bucket_name: str, predicate: Callable | None = None
    ) -> list[tuple[str, str]]:
        bucket = self.resource.Bucket(bucket_name)

        urls = [
            (
                obj.key,
                bucket.meta.client.generate_presigned_url(
                    "get_object",
                    Params={"Bucket": bucket_name, "Key": obj.key},
                ),
            )
            for obj in bucket.objects.all()
        ]

        if predicate is not None:
            urls = list(filter(predicate, urls))

        return urls


def download(tup: tuple[str, str, str]):
    header = tup[0]
    url = tup[1]
    output_path = tup[2]

    @retry(stop=stop_after_attempt(3), wait=wait_fixed(10))
    def execute():
        return_code = os.system(f"curl -L -o {output_path} '{url}' -H '{header}'")
        if return_code != 0:
            raise RuntimeError(f"Failed to download {url}")

    return execute()


def read_folder_as_dataframe(
    folder_path: Path,
    extension: Literal["csv", "json", "parquet"],
    add_filename_as_column: bool,
) -> pd.DataFrame:
    def resolve_extension(file_path: Path) -> pd.DataFrame:
        if extension == "csv":
            return pd.read_csv(file_path, index_col=0, parse_dates=True)
        if extension == "json":
            return pd.read_json(file_path)
        if extension == "parquet":
            return pd.read_parquet(file_path)
        raise ValueError(f"Unknown extension type: {extension}")

    def add_name_as_column(df: pd.DataFrame, filename: str) -> pd.DataFrame:
        if add_filename_as_column:
            return df.assign(filename=filename)

        return df

    return concat_on_index(
        [
            add_name_as_column(resolve_extension(file_path), file_path.stem)
            for file_path in sorted(
                folder_path.glob(f"*.{extension}"), key=lambda x: x.name
            )
        ]
    )
