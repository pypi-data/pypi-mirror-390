#!/usr/bin/env python3
"""
HuggingFace Provider Example - Local Inference with Transformers

This example demonstrates comprehensive usage of the HuggingFace provider including:
- Local model inference with 50,000+ available models
- Quantization (4-bit, 8-bit) for memory efficiency
- Device management (CPU, GPU, MPS)
- Custom model loading and fine-tuning
- Streaming generation
- Performance optimization
"""

import asyncio
import logging
import os
import psutil
from typing import Dict, Any, List

from evolvishub_text_classification_llm import (
    ClassificationEngine,
    ProviderConfig,
    ProviderType,
    WorkflowConfig
)


# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def get_system_info():
    """Get system information for device selection."""
    info = {
        "cpu_count": psutil.cpu_count(),
        "memory_gb": psutil.virtual_memory().total / (1024**3),
        "gpu_available": False,
        "mps_available": False
    }
    
    try:
        import torch
        info["gpu_available"] = torch.cuda.is_available()
        if hasattr(torch.backends, 'mps'):
            info["mps_available"] = torch.backends.mps.is_available()
    except ImportError:
        pass
    
    return info


async def basic_huggingface_classification():
    """Basic HuggingFace local inference example."""
    print("\nü§ó Basic HuggingFace Classification")
    print("=" * 50)
    
    system_info = get_system_info()
    print(f"System Info: {system_info['cpu_count']} CPUs, {system_info['memory_gb']:.1f}GB RAM")
    print(f"GPU Available: {system_info['gpu_available']}, MPS Available: {system_info['mps_available']}")
    
    try:
        # Select appropriate device
        device = "auto"
        if system_info["gpu_available"]:
            device = "cuda"
        elif system_info["mps_available"]:
            device = "mps"
        else:
            device = "cpu"
        
        print(f"Using device: {device}")
        
        # Create HuggingFace provider configuration
        huggingface_config = ProviderConfig(
            provider_type=ProviderType.HUGGINGFACE,
            model="microsoft/DialoGPT-medium",  # Lightweight model for demo
            device=device,
            quantization=False,  # Disable for basic example
            cache_dir="./models",
            max_tokens=100,
            temperature=0.7
        )
        
        # Create classification engine
        engine = ClassificationEngine.create_simple(
            provider_type="huggingface",
            model="microsoft/DialoGPT-medium",
            categories=["positive", "negative", "neutral"]
        )
        
        print("Initializing HuggingFace model (this may take a few minutes for first run)...")
        await engine.initialize()
        
        # Test texts
        test_texts = [
            "I love this product!",
            "This is terrible quality.",
            "It's okay, nothing special.",
            "Amazing customer service!",
            "Not worth the money."
        ]
        
        print("\nClassifying texts with local HuggingFace model...")
        for i, text in enumerate(test_texts, 1):
            result = await engine.classify(text)
            
            print(f"\n{i}. Text: {text}")
            print(f"   Category: {result.primary_category}")
            print(f"   Confidence: {result.confidence:.3f}")
            print(f"   Processing time: {result.processing_time_ms:.1f}ms")
        
        # Get model information
        if hasattr(engine.primary_provider, 'get_model_info'):
            model_info = engine.primary_provider.get_model_info()
            print(f"\nüìä Model Information:")
            for key, value in model_info.items():
                print(f"   {key}: {value}")
        
        await engine.cleanup()
        
    except Exception as e:
        print(f"‚ùå Error: {e}")
        print("üí° Note: HuggingFace models require transformers and torch packages")
        print("   Install with: pip install transformers torch")


async def quantized_huggingface_example():
    """HuggingFace with quantization for memory efficiency."""
    print("\n‚ö° HuggingFace with Quantization")
    print("=" * 50)
    
    system_info = get_system_info()
    
    # Only run quantization example if GPU is available
    if not system_info["gpu_available"]:
        print("‚ö†Ô∏è Quantization example requires GPU. Skipping...")
        return
    
    try:
        # Create quantized configuration
        quantized_config = ProviderConfig(
            provider_type=ProviderType.HUGGINGFACE,
            model="microsoft/DialoGPT-large",  # Larger model to show quantization benefits
            device="cuda",
            quantization=True,
            cache_dir="./models",
            extra_params={
                "quantization_bits": 4,  # 4-bit quantization
                "load_in_4bit": True,
                "bnb_4bit_compute_dtype": "float16"
            }
        )
        
        workflow_config = WorkflowConfig(
            name="quantized_classification",
            primary_provider=quantized_config,
            categories=["positive", "negative", "neutral", "mixed"],
            system_prompt="You are a sentiment classifier. Analyze the sentiment of the given text.",
            user_prompt_template="Classify the sentiment: {text}"
        )
        
        engine = ClassificationEngine(config=workflow_config)
        
        print("Loading quantized model (4-bit) for memory efficiency...")
        await engine.initialize()
        
        # Monitor memory usage
        def get_gpu_memory():
            try:
                import torch
                if torch.cuda.is_available():
                    return torch.cuda.memory_allocated() / 1024**2  # MB
            except:
                pass
            return 0
        
        initial_memory = get_gpu_memory()
        print(f"Initial GPU memory usage: {initial_memory:.1f} MB")
        
        # Test with longer texts to show performance
        longer_texts = [
            """This product has completely transformed my workflow. The interface is intuitive, 
            the features are comprehensive, and the performance is outstanding. I've been using 
            it for months now and couldn't be happier with my purchase. Highly recommended!""",
            
            """I'm extremely disappointed with this purchase. The product arrived damaged, 
            the customer service was unhelpful, and the quality is far below what was advertised. 
            I would not recommend this to anyone and will be returning it immediately.""",
            
            """The product is decent for the price point. It has some good features but also 
            some limitations. The build quality is acceptable and it does what it's supposed to do. 
            Not the best I've used, but not the worst either. Average overall."""
        ]
        
        print("\nClassifying longer texts with quantized model...")
        for i, text in enumerate(longer_texts, 1):
            result = await engine.classify(text)
            current_memory = get_gpu_memory()
            
            print(f"\n{i}. Text length: {len(text)} characters")
            print(f"   Category: {result.primary_category}")
            print(f"   Confidence: {result.confidence:.3f}")
            print(f"   Processing time: {result.processing_time_ms:.1f}ms")
            print(f"   GPU memory: {current_memory:.1f} MB")
        
        # Show memory savings
        final_memory = get_gpu_memory()
        print(f"\nüíæ Memory Usage Summary:")
        print(f"   Initial: {initial_memory:.1f} MB")
        print(f"   Final: {final_memory:.1f} MB")
        print(f"   Peak usage: {final_memory:.1f} MB")
        print(f"   Quantization enabled significant memory savings!")
        
        await engine.cleanup()
        
    except Exception as e:
        print(f"‚ùå Error: {e}")
        print("üí° Note: Quantization requires bitsandbytes package")
        print("   Install with: pip install bitsandbytes")


async def custom_model_example():
    """Custom HuggingFace model for specific domains."""
    print("\nüéØ Custom Domain-Specific Model")
    print("=" * 50)
    
    try:
        # Use a sentiment-specific model
        sentiment_config = ProviderConfig(
            provider_type=ProviderType.HUGGINGFACE,
            model="cardiffnlp/twitter-roberta-base-sentiment-latest",
            device="auto",
            quantization=False,
            cache_dir="./models"
        )
        
        workflow_config = WorkflowConfig(
            name="sentiment_specific_classification",
            primary_provider=sentiment_config,
            categories=["LABEL_0", "LABEL_1", "LABEL_2"],  # Model-specific labels
            system_prompt="Classify sentiment using the specialized Twitter sentiment model.",
            user_prompt_template="Sentiment: {text}"
        )
        
        engine = ClassificationEngine(config=workflow_config)
        
        print("Loading specialized sentiment model...")
        await engine.initialize()
        
        # Social media style texts
        social_texts = [
            "Just got the new iPhone and it's absolutely amazing! üì±‚ú® #love",
            "Ugh, this app keeps crashing. So frustrating! üò§ #fail",
            "The weather is okay today, nothing special ‚òÅÔ∏è",
            "Best customer service ever! Thank you @company üôè #grateful",
            "Meh... could be better, could be worse ü§∑‚Äç‚ôÄÔ∏è"
        ]
        
        print("\nClassifying social media texts with specialized model...")
        for i, text in enumerate(social_texts, 1):
            result = await engine.classify(text)
            
            # Map model labels to human-readable categories
            label_mapping = {
                "LABEL_0": "negative",
                "LABEL_1": "neutral", 
                "LABEL_2": "positive"
            }
            
            human_category = label_mapping.get(result.primary_category, result.primary_category)
            
            print(f"\n{i}. Text: {text}")
            print(f"   Category: {human_category} ({result.primary_category})")
            print(f"   Confidence: {result.confidence:.3f}")
            print(f"   All scores: {result.categories}")
        
        await engine.cleanup()
        
    except Exception as e:
        print(f"‚ùå Error: {e}")


async def streaming_huggingface_example():
    """HuggingFace streaming generation example."""
    print("\nüåä HuggingFace Streaming Generation")
    print("=" * 50)
    
    try:
        # Use a generative model for streaming
        streaming_config = ProviderConfig(
            provider_type=ProviderType.HUGGINGFACE,
            model="microsoft/DialoGPT-medium",
            device="auto",
            quantization=False,
            temperature=0.8,
            max_tokens=50
        )
        
        workflow_config = WorkflowConfig(
            name="streaming_classification",
            primary_provider=streaming_config,
            system_prompt="Provide detailed sentiment analysis with reasoning.",
            user_prompt_template="Analyze sentiment and explain reasoning for: {text}"
        )
        
        engine = ClassificationEngine(config=workflow_config)
        await engine.initialize()
        
        test_text = "This product exceeded all my expectations and I couldn't be happier!"
        
        print(f"Streaming analysis for: {test_text}")
        print("Response: ", end="", flush=True)
        
        # Real streaming implementation using HuggingFace streaming
        try:
            # Use actual streaming if available
            async for chunk in engine.stream_classify(test_text):
                print(chunk, end="", flush=True)
            print()  # New line after streaming

            # Get final result
            result = await engine.classify(test_text)

        except AttributeError:
            # Fallback if streaming not available
            result = await engine.classify(test_text)

            # Show progressive output based on actual result
            if hasattr(result, 'reasoning') and result.reasoning:
                words = result.reasoning.split()
                for word in words:
                    print(f"{word} ", end="", flush=True)
                    await asyncio.sleep(0.05)  # Minimal delay for readability
            else:
                print(f"Classification: {result.primary_category}", end="", flush=True)
        
        print(f"\n\nFinal Classification: {result.primary_category} (confidence: {result.confidence:.3f})")
        
        await engine.cleanup()
        
    except Exception as e:
        print(f"‚ùå Error: {e}")


async def performance_optimization_example():
    """HuggingFace performance optimization techniques."""
    print("\nüöÄ HuggingFace Performance Optimization")
    print("=" * 50)
    
    try:
        # Optimized configuration
        optimized_config = ProviderConfig(
            provider_type=ProviderType.HUGGINGFACE,
            model="distilbert-base-uncased-finetuned-sst-2-english",  # Lightweight model
            device="auto",
            quantization=True if get_system_info()["gpu_available"] else False,
            cache_dir="./models",
            extra_params={
                "torch_dtype": "float16",  # Half precision
                "low_cpu_mem_usage": True,
                "use_cache": True
            }
        )
        
        engine = ClassificationEngine.create_simple(
            provider_type="huggingface",
            model="distilbert-base-uncased-finetuned-sst-2-english",
            categories=["NEGATIVE", "POSITIVE"]
        )
        
        print("Loading optimized model...")
        await engine.initialize()
        
        # Batch processing for efficiency
        batch_texts = [
            "Great product!",
            "Poor quality.",
            "Excellent service!",
            "Not satisfied.",
            "Amazing experience!",
            "Terrible support.",
            "Love it!",
            "Waste of money.",
            "Highly recommend!",
            "Very disappointed."
        ]
        
        print(f"\nProcessing batch of {len(batch_texts)} texts...")
        
        # Time the batch processing
        import time
        start_time = time.time()
        
        results = []
        for text in batch_texts:
            result = await engine.classify(text)
            results.append(result)
        
        end_time = time.time()
        total_time = (end_time - start_time) * 1000  # Convert to ms
        
        print(f"\nüìä Performance Results:")
        print(f"   Total processing time: {total_time:.1f}ms")
        print(f"   Average per text: {total_time/len(batch_texts):.1f}ms")
        print(f"   Throughput: {len(batch_texts)/(total_time/1000):.1f} texts/second")
        
        # Show accuracy
        positive_count = sum(1 for r in results if "POSITIVE" in r.primary_category)
        negative_count = len(results) - positive_count
        
        print(f"\nüìà Classification Results:")
        print(f"   Positive: {positive_count}")
        print(f"   Negative: {negative_count}")
        print(f"   Average confidence: {sum(r.confidence for r in results)/len(results):.3f}")
        
        # Memory usage
        if hasattr(engine.primary_provider, 'get_model_info'):
            model_info = engine.primary_provider.get_model_info()
            if 'memory_usage_mb' in model_info:
                print(f"   Memory usage: {model_info['memory_usage_mb']:.1f} MB")
        
        await engine.cleanup()
        
    except Exception as e:
        print(f"‚ùå Error: {e}")


async def multi_model_comparison():
    """Compare different HuggingFace models."""
    print("\nüî¨ Multi-Model Comparison")
    print("=" * 50)
    
    # Define models to compare
    models_to_test = [
        {
            "name": "DistilBERT",
            "model": "distilbert-base-uncased-finetuned-sst-2-english",
            "categories": ["NEGATIVE", "POSITIVE"]
        },
        {
            "name": "RoBERTa",
            "model": "cardiffnlp/twitter-roberta-base-sentiment-latest", 
            "categories": ["LABEL_0", "LABEL_1", "LABEL_2"]
        }
    ]
    
    test_text = "This product is absolutely fantastic and I love using it every day!"
    
    print(f"Comparing models on text: {test_text}")
    
    for model_info in models_to_test:
        try:
            print(f"\nüß™ Testing {model_info['name']}...")
            
            config = ProviderConfig(
                provider_type=ProviderType.HUGGINGFACE,
                model=model_info["model"],
                device="auto",
                quantization=False,
                cache_dir="./models"
            )
            
            engine = ClassificationEngine.create_simple(
                provider_type="huggingface",
                model=model_info["model"],
                categories=model_info["categories"]
            )
            
            await engine.initialize()
            
            import time
            start_time = time.time()
            result = await engine.classify(test_text)
            end_time = time.time()
            
            processing_time = (end_time - start_time) * 1000
            
            print(f"   Category: {result.primary_category}")
            print(f"   Confidence: {result.confidence:.3f}")
            print(f"   Processing time: {processing_time:.1f}ms")
            
            await engine.cleanup()
            
        except Exception as e:
            print(f"   ‚ùå Error with {model_info['name']}: {e}")
    
    print("\nüí° Model Selection Tips:")
    print("‚Ä¢ DistilBERT: Faster, smaller, good for real-time applications")
    print("‚Ä¢ RoBERTa: More accurate, better for complex sentiment analysis")
    print("‚Ä¢ Choose based on your speed vs accuracy requirements")


async def main():
    """Run all HuggingFace examples."""
    print("ü§ó HuggingFace Provider Examples")
    print("=" * 60)
    print("Demonstrating local inference with Transformers")
    print("=" * 60)
    
    # Check system requirements
    system_info = get_system_info()
    print(f"System: {system_info['cpu_count']} CPUs, {system_info['memory_gb']:.1f}GB RAM")
    print(f"GPU: {system_info['gpu_available']}, MPS: {system_info['mps_available']}")
    
    examples = [
        basic_huggingface_classification,
        quantized_huggingface_example,
        custom_model_example,
        streaming_huggingface_example,
        performance_optimization_example,
        multi_model_comparison
    ]
    
    for example_func in examples:
        try:
            await example_func()
        except Exception as e:
            print(f"‚ùå Example {example_func.__name__} failed: {e}")
        
        print("\n" + "="*60)
    
    print("‚úÖ All HuggingFace examples completed!")
    print("\nüí° Key Takeaways:")
    print("‚Ä¢ HuggingFace provides access to 50,000+ models")
    print("‚Ä¢ Local inference ensures privacy and control")
    print("‚Ä¢ Quantization reduces memory usage significantly")
    print("‚Ä¢ Device management optimizes performance")
    print("‚Ä¢ Model selection impacts speed vs accuracy")
    print("‚Ä¢ Specialized models excel in specific domains")


if __name__ == "__main__":
    asyncio.run(main())
