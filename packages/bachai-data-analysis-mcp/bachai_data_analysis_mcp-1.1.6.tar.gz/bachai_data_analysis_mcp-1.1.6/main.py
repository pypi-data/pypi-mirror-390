#!/usr/bin/env python3
"""
æ•°æ®åˆ†æ MCP æœåŠ¡å™¨ - stdio å’Œ SSE åŒæ¨¡å¼
"""

import json
import sys
from typing import Any, Dict, Optional
from pathlib import Path

# å»¶è¿Ÿå¯¼å…¥ï¼šåªåœ¨éœ€è¦æ—¶å¯¼å…¥é‡å‹åº“
def _lazy_imports():
    """å»¶è¿Ÿå¯¼å…¥æ‰€æœ‰æ•°æ®åˆ†æç›¸å…³çš„åº“"""
    global pd, np, plt, sns
    import pandas as pd
    import numpy as np
    import matplotlib.pyplot as plt
    import seaborn as sns

# å»¶è¿Ÿå¯¼å…¥ï¼šåªåœ¨ SSE æ¨¡å¼æ—¶å¯¼å…¥ FastAPI ç›¸å…³åº“
def _lazy_imports_sse():
    """å»¶è¿Ÿå¯¼å…¥ SSE æ¨¡å¼æ‰€éœ€çš„åº“"""
    global FastAPI, Request, Response, StreamingResponse, JSONResponse
    global CORSMiddleware, EventSourceResponse, uvicorn, asyncio, uuid
    from fastapi import FastAPI, Request, Response
    from fastapi.responses import StreamingResponse, JSONResponse
    from fastapi.middleware.cors import CORSMiddleware
    from sse_starlette.sse import EventSourceResponse
    import uvicorn
    import asyncio
    import uuid

# å­˜å‚¨åŠ è½½çš„æ•°æ®é›†
loaded_datasets: Dict[str, Any] = {}

# å­˜å‚¨å¾…å¤„ç†çš„æ¶ˆæ¯é˜Ÿåˆ—ï¼ˆç”¨äº SSE é€šä¿¡ï¼‰
message_queues: Dict[str, Any] = {}
response_queues: Dict[str, Any] = {}

# FastAPI app å°†åœ¨ SSE æ¨¡å¼ä¸‹åˆå§‹åŒ–
app = None


class DataAnalysisMcpServer:
    def __init__(self):
        # ç¡®ä¿æ•°æ®åˆ†æåº“å·²å¯¼å…¥
        _lazy_imports()
        
        self.server_info = {
            "name": "data-analysis-mcp",
            "version": "1.1.6"
        }
    
    def handle_request(self, request: Dict[str, Any]) -> Dict[str, Any]:
        """å¤„ç†è¯·æ±‚"""
        method = request.get("method")
        params = request.get("params", {})
        request_id = request.get("id")
        
        try:
            if method == "initialize":
                result = self.handle_initialize()
            elif method == "tools/list":
                result = self.handle_list_tools()
            elif method == "tools/call":
                result = self.handle_tool_call(params)
            else:
                raise ValueError(f"Unknown method: {method}")
            
            return {
                "jsonrpc": "2.0",
                "id": request_id,
                "result": result
            }
        except Exception as e:
            return {
                "jsonrpc": "2.0",
                "id": request_id,
                "error": {
                    "code": -32000,
                    "message": str(e)
                }
            }
    
    def handle_initialize(self) -> Dict[str, Any]:
        """å¤„ç†åˆå§‹åŒ–"""
        return {
            "protocolVersion": "2024-11-05",
            "serverInfo": self.server_info,
            "capabilities": {
                "tools": {}
            }
        }
    
    def handle_list_tools(self) -> Dict[str, Any]:
        """åˆ—å‡ºå¯ç”¨å·¥å…·"""
        return {
            "tools": [
                {
                    "name": "load_data",
                    "description": "åŠ è½½æ•°æ®æ–‡ä»¶ï¼ˆæ”¯æŒCSVã€Excelã€JSONï¼‰",
                    "inputSchema": {
                        "type": "object",
                        "properties": {
                            "filepath": {
                                "type": "string",
                                "description": "æ•°æ®æ–‡ä»¶è·¯å¾„"
                            },
                            "dataset_name": {
                                "type": "string",
                                "description": "æ•°æ®é›†åç§°ï¼ˆç”¨äºåç»­å¼•ç”¨ï¼‰"
                            },
                            "file_type": {
                                "type": "string",
                                "description": "æ–‡ä»¶ç±»å‹ï¼ˆcsv/excel/jsonï¼Œå¯é€‰ï¼Œè‡ªåŠ¨æ£€æµ‹ï¼‰"
                            }
                        },
                        "required": ["filepath"]
                    }
                },
                {
                    "name": "describe_data",
                    "description": "è·å–æ•°æ®é›†çš„æè¿°æ€§ç»Ÿè®¡ä¿¡æ¯",
                    "inputSchema": {
                        "type": "object",
                        "properties": {
                            "dataset_name": {
                                "type": "string",
                                "description": "æ•°æ®é›†åç§°"
                            }
                        },
                        "required": ["dataset_name"]
                    }
                },
                {
                    "name": "analyze_column",
                    "description": "åˆ†æç‰¹å®šåˆ—çš„æ•°æ®",
                    "inputSchema": {
                        "type": "object",
                        "properties": {
                            "dataset_name": {
                                "type": "string",
                                "description": "æ•°æ®é›†åç§°"
                            },
                            "column_name": {
                                "type": "string",
                                "description": "åˆ—å"
                            }
                        },
                        "required": ["dataset_name", "column_name"]
                    }
                },
                {
                    "name": "correlation_analysis",
                    "description": "è®¡ç®—æ•°å€¼åˆ—ä¹‹é—´çš„ç›¸å…³æ€§",
                    "inputSchema": {
                        "type": "object",
                        "properties": {
                            "dataset_name": {
                                "type": "string",
                                "description": "æ•°æ®é›†åç§°"
                            }
                        },
                        "required": ["dataset_name"]
                    }
                },
                {
                    "name": "list_datasets",
                    "description": "åˆ—å‡ºå·²åŠ è½½çš„æ•°æ®é›†",
                    "inputSchema": {
                        "type": "object",
                        "properties": {}
                    }
                }
            ]
        }
    
    def handle_tool_call(self, params: Dict[str, Any]) -> Dict[str, Any]:
        """å¤„ç†å·¥å…·è°ƒç”¨"""
        tool_name = params.get("name")
        args = params.get("arguments", {})
        
        try:
            if tool_name == "load_data":
                result = self.load_data(args)
            elif tool_name == "describe_data":
                result = self.describe_data(args)
            elif tool_name == "analyze_column":
                result = self.analyze_column(args)
            elif tool_name == "correlation_analysis":
                result = self.correlation_analysis(args)
            elif tool_name == "list_datasets":
                result = self.list_datasets(args)
            else:
                raise ValueError(f"Unknown tool: {tool_name}")
            
            return {
                "content": [
                    {
                        "type": "text",
                        "text": result
                    }
                ]
            }
        except Exception as e:
            return {
                "content": [
                    {
                        "type": "text",
                        "text": f"é”™è¯¯: {str(e)}"
                    }
                ]
            }
    
    def load_data(self, args: Dict[str, Any]) -> str:
        """åŠ è½½æ•°æ®æ–‡ä»¶"""
        filepath = args.get("filepath")
        dataset_name = args.get("dataset_name", "default")
        file_type = args.get("file_type")
        
        path = Path(filepath)
        if not path.exists():
            return f"é”™è¯¯: æ–‡ä»¶ä¸å­˜åœ¨ - {filepath}"
        
        try:
            # è‡ªåŠ¨æ£€æµ‹æ–‡ä»¶ç±»å‹
            if file_type is None:
                ext = path.suffix.lower()
                if ext == '.csv':
                    file_type = 'csv'
                elif ext in ['.xlsx', '.xls']:
                    file_type = 'excel'
                elif ext == '.json':
                    file_type = 'json'
                else:
                    return f"é”™è¯¯: ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹ - {ext}"
            
            # åŠ è½½æ•°æ®
            if file_type == 'csv':
                df = pd.read_csv(filepath)
            elif file_type == 'excel':
                df = pd.read_excel(filepath)
            elif file_type == 'json':
                df = pd.read_json(filepath)
            else:
                return f"é”™è¯¯: ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹ - {file_type}"
            
            loaded_datasets[dataset_name] = df
            
            output = f"=== æ•°æ®åŠ è½½æˆåŠŸ ===\n"
            output += f"æ•°æ®é›†åç§°: {dataset_name}\n"
            output += f"æ–‡ä»¶è·¯å¾„: {filepath}\n"
            output += f"è¡Œæ•°: {len(df)}\n"
            output += f"åˆ—æ•°: {len(df.columns)}\n"
            output += f"åˆ—å: {', '.join(df.columns.tolist())}\n"
            output += f"\nå‰5è¡Œæ•°æ®:\n{df.head().to_string()}\n"
            
            return output
        except Exception as e:
            return f"é”™è¯¯: åŠ è½½æ•°æ®å¤±è´¥ - {str(e)}"
    
    def describe_data(self, args: Dict[str, Any]) -> str:
        """æè¿°æ€§ç»Ÿè®¡"""
        dataset_name = args.get("dataset_name")
        
        if dataset_name not in loaded_datasets:
            return f"é”™è¯¯: æ•°æ®é›† '{dataset_name}' æœªåŠ è½½"
        
        df = loaded_datasets[dataset_name]
        
        output = f"=== æ•°æ®é›†æè¿°: {dataset_name} ===\n\n"
        output += f"å½¢çŠ¶: {df.shape[0]} è¡Œ Ã— {df.shape[1]} åˆ—\n\n"
        
        # æ•°æ®ç±»å‹
        output += "åˆ—ä¿¡æ¯:\n"
        for col in df.columns:
            output += f"  {col}: {df[col].dtype}\n"
        
        # ç¼ºå¤±å€¼
        missing = df.isnull().sum()
        if missing.sum() > 0:
            output += f"\nç¼ºå¤±å€¼:\n"
            for col, count in missing.items():
                if count > 0:
                    output += f"  {col}: {count} ({count/len(df)*100:.2f}%)\n"
        else:
            output += f"\næ— ç¼ºå¤±å€¼\n"
        
        # æ•°å€¼åˆ—ç»Ÿè®¡
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        if len(numeric_cols) > 0:
            output += f"\næ•°å€¼åˆ—ç»Ÿè®¡:\n"
            output += df[numeric_cols].describe().to_string()
        
        # åˆ†ç±»åˆ—ç»Ÿè®¡
        categorical_cols = df.select_dtypes(include=['object']).columns
        if len(categorical_cols) > 0:
            output += f"\n\nåˆ†ç±»åˆ—ç»Ÿè®¡:\n"
            for col in categorical_cols[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                output += f"\n{col}:\n"
                output += f"  å”¯ä¸€å€¼æ•°: {df[col].nunique()}\n"
                value_counts = df[col].value_counts().head(5)
                output += f"  å‰5ä¸ªå€¼:\n"
                for val, count in value_counts.items():
                    output += f"    {val}: {count}\n"
        
        return output
    
    def analyze_column(self, args: Dict[str, Any]) -> str:
        """åˆ†æç‰¹å®šåˆ—"""
        dataset_name = args.get("dataset_name")
        column_name = args.get("column_name")
        
        if dataset_name not in loaded_datasets:
            return f"é”™è¯¯: æ•°æ®é›† '{dataset_name}' æœªåŠ è½½"
        
        df = loaded_datasets[dataset_name]
        
        if column_name not in df.columns:
            return f"é”™è¯¯: åˆ— '{column_name}' ä¸å­˜åœ¨"
        
        col = df[column_name]
        
        output = f"=== åˆ—åˆ†æ: {column_name} ===\n\n"
        output += f"æ•°æ®ç±»å‹: {col.dtype}\n"
        output += f"æ€»æ•°: {len(col)}\n"
        output += f"ç¼ºå¤±å€¼: {col.isnull().sum()} ({col.isnull().sum()/len(col)*100:.2f}%)\n"
        output += f"å”¯ä¸€å€¼: {col.nunique()}\n\n"
        
        if pd.api.types.is_numeric_dtype(col):
            # æ•°å€¼å‹åˆ—
            output += "ç»Ÿè®¡é‡:\n"
            output += f"  å‡å€¼: {col.mean():.4f}\n"
            output += f"  ä¸­ä½æ•°: {col.median():.4f}\n"
            output += f"  æ ‡å‡†å·®: {col.std():.4f}\n"
            output += f"  æœ€å°å€¼: {col.min():.4f}\n"
            output += f"  æœ€å¤§å€¼: {col.max():.4f}\n"
            output += f"  25%åˆ†ä½æ•°: {col.quantile(0.25):.4f}\n"
            output += f"  75%åˆ†ä½æ•°: {col.quantile(0.75):.4f}\n"
        else:
            # åˆ†ç±»å‹åˆ—
            output += "å€¼é¢‘ç‡ï¼ˆå‰10ï¼‰:\n"
            value_counts = col.value_counts().head(10)
            for val, count in value_counts.items():
                output += f"  {val}: {count} ({count/len(col)*100:.2f}%)\n"
        
        return output
    
    def correlation_analysis(self, args: Dict[str, Any]) -> str:
        """ç›¸å…³æ€§åˆ†æ"""
        dataset_name = args.get("dataset_name")
        
        if dataset_name not in loaded_datasets:
            return f"é”™è¯¯: æ•°æ®é›† '{dataset_name}' æœªåŠ è½½"
        
        df = loaded_datasets[dataset_name]
        numeric_cols = df.select_dtypes(include=[np.number])
        
        if numeric_cols.shape[1] < 2:
            return "é”™è¯¯: è‡³å°‘éœ€è¦2ä¸ªæ•°å€¼åˆ—æ‰èƒ½è¿›è¡Œç›¸å…³æ€§åˆ†æ"
        
        corr_matrix = numeric_cols.corr()
        
        output = f"=== ç›¸å…³æ€§åˆ†æ: {dataset_name} ===\n\n"
        output += "ç›¸å…³ç³»æ•°çŸ©é˜µ:\n"
        output += corr_matrix.to_string()
        
        # æ‰¾å‡ºå¼ºç›¸å…³çš„åˆ—å¯¹
        output += "\n\nå¼ºç›¸å…³åˆ—å¯¹ï¼ˆ|r| > 0.7ï¼‰:\n"
        strong_corr = []
        for i in range(len(corr_matrix.columns)):
            for j in range(i+1, len(corr_matrix.columns)):
                corr_val = corr_matrix.iloc[i, j]
                if abs(corr_val) > 0.7:
                    strong_corr.append(
                        (corr_matrix.columns[i], corr_matrix.columns[j], corr_val)
                    )
        
        if strong_corr:
            for col1, col2, corr_val in sorted(strong_corr, key=lambda x: abs(x[2]), reverse=True):
                output += f"  {col1} â†” {col2}: {corr_val:.4f}\n"
        else:
            output += "  æœªå‘ç°å¼ºç›¸å…³çš„åˆ—å¯¹\n"
        
        return output
    
    def list_datasets(self, args: Dict[str, Any]) -> str:
        """åˆ—å‡ºå·²åŠ è½½çš„æ•°æ®é›†"""
        if not loaded_datasets:
            return "å½“å‰æ²¡æœ‰åŠ è½½çš„æ•°æ®é›†ã€‚"
        
        output = "=== å·²åŠ è½½çš„æ•°æ®é›† ===\n\n"
        for name, df in loaded_datasets.items():
            output += f"ğŸ“Š {name}\n"
            output += f"   è¡Œæ•°: {df.shape[0]}\n"
            output += f"   åˆ—æ•°: {df.shape[1]}\n"
            output += f"   åˆ—å: {', '.join(df.columns.tolist()[:5])}"
            if len(df.columns) > 5:
                output += f" ... (å…±{len(df.columns)}åˆ—)"
            output += "\n\n"
        
        return output


def _create_sse_app():
    """åˆ›å»ºå¹¶é…ç½® FastAPI åº”ç”¨ï¼ˆä»…åœ¨ SSE æ¨¡å¼ä¸‹è°ƒç”¨ï¼‰"""
    # å¯¼å…¥ SSE ç›¸å…³åº“
    _lazy_imports_sse()
    _lazy_imports()  # ä¹Ÿéœ€è¦æ•°æ®åˆ†æåº“
    
    # åˆ›å»º FastAPI åº”ç”¨
    app = FastAPI(title="Data Analysis MCP Server")
    
    # æ·»åŠ  CORS æ”¯æŒ
    app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],
        allow_credentials=True,
        allow_methods=["*"],
        allow_headers=["*"],
    )
    
    # åˆ›å»ºæœåŠ¡å™¨å®ä¾‹
    mcp_server = DataAnalysisMcpServer()
    
    @app.get("/")
    async def root():
        """æ ¹è·¯å¾„ï¼Œè¿”å›æœåŠ¡å™¨ä¿¡æ¯"""
        return {
            "name": "Data Analysis MCP Server",
            "version": "1.1.6",
            "transport": "SSE",
            "endpoints": {
                "sse": "/sse",
                "messages": "/message"
            }
        }
    
    @app.get("/sse")
    async def sse_endpoint(request: Request):
        """SSE ç«¯ç‚¹ - ç”¨äºå»ºç«‹ SSE è¿æ¥å¹¶æ¥æ”¶æœåŠ¡å™¨æ¶ˆæ¯"""
        session_id = str(uuid.uuid4())
        response_queue = asyncio.Queue()
        response_queues[session_id] = response_queue
        
        async def event_generator():
            """ç”Ÿæˆ SSE äº‹ä»¶"""
            try:
                # å‘é€ endpoint äº‹ä»¶ï¼Œå‘Šè¯‰å®¢æˆ·ç«¯æ¶ˆæ¯å‘é€åœ°å€
                yield {
                    "event": "endpoint",
                    "data": f"/message?sessionId={session_id}"
                }
                
                # æŒç»­å‘é€é˜Ÿåˆ—ä¸­çš„å“åº”
                while True:
                    if await request.is_disconnected():
                        break
                    
                    try:
                        # ç­‰å¾…å“åº”æ¶ˆæ¯ï¼Œå¸¦è¶…æ—¶
                        response = await asyncio.wait_for(
                            response_queue.get(),
                            timeout=30.0
                        )
                        
                        # å‘é€æ¶ˆæ¯äº‹ä»¶
                        yield {
                            "event": "message",
                            "data": json.dumps(response)
                        }
                    except asyncio.TimeoutError:
                        # è¶…æ—¶å‘é€å¿ƒè·³
                        continue
                        
            except asyncio.CancelledError:
                pass
            finally:
                # æ¸…ç†ä¼šè¯
                if session_id in response_queues:
                    del response_queues[session_id]
        
        return EventSourceResponse(event_generator())
    
    @app.post("/message")
    async def message_endpoint(request: Request, sessionId: str = None):
        """å¤„ç† MCP æ¶ˆæ¯è¯·æ±‚"""
        try:
            body = await request.json()
            response = mcp_server.handle_request(body)
            
            # å¦‚æœæœ‰ sessionIdï¼Œé€šè¿‡ SSE è¿”å›
            if sessionId and sessionId in response_queues:
                await response_queues[sessionId].put(response)
                return Response(status_code=202)  # Accepted
            
            # å¦åˆ™ç›´æ¥è¿”å› JSON å“åº”
            return JSONResponse(content=response)
            
        except Exception as e:
            error_response = {
                "jsonrpc": "2.0",
                "error": {
                    "code": -32700,
                    "message": f"Parse error: {str(e)}"
                }
            }
            
            if sessionId and sessionId in response_queues:
                await response_queues[sessionId].put(error_response)
                return Response(status_code=202)
            
            return JSONResponse(content=error_response)
    
    # å…¼å®¹æ—§çš„ /messages ç«¯ç‚¹
    @app.post("/messages")
    async def messages_endpoint(request: Request):
        """å¤„ç† MCP æ¶ˆæ¯è¯·æ±‚ï¼ˆå…¼å®¹ç«¯ç‚¹ï¼‰"""
        return await message_endpoint(request)
    
    return app


def main_stdio():
    """Main entry point for stdio mode (for supergateway/Claude Desktop)"""
    import traceback
    
    print("ğŸš€ å¯åŠ¨ Data Analysis MCP Server (stdio æ¨¡å¼)", file=sys.stderr)
    print("ğŸ“¥ ç­‰å¾…æ¥è‡ª stdin çš„ JSON-RPC è¯·æ±‚...", file=sys.stderr)
    
    # åˆ›å»ºæœåŠ¡å™¨å®ä¾‹ï¼ˆä¸ä½¿ç”¨å…¨å±€çš„ï¼‰
    try:
        server = DataAnalysisMcpServer()
        print("âœ… æœåŠ¡å™¨å®ä¾‹åˆ›å»ºæˆåŠŸ", file=sys.stderr)
    except Exception as e:
        print(f"âŒ æœåŠ¡å™¨å®ä¾‹åˆ›å»ºå¤±è´¥: {e}", file=sys.stderr)
        traceback.print_exc(file=sys.stderr)
        sys.exit(1)
    
    # ä» stdin è¯»å–è¯·æ±‚ï¼Œå‘ stdout å‘é€å“åº”
    try:
        for line in sys.stdin:
            line = line.strip()
            if not line:
                continue
                
            print(f"ğŸ“¨ æ”¶åˆ°è¯·æ±‚: {line[:100]}...", file=sys.stderr)
            
            try:
                request = json.loads(line)
                print(f"ğŸ”„ å¤„ç†æ–¹æ³•: {request.get('method')}", file=sys.stderr)
                
                response = server.handle_request(request)
                response_json = json.dumps(response)
                
                print(response_json, flush=True)
                print(f"âœ… å“åº”å·²å‘é€: {response_json[:100]}...", file=sys.stderr)
                
            except json.JSONDecodeError as e:
                print(f"âŒ JSON è§£æé”™è¯¯: {e}", file=sys.stderr)
                error_response = {
                    "jsonrpc": "2.0",
                    "error": {
                        "code": -32700,
                        "message": f"Parse error: {str(e)}"
                    }
                }
                print(json.dumps(error_response), flush=True)
                
            except Exception as e:
                print(f"âŒ å¤„ç†è¯·æ±‚æ—¶å‡ºé”™: {e}", file=sys.stderr)
                traceback.print_exc(file=sys.stderr)
                error_response = {
                    "jsonrpc": "2.0",
                    "error": {
                        "code": -32603,
                        "message": f"Internal error: {str(e)}"
                    }
                }
                print(json.dumps(error_response), flush=True)
                
    except KeyboardInterrupt:
        print("â¹ï¸  æœåŠ¡å™¨è¢«ä¸­æ–­", file=sys.stderr)
    except Exception as e:
        print(f"âŒ è‡´å‘½é”™è¯¯: {e}", file=sys.stderr)
        traceback.print_exc(file=sys.stderr)
        sys.exit(1)


def main_sse():
    """Main entry point for SSE mode (standalone HTTP server)"""
    print("ğŸš€ å¯åŠ¨ Data Analysis MCP Server (SSE æ¨¡å¼)", file=sys.stderr)
    print("ğŸ“¡ SSE Endpoint: http://localhost:8000/sse", file=sys.stderr)
    print("ğŸ“¨ Messages Endpoint: http://localhost:8000/messages", file=sys.stderr)
    print("ğŸ“– API Docs: http://localhost:8000/docs", file=sys.stderr)
    
    # åˆ›å»º FastAPI åº”ç”¨ï¼ˆè¿™ä¼šå¯¼å…¥æ‰€æœ‰å¿…è¦çš„åº“ï¼‰
    app = _create_sse_app()
    
    # uvicorn å·²ç»åœ¨ _lazy_imports_sse() ä¸­å¯¼å…¥
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=8000,
        log_level="info"
    )


def main():
    """Main entry point - defaults to stdio mode for compatibility"""
    main_stdio()


if __name__ == "__main__":
    # å½“ç›´æ¥è¿è¡Œæ—¶ï¼Œä½¿ç”¨ SSE æ¨¡å¼
    main_sse()
