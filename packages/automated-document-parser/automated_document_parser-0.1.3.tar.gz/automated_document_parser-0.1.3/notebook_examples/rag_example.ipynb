{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2138ee03",
   "metadata": {},
   "source": [
    "# RAG Application with Automated Document Parser\n",
    "\n",
    "This notebook demonstrates how to build a complete RAG (Retrieval-Augmented Generation) application using the automated-document-parser package and LangChain with HuggingFace models.\n",
    "\n",
    "RAG enables you to:\n",
    "- Load and process documents from various formats\n",
    "- Create embeddings and store them in a vector database\n",
    "- Retrieve relevant context for user queries\n",
    "- Generate accurate answers based on your documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40904952",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Import all necessary components for our RAG application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "633c373e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pulkit/Desktop/test/document_parser/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Our custom document parser\n",
    "from automated_document_parser import DocumentParser\n",
    "\n",
    "# LangChain components\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df6a65f",
   "metadata": {},
   "source": [
    "## 3. Configure HuggingFace Models\n",
    "\n",
    "We'll use free, small HuggingFace models for testing. No API key required!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b47c84bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
      "Language model: google/flan-t5-small\n",
      "No API key required - models run locally!\n"
     ]
    }
   ],
   "source": [
    "# Configure models to use\n",
    "# We'll use small, efficient models for testing\n",
    "\n",
    "# Embedding model - small and fast\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"  # 384 dimensions, 22M parameters\n",
    "\n",
    "# Language model for generation - small model for testing\n",
    "LLM_MODEL = \"google/flan-t5-small\"  # 60M parameters, good for testing\n",
    "\n",
    "print(f\"Embedding model: {EMBEDDING_MODEL}\")\n",
    "print(f\"Language model: {LLM_MODEL}\")\n",
    "print(\"No API key required - models run locally!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b54bb0",
   "metadata": {},
   "source": [
    "## 4. Create Sample Documents\n",
    "\n",
    "Let's create some sample documents for our RAG system to demonstrate the functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be9d1f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample documents created in sample_docs/\n",
      "   - python_basics.txt\n",
      "   - machine_learning.txt\n",
      "   - data.csv\n"
     ]
    }
   ],
   "source": [
    "# Create sample documents directory\n",
    "sample_docs_dir = Path(\"sample_docs\")\n",
    "sample_docs_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Create sample text file\n",
    "with open(sample_docs_dir / \"python_basics.txt\", \"w\") as f:\n",
    "    f.write(\"\"\"\n",
    "Python Programming Basics\n",
    "\n",
    "Python is a high-level, interpreted programming language known for its simplicity and readability.\n",
    "It was created by Guido van Rossum and first released in 1991.\n",
    "\n",
    "Key Features:\n",
    "- Easy to learn and read\n",
    "- Dynamically typed\n",
    "- Object-oriented\n",
    "- Large standard library\n",
    "- Cross-platform compatibility\n",
    "\n",
    "Common Use Cases:\n",
    "- Web development (Django, Flask)\n",
    "- Data science and machine learning\n",
    "- Automation and scripting\n",
    "- API development\n",
    "\"\"\")\n",
    "\n",
    "# Create another sample file\n",
    "with open(sample_docs_dir / \"machine_learning.txt\", \"w\") as f:\n",
    "    f.write(\"\"\"\n",
    "Introduction to Machine Learning\n",
    "\n",
    "Machine Learning is a subset of artificial intelligence that enables systems to learn\n",
    "and improve from experience without being explicitly programmed.\n",
    "\n",
    "Types of Machine Learning:\n",
    "1. Supervised Learning - Learning from labeled data\n",
    "2. Unsupervised Learning - Finding patterns in unlabeled data\n",
    "3. Reinforcement Learning - Learning through trial and error\n",
    "\n",
    "Popular ML Libraries:\n",
    "- Scikit-learn for classical ML\n",
    "- TensorFlow and PyTorch for deep learning\n",
    "- Pandas for data manipulation\n",
    "- NumPy for numerical computations\n",
    "\"\"\")\n",
    "\n",
    "# Create a CSV file\n",
    "with open(sample_docs_dir / \"data.csv\", \"w\") as f:\n",
    "    f.write(\"\"\"language,year_created,paradigm\n",
    "Python,1991,Multi-paradigm\n",
    "JavaScript,1995,Multi-paradigm\n",
    "Java,1995,Object-oriented\n",
    "Rust,2010,Multi-paradigm\n",
    "\"\"\")\n",
    "\n",
    "print(f\"Sample documents created in {sample_docs_dir}/\")\n",
    "print(f\"   - python_basics.txt\")\n",
    "print(f\"   - machine_learning.txt\")\n",
    "print(f\"   - data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d905f1",
   "metadata": {},
   "source": [
    "## 5. Load Documents Using Automated Document Parser\n",
    "\n",
    "Now we'll use our custom DocumentParser to automatically detect and load documents of different formats. The parser intelligently detects file types based on extensions and uses the appropriate loader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "134a111a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6 documents from 3 files:\n",
      "  - python_basics.txt: 1 document(s)\n",
      "  - data.csv: 4 document(s)\n",
      "  - machine_learning.txt: 1 document(s)\n",
      "\n",
      "Total document chunks: 6\n",
      "\n",
      "--- Sample Document ---\n",
      "Content preview: \n",
      "Python Programming Basics\n",
      "\n",
      "Python is a high-level, interpreted programming language known for its simplicity and readability.\n",
      "It was created by Guido van Rossum and first released in 1991.\n",
      "\n",
      "Key Featu...\n",
      "Metadata: {'source': '/Users/pulkit/Desktop/test/document_parser/notebooks/sample_docs/python_basics.txt', 'file_name': 'python_basics.txt', 'file_type': '.txt'}\n"
     ]
    }
   ],
   "source": [
    "# Initialize the DocumentParser\n",
    "parser = DocumentParser()\n",
    "\n",
    "# Get all files in the sample directory\n",
    "file_paths = [str(f) for f in sample_docs_dir.glob(\"*\") if f.is_file()]\n",
    "\n",
    "# Parse all documents using our automated parser\n",
    "parsed_docs = parser.parse_multiple(file_paths)\n",
    "\n",
    "# Display loaded documents\n",
    "total_docs = sum(len(docs) for docs in parsed_docs.values())\n",
    "print(f\"Loaded {total_docs} documents from {len(parsed_docs)} files:\")\n",
    "for file_path, docs in parsed_docs.items():\n",
    "    print(f\"  - {Path(file_path).name}: {len(docs)} document(s)\")\n",
    "\n",
    "# Flatten all documents into a single list\n",
    "all_documents = []\n",
    "for docs in parsed_docs.values():\n",
    "    all_documents.extend(docs)\n",
    "\n",
    "print(f\"\\nTotal document chunks: {len(all_documents)}\")\n",
    "\n",
    "# Display first document as example\n",
    "if all_documents:\n",
    "    print(\"\\n--- Sample Document ---\")\n",
    "    print(f\"Content preview: {all_documents[0].page_content[:200]}...\")\n",
    "    print(f\"Metadata: {all_documents[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ba8462",
   "metadata": {},
   "source": [
    "## 6. Split Documents into Chunks\n",
    "\n",
    "For better retrieval accuracy, we split documents into smaller chunks. This ensures:\n",
    "- Each chunk fits within the embedding model's context window\n",
    "- More precise retrieval of relevant information\n",
    "- Better matching between queries and document segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "622b6e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 6 documents into 7 chunks\n",
      "\n",
      "Sample chunk:\n",
      "Content: Python Programming Basics\n",
      "\n",
      "Python is a high-level, interpreted programming language known for its simplicity and readability.\n",
      "It was created by Guido van Rossum and first released in 1991.\n",
      "\n",
      "Key Featur...\n",
      "Metadata: {'source': '/Users/pulkit/Desktop/test/document_parser/notebooks/sample_docs/python_basics.txt', 'file_name': 'python_basics.txt', 'file_type': '.txt'}\n"
     ]
    }
   ],
   "source": [
    "# Split documents into smaller chunks\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# Split all documents\n",
    "split_documents = text_splitter.split_documents(all_documents)\n",
    "\n",
    "print(f\"Split {len(all_documents)} documents into {len(split_documents)} chunks\")\n",
    "print(f\"\\nSample chunk:\")\n",
    "print(f\"Content: {split_documents[0].page_content[:200]}...\")\n",
    "print(f\"Metadata: {split_documents[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0ac94e",
   "metadata": {},
   "source": [
    "## 7. Create Embeddings and Vector Store\n",
    "\n",
    "Initialize HuggingFace embeddings and create a FAISS vector store. The vector store enables:\n",
    "- Fast similarity search across thousands of documents\n",
    "- Efficient retrieval of relevant context\n",
    "- Semantic matching beyond keyword search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ccd6df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
      "Creating vector store...\n",
      "Vector store created with 7 embeddings\n",
      "\n",
      "Test search for: 'What is Python?'\n",
      "Found 2 relevant chunks\n",
      "\n",
      "Top result preview:\n",
      "Python Programming Basics\n",
      "\n",
      "Python is a high-level, interpreted programming language known for its simplicity and readability.\n",
      "It was created by Guido van Rossum and first released in 1991.\n",
      "\n",
      "Key Featur\n",
      "Vector store created with 7 embeddings\n",
      "\n",
      "Test search for: 'What is Python?'\n",
      "Found 2 relevant chunks\n",
      "\n",
      "Top result preview:\n",
      "Python Programming Basics\n",
      "\n",
      "Python is a high-level, interpreted programming language known for its simplicity and readability.\n",
      "It was created by Guido van Rossum and first released in 1991.\n",
      "\n",
      "Key Featur\n"
     ]
    }
   ],
   "source": [
    "# Initialize HuggingFace embeddings\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL,\n",
    "    model_kwargs={'device': 'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "print(f\"Using embedding model: {EMBEDDING_MODEL}\")\n",
    "print(\"Creating vector store...\")\n",
    "\n",
    "# Create FAISS vector store\n",
    "vectorstore = FAISS.from_documents(\n",
    "    documents=split_documents,\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "print(f\"Vector store created with {vectorstore.index.ntotal} embeddings\")\n",
    "\n",
    "# Test similarity search\n",
    "query = \"What is Python?\"\n",
    "results = vectorstore.similarity_search(query, k=2)\n",
    "print(f\"\\nTest search for: '{query}'\")\n",
    "print(f\"Found {len(results)} relevant chunks\")\n",
    "print(f\"\\nTop result preview:\")\n",
    "print(results[0].page_content[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ebbb63",
   "metadata": {},
   "source": [
    "## 8. Configure Language Model\n",
    "\n",
    "Set up the HuggingFace language model for generating answers. We're using flan-t5-small, a compact model that's perfect for testing and learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37885586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading language model: google/flan-t5-small\n",
      "This may take a moment on first run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Initialize the language model pipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "print(f\"Loading language model: {LLM_MODEL}\")\n",
    "print(\"This may take a moment on first run...\")\n",
    "\n",
    "text_generation_pipeline = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=LLM_MODEL,\n",
    "    max_length=512,\n",
    "    device=-1  # Use CPU\n",
    ")\n",
    "\n",
    "# Create LangChain LLM wrapper\n",
    "llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "\n",
    "print(\"Language model loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9137038a",
   "metadata": {},
   "source": [
    "## 9. Create Simple Q&amp;A Function\n",
    "\n",
    "Now we'll create a simple question-answering function that:\n",
    "1. **Retrieves**: Searches the vector store for relevant documents\n",
    "2. **Generates**: Uses the LLM to answer based on retrieved context\n",
    "3. **Returns**: Provides a single answer without maintaining history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "081e545a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Q&amp;A function created successfully!\n",
      "\n",
      "Ready to answer questions based on your documents!\n"
     ]
    }
   ],
   "source": [
    "# Create a simple Q&amp;A function\n",
    "def answer_question(question, retriever, llm, k=3):\n",
    "    \"\"\"\n",
    "    Answer a question based on retrieved documents.\n",
    "    \n",
    "    Args:\n",
    "        question: The user's question\n",
    "        retriever: Vector store retriever\n",
    "        llm: Language model\n",
    "        k: Number of documents to retrieve\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (answer, source_documents)\n",
    "    \"\"\"\n",
    "    # Retrieve relevant documents\n",
    "    docs = retriever.invoke(question)\n",
    "    \n",
    "    # Combine document contents\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "    \n",
    "    # Create prompt\n",
    "    prompt = f\"\"\"Answer the question based on the context below. If the answer cannot be found in the context, say \"I don't have enough information to answer that.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    # Generate answer\n",
    "    answer = llm.invoke(prompt)\n",
    "    \n",
    "    return answer, docs\n",
    "\n",
    "# Create retriever\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "print(\"Simple Q&amp;A function created successfully!\")\n",
    "print(\"\\nReady to answer questions based on your documents!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85037e8",
   "metadata": {},
   "source": [
    "## 10. Ask Questions and Get Answers\n",
    "\n",
    "Now let's test our Q&amp;A function! The system will:\n",
    "1. Take your question\n",
    "2. Search for relevant document chunks\n",
    "3. Generate an answer based on the context\n",
    "4. Return the answer with source attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e7f55c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: What is Python and when was it created?\n",
      "--------------------------------------------------------------------------------\n",
      "Answer: Python Programming Basics Python is a high-level, interpreted programming language known for its simplicity and readability. It was created by Guido van Rossum and first released in 1991.\n",
      "\n",
      "Sources used:\n",
      "  1. python_basics.txt\n",
      "  2. data.csv\n",
      "  3. data.csv\n",
      "\n",
      "Question: What are the types of machine learning?\n",
      "--------------------------------------------------------------------------------\n",
      "Answer: Python Programming Basics Python is a high-level, interpreted programming language known for its simplicity and readability. It was created by Guido van Rossum and first released in 1991.\n",
      "\n",
      "Sources used:\n",
      "  1. python_basics.txt\n",
      "  2. data.csv\n",
      "  3. data.csv\n",
      "\n",
      "Question: What are the types of machine learning?\n",
      "--------------------------------------------------------------------------------\n",
      "Answer: a subset of artificial intelligence that enables systems to learn and improve from experience without being explicitly programmed\n",
      "\n",
      "Sources used:\n",
      "  1. machine_learning.txt\n",
      "  2. machine_learning.txt\n",
      "  3. data.csv\n",
      "\n",
      "Question: Which programming language was created in 2010?\n",
      "--------------------------------------------------------------------------------\n",
      "Answer: Python\n",
      "\n",
      "Sources used:\n",
      "  1. data.csv\n",
      "  2. data.csv\n",
      "  3. data.csv\n",
      "Answer: a subset of artificial intelligence that enables systems to learn and improve from experience without being explicitly programmed\n",
      "\n",
      "Sources used:\n",
      "  1. machine_learning.txt\n",
      "  2. machine_learning.txt\n",
      "  3. data.csv\n",
      "\n",
      "Question: Which programming language was created in 2010?\n",
      "--------------------------------------------------------------------------------\n",
      "Answer: Python\n",
      "\n",
      "Sources used:\n",
      "  1. data.csv\n",
      "  2. data.csv\n",
      "  3. data.csv\n"
     ]
    }
   ],
   "source": [
    "# Ask questions about the documents\n",
    "questions = [\n",
    "    \"What is Python and when was it created?\",\n",
    "    \"What are the types of machine learning?\",\n",
    "    \"Which programming language was created in 2010?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    answer, source_docs = answer_question(question, retriever, llm)\n",
    "    \n",
    "    print(f\"Answer: {answer}\")\n",
    "    print(f\"\\nSources used:\")\n",
    "    for i, doc in enumerate(source_docs, 1):\n",
    "        source = doc.metadata.get('source', 'Unknown')\n",
    "        file_name = Path(source).name if source != 'Unknown' else 'Unknown'\n",
    "        print(f\"  {i}. {file_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "document-parser",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
