{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2138ee03",
   "metadata": {},
   "source": [
    "# RAG Application with Automated Document Parser\n",
    "\n",
    "This notebook demonstrates how to build a complete RAG (Retrieval-Augmented Generation) application using the automated-document-parser package and LangChain with HuggingFace models.\n",
    "\n",
    "RAG enables you to:\n",
    "- Load and process documents from various formats\n",
    "- Create embeddings and store them in a vector database\n",
    "- Retrieve relevant context for user queries\n",
    "- Generate accurate answers based on your documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40904952",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "\n",
    "Import all necessary components for our RAG application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "633c373e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pulkit/Desktop/test/document_parser/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Our custom document parser\n",
    "from automated_document_parser import DocumentParser\n",
    "\n",
    "# LangChain components\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df6a65f",
   "metadata": {},
   "source": [
    "## 3. Configure HuggingFace Models\n",
    "\n",
    "We'll use free, small HuggingFace models for testing. No API key required!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b47c84bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
      "Language model: google/flan-t5-small\n",
      "No API key required - models run locally!\n"
     ]
    }
   ],
   "source": [
    "# Configure models to use\n",
    "# We'll use small, efficient models for testing\n",
    "\n",
    "# Embedding model - small and fast\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"  # 384 dimensions, 22M parameters\n",
    "\n",
    "# Language model for generation - small model for testing\n",
    "LLM_MODEL = \"google/flan-t5-small\"  # 60M parameters, good for testing\n",
    "\n",
    "print(f\"Embedding model: {EMBEDDING_MODEL}\")\n",
    "print(f\"Language model: {LLM_MODEL}\")\n",
    "print(\"No API key required - models run locally!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b54bb0",
   "metadata": {},
   "source": [
    "## 4. Create Sample Documents\n",
    "\n",
    "Let's create some sample documents for our RAG system to demonstrate the functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be9d1f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample documents created in sample_docs/\n",
      "   - python_basics.txt\n",
      "   - machine_learning.txt\n",
      "   - data.csv\n"
     ]
    }
   ],
   "source": [
    "# Create sample documents directory\n",
    "sample_docs_dir = Path(\"sample_docs\")\n",
    "sample_docs_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Create sample text file\n",
    "with open(sample_docs_dir / \"python_basics.txt\", \"w\") as f:\n",
    "    f.write(\"\"\"\n",
    "Python Programming Basics\n",
    "\n",
    "Python is a high-level, interpreted programming language known for its simplicity and readability.\n",
    "It was created by Guido van Rossum and first released in 1991.\n",
    "\n",
    "Key Features:\n",
    "- Easy to learn and read\n",
    "- Dynamically typed\n",
    "- Object-oriented\n",
    "- Large standard library\n",
    "- Cross-platform compatibility\n",
    "\n",
    "Common Use Cases:\n",
    "- Web development (Django, Flask)\n",
    "- Data science and machine learning\n",
    "- Automation and scripting\n",
    "- API development\n",
    "\"\"\")\n",
    "\n",
    "# Create another sample file\n",
    "with open(sample_docs_dir / \"machine_learning.txt\", \"w\") as f:\n",
    "    f.write(\"\"\"\n",
    "Introduction to Machine Learning\n",
    "\n",
    "Machine Learning is a subset of artificial intelligence that enables systems to learn\n",
    "and improve from experience without being explicitly programmed.\n",
    "\n",
    "Types of Machine Learning:\n",
    "1. Supervised Learning - Learning from labeled data\n",
    "2. Unsupervised Learning - Finding patterns in unlabeled data\n",
    "3. Reinforcement Learning - Learning through trial and error\n",
    "\n",
    "Popular ML Libraries:\n",
    "- Scikit-learn for classical ML\n",
    "- TensorFlow and PyTorch for deep learning\n",
    "- Pandas for data manipulation\n",
    "- NumPy for numerical computations\n",
    "\"\"\")\n",
    "\n",
    "# Create a CSV file\n",
    "with open(sample_docs_dir / \"data.csv\", \"w\") as f:\n",
    "    f.write(\"\"\"language,year_created,paradigm\n",
    "Python,1991,Multi-paradigm\n",
    "JavaScript,1995,Multi-paradigm\n",
    "Java,1995,Object-oriented\n",
    "Rust,2010,Multi-paradigm\n",
    "\"\"\")\n",
    "\n",
    "print(f\"Sample documents created in {sample_docs_dir}/\")\n",
    "print(f\"   - python_basics.txt\")\n",
    "print(f\"   - machine_learning.txt\")\n",
    "print(f\"   - data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d905f1",
   "metadata": {},
   "source": [
    "## 5. Load Documents Using Automated Document Parser\n",
    "\n",
    "Now we'll use DocumentParser with automatic file type detection:\n",
    "\n",
    "**Step 1:** Specify the loading method (optional - uses defaults if not specified)\n",
    "**Step 2:** Parser automatically detects file types and loads them with the specified settings\n",
    "\n",
    "The parser intelligently detects file types based on extensions and uses the appropriate loader for each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134a111a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6 documents from 3 files:\n",
      "  - python_basics.txt: 1 document(s)\n",
      "  - data.csv: 4 document(s)\n",
      "  - machine_learning.txt: 1 document(s)\n",
      "\n",
      "Total document chunks: 6\n",
      "\n",
      "--- Sample Document ---\n",
      "Content preview: \n",
      "Python Programming Basics\n",
      "\n",
      "Python is a high-level, interpreted programming language known for its simplicity and readability.\n",
      "It was created by Guido van Rossum and first released in 1991.\n",
      "\n",
      "Key Featu...\n",
      "Metadata: {'source': '/Users/pulkit/Desktop/test/document_parser/notebook_examples/sample_docs/python_basics.txt', 'file_name': 'python_basics.txt', 'file_type': '.txt'}\n"
     ]
    }
   ],
   "source": [
    "# Initialize the DocumentParser\n",
    "parser = DocumentParser()\n",
    "\n",
    "# Get all files in the sample directory\n",
    "file_paths = [str(f) for f in sample_docs_dir.glob(\"*\") if f.is_file()]\n",
    "\n",
    "# Step 1: Specify the method (optional - defaults shown here)\n",
    "# Step 2: Parser automatically detects each file type and loads appropriately\n",
    "parsed_docs = parser.parse_multiple(\n",
    "    file_paths,\n",
    "    pdf_loader_method=\"pypdf\",  # Method for PDF files (if any)\n",
    "    encoding=\"utf-8\"             # Encoding for text/CSV files\n",
    ")\n",
    "\n",
    "# Display loaded documents\n",
    "total_docs = sum(len(docs) for docs in parsed_docs.values())\n",
    "print(f\"Loaded {total_docs} documents from {len(parsed_docs)} files:\")\n",
    "for file_path, docs in parsed_docs.items():\n",
    "    print(f\"  - {Path(file_path).name}: {len(docs)} document(s)\")\n",
    "\n",
    "# Flatten all documents into a single list\n",
    "all_documents = []\n",
    "for docs in parsed_docs.values():\n",
    "    all_documents.extend(docs)\n",
    "\n",
    "print(f\"\\nTotal document chunks: {len(all_documents)}\")\n",
    "\n",
    "# Display first document as example\n",
    "if all_documents:\n",
    "    print(\"\\n--- Sample Document ---\")\n",
    "    print(f\"Content preview: {all_documents[0].page_content[:200]}...\")\n",
    "    print(f\"Metadata: {all_documents[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ba8462",
   "metadata": {},
   "source": [
    "## 6. Split Documents into Chunks\n",
    "\n",
    "For better retrieval accuracy, we split documents into smaller chunks. This ensures:\n",
    "- Each chunk fits within the embedding model's context window\n",
    "- More precise retrieval of relevant information\n",
    "- Better matching between queries and document segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "622b6e37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 6 documents into 7 chunks\n",
      "\n",
      "Sample chunk:\n",
      "Content: Python Programming Basics\n",
      "\n",
      "Python is a high-level, interpreted programming language known for its simplicity and readability.\n",
      "It was created by Guido van Rossum and first released in 1991.\n",
      "\n",
      "Key Featur...\n",
      "Metadata: {'source': '/Users/pulkit/Desktop/test/document_parser/notebook_examples/sample_docs/python_basics.txt', 'file_name': 'python_basics.txt', 'file_type': '.txt'}\n"
     ]
    }
   ],
   "source": [
    "# Split documents into smaller chunks\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    ")\n",
    "\n",
    "# Split all documents\n",
    "split_documents = text_splitter.split_documents(all_documents)\n",
    "\n",
    "print(f\"Split {len(all_documents)} documents into {len(split_documents)} chunks\")\n",
    "print(f\"\\nSample chunk:\")\n",
    "print(f\"Content: {split_documents[0].page_content[:200]}...\")\n",
    "print(f\"Metadata: {split_documents[0].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0ac94e",
   "metadata": {},
   "source": [
    "## 7. Create Embeddings and Vector Store\n",
    "\n",
    "Initialize HuggingFace embeddings and create a FAISS vector store. The vector store enables:\n",
    "- Fast similarity search across thousands of documents\n",
    "- Efficient retrieval of relevant context\n",
    "- Semantic matching beyond keyword search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ccd6df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
      "Creating vector store...\n",
      "Vector store created with 7 embeddings\n",
      "\n",
      "Test search for: 'What is Python?'\n",
      "Found 2 relevant chunks\n",
      "\n",
      "Top result preview:\n",
      "Python Programming Basics\n",
      "\n",
      "Python is a high-level, interpreted programming language known for its simplicity and readability.\n",
      "It was created by Guido van Rossum and first released in 1991.\n",
      "\n",
      "Key Featur\n",
      "Vector store created with 7 embeddings\n",
      "\n",
      "Test search for: 'What is Python?'\n",
      "Found 2 relevant chunks\n",
      "\n",
      "Top result preview:\n",
      "Python Programming Basics\n",
      "\n",
      "Python is a high-level, interpreted programming language known for its simplicity and readability.\n",
      "It was created by Guido van Rossum and first released in 1991.\n",
      "\n",
      "Key Featur\n"
     ]
    }
   ],
   "source": [
    "# Initialize HuggingFace embeddings\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL,\n",
    "    model_kwargs={'device': 'cpu'},\n",
    "    encode_kwargs={'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "print(f\"Using embedding model: {EMBEDDING_MODEL}\")\n",
    "print(\"Creating vector store...\")\n",
    "\n",
    "# Create FAISS vector store\n",
    "vectorstore = FAISS.from_documents(\n",
    "    documents=split_documents,\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "print(f\"Vector store created with {vectorstore.index.ntotal} embeddings\")\n",
    "\n",
    "# Test similarity search\n",
    "query = \"What is Python?\"\n",
    "results = vectorstore.similarity_search(query, k=2)\n",
    "print(f\"\\nTest search for: '{query}'\")\n",
    "print(f\"Found {len(results)} relevant chunks\")\n",
    "print(f\"\\nTop result preview:\")\n",
    "print(results[0].page_content[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ebbb63",
   "metadata": {},
   "source": [
    "## 8. Configure Language Model\n",
    "\n",
    "Set up the HuggingFace language model for generating answers. We're using flan-t5-small, a compact model that's perfect for testing and learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37885586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading language model: google/flan-t5-small\n",
      "This may take a moment on first run...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language model loaded successfully\n"
     ]
    }
   ],
   "source": [
    "# Initialize the language model pipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "print(f\"Loading language model: {LLM_MODEL}\")\n",
    "print(\"This may take a moment on first run...\")\n",
    "\n",
    "text_generation_pipeline = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=LLM_MODEL,\n",
    "    max_length=512,\n",
    "    device=-1  # Use CPU\n",
    ")\n",
    "\n",
    "# Create LangChain LLM wrapper\n",
    "llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "\n",
    "print(\"Language model loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9137038a",
   "metadata": {},
   "source": [
    "## 9. Create Simple Q&amp;A Function\n",
    "\n",
    "Now we'll create a simple question-answering function that:\n",
    "1. **Retrieves**: Searches the vector store for relevant documents\n",
    "2. **Generates**: Uses the LLM to answer based on retrieved context\n",
    "3. **Returns**: Provides a single answer without maintaining history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "081e545a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple Q&amp;A function created successfully!\n",
      "\n",
      "Ready to answer questions based on your documents!\n"
     ]
    }
   ],
   "source": [
    "# Create a simple Q&amp;A function\n",
    "def answer_question(question, retriever, llm, k=3):\n",
    "    \"\"\"\n",
    "    Answer a question based on retrieved documents.\n",
    "    \n",
    "    Args:\n",
    "        question: The user's question\n",
    "        retriever: Vector store retriever\n",
    "        llm: Language model\n",
    "        k: Number of documents to retrieve\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (answer, source_documents)\n",
    "    \"\"\"\n",
    "    # Retrieve relevant documents\n",
    "    docs = retriever.invoke(question)\n",
    "    \n",
    "    # Combine document contents\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "    \n",
    "    # Create prompt\n",
    "    prompt = f\"\"\"Answer the question based on the context below. If the answer cannot be found in the context, say \"I don't have enough information to answer that.\"\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    # Generate answer\n",
    "    answer = llm.invoke(prompt)\n",
    "    \n",
    "    return answer, docs\n",
    "\n",
    "# Create retriever\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "print(\"Simple Q&amp;A function created successfully!\")\n",
    "print(\"\\nReady to answer questions based on your documents!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85037e8",
   "metadata": {},
   "source": [
    "## 10. Ask Questions and Get Answers\n",
    "\n",
    "Now let's test our Q&amp;A function! The system will:\n",
    "1. Take your question\n",
    "2. Search for relevant document chunks\n",
    "3. Generate an answer based on the context\n",
    "4. Return the answer with source attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e7f55c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: What are the types of machine learning?\n",
      "--------------------------------------------------------------------------------\n",
      "Answer: a subset of artificial intelligence that enables systems to learn and improve from experience without being explicitly programmed\n",
      "\n",
      "Sources used:\n",
      "  1. machine_learning.txt\n",
      "  2. machine_learning.txt\n",
      "  3. data.csv\n",
      "\n",
      "Question: Which programming language was created in 2010?\n",
      "--------------------------------------------------------------------------------\n",
      "Answer: Python\n",
      "\n",
      "Sources used:\n",
      "  1. data.csv\n",
      "  2. data.csv\n",
      "  3. data.csv\n",
      "Answer: a subset of artificial intelligence that enables systems to learn and improve from experience without being explicitly programmed\n",
      "\n",
      "Sources used:\n",
      "  1. machine_learning.txt\n",
      "  2. machine_learning.txt\n",
      "  3. data.csv\n",
      "\n",
      "Question: Which programming language was created in 2010?\n",
      "--------------------------------------------------------------------------------\n",
      "Answer: Python\n",
      "\n",
      "Sources used:\n",
      "  1. data.csv\n",
      "  2. data.csv\n",
      "  3. data.csv\n"
     ]
    }
   ],
   "source": [
    "# Ask questions about the documents\n",
    "questions = [\n",
    "    \"What are the types of machine learning?\",\n",
    "    \"Which programming language was created in 2010?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"\\nQuestion: {question}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    answer, source_docs = answer_question(question, retriever, llm)\n",
    "    \n",
    "    print(f\"Answer: {answer}\")\n",
    "    print(f\"\\nSources used:\")\n",
    "    for i, doc in enumerate(source_docs, 1):\n",
    "        source = doc.metadata.get('source', 'Unknown')\n",
    "        file_name = Path(source).name if source != 'Unknown' else 'Unknown'\n",
    "        print(f\"  {i}. {file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18c43b3",
   "metadata": {},
   "source": [
    "## 11. Advanced: Using Specific File Loaders\n",
    "\n",
    "The document parser now has a modular structure with dedicated loaders for each file type. You can use specific loaders directly for more control:\n",
    "\n",
    "### File Loaders (in `file_load/` module):\n",
    "- **TextFileLoader** - For .txt and .md files\n",
    "- **CSVFileLoader** - For CSV files with encoding support\n",
    "- **JSONFileLoader** - For JSON files with jq schema support\n",
    "- **DOCXFileLoader** - For Microsoft Word documents\n",
    "- **HTMLFileLoader** - For HTML files\n",
    "\n",
    "### PDF Loaders (in `pdf_load/` module):\n",
    "- **pypdf** - Basic PDF text extraction\n",
    "- **unstructured** - Advanced OCR and layout detection\n",
    "- **amazon_textract** - AWS Textract for high-accuracy OCR\n",
    "- **mathpix** - Specialized for mathematical formulas\n",
    "- **pdfplumber** - High accuracy text and table extraction\n",
    "- **pypdfium2** - Google PDFium library\n",
    "- **pymupdf** - PyMuPDF (fitz) backend\n",
    "- **pymupdf4llm** - LLM-optimized extraction\n",
    "- **opendataloader** - Advanced multi-format parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aff7c079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 document(s) from text file\n",
      "Preview: \n",
      "Python Programming Basics\n",
      "\n",
      "Python is a high-level, interpreted programming language known for its simplicity and readability.\n",
      "It was created by Guido...\n",
      "\n",
      "Loaded 4 row(s) from CSV file\n",
      "First row: language: Python\n",
      "year_created: 1991\n",
      "paradigm: Multi-paradigm...\n",
      "\n",
      "Installation commands for loaders:\n",
      "  Text: pip install langchain-community\n",
      "  CSV: pip install langchain-community\n"
     ]
    }
   ],
   "source": [
    "# Example: Using specific file loaders directly\n",
    "from automated_document_parser.loaders.file_load import (\n",
    "    TextFileLoader,\n",
    "    CSVFileLoader,\n",
    "    JSONFileLoader\n",
    ")\n",
    "\n",
    "# Load a text file with specific encoding\n",
    "text_loader = TextFileLoader(\n",
    "    file_path=str(sample_docs_dir / \"python_basics.txt\"),\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "text_docs = text_loader.load()\n",
    "print(f\"Loaded {len(text_docs)} document(s) from text file\")\n",
    "print(f\"Preview: {text_docs[0].page_content[:150]}...\\n\")\n",
    "\n",
    "# Load a CSV file\n",
    "csv_loader = CSVFileLoader(\n",
    "    file_path=str(sample_docs_dir / \"data.csv\"),\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "csv_docs = csv_loader.load()\n",
    "print(f\"Loaded {len(csv_docs)} row(s) from CSV file\")\n",
    "print(f\"First row: {csv_docs[0].page_content[:100]}...\\n\")\n",
    "\n",
    "# Get installation commands if needed\n",
    "print(\"Installation commands for loaders:\")\n",
    "print(f\"  Text: {text_loader.get_install_command()}\")\n",
    "print(f\"  CSV: {csv_loader.get_install_command()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa52a86",
   "metadata": {},
   "source": [
    "## 12. Advanced: Using Different PDF Loaders\n",
    "\n",
    "The document parser supports multiple PDF loading methods, each optimized for different use cases. Let's explore how to use them:\n",
    "\n",
    "**Note:** Most PDF loaders require additional dependencies. You can install them as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1436700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. PyPDF Loader (default)\n",
      "   - Basic PDF text extraction\n",
      "   - No extra dependencies required\n",
      "   Usage: PDFLoader('file.pdf', method='pypdf')\n",
      "\n",
      "2. Mathpix Loader\n",
      "   - Specialized for mathematical formulas and equations\n",
      "   - Requires: langchain-community\n",
      "   - Requires API key: set MATHPIX_APP_ID and MATHPIX_APP_KEY\n",
      "   Usage: PDFLoader('file.pdf', method='mathpix', mathpix_app_id='...', mathpix_app_key='...')\n",
      "\n",
      "3. PDFPlumber Loader\n",
      "   - High accuracy text and table extraction\n",
      "   - Requires: pdfplumber\n",
      "   - Install: pip install pdfplumber\n",
      "   Usage: PDFLoader('file.pdf', method='pdfplumber')\n",
      "\n",
      "4. PyMuPDF Loader\n",
      "   - Fast PDF processing with fitz backend\n",
      "   - Requires: pymupdf\n",
      "   - Install: pip install pymupdf\n",
      "   Usage: PDFLoader('file.pdf', method='pymupdf')\n",
      "\n",
      "5. PyMuPDF4LLM Loader\n",
      "   - Optimized for LLM consumption\n",
      "   - Preserves document structure for better context\n",
      "   - Requires: pymupdf4llm\n",
      "   - Install: pip install pymupdf4llm\n",
      "   Usage: PDFLoader('file.pdf', method='pymupdf4llm')\n",
      "\n",
      "\n",
      "To check installation requirements:\n",
      "loader = PDFLoader('file.pdf', method='pdfplumber')\n",
      "print(loader.get_install_command())\n"
     ]
    }
   ],
   "source": [
    "# Example: Using different PDF loading methods\n",
    "from automated_document_parser.loaders import PDFLoader, load_pdf\n",
    "\n",
    "# Note: For this demo, we'll show the API without actually loading PDFs\n",
    "# since we don't have sample PDF files in this notebook\n",
    "\n",
    "# Method 1: Using PyPDF (default - no extra dependencies)\n",
    "print(\"1. PyPDF Loader (default)\")\n",
    "print(\"   - Basic PDF text extraction\")\n",
    "print(\"   - No extra dependencies required\")\n",
    "print(\"   Usage: PDFLoader('file.pdf', method='pypdf')\\n\")\n",
    "\n",
    "# Method 2: Using Mathpix for mathematical content\n",
    "print(\"2. Mathpix Loader\")\n",
    "print(\"   - Specialized for mathematical formulas and equations\")\n",
    "print(\"   - Requires: langchain-community\")\n",
    "print(\"   - Requires API key: set MATHPIX_APP_ID and MATHPIX_APP_KEY\")\n",
    "print(\"   Usage: PDFLoader('file.pdf', method='mathpix', mathpix_app_id='...', mathpix_app_key='...')\\n\")\n",
    "\n",
    "# Method 3: Using PDFPlumber for high accuracy\n",
    "print(\"3. PDFPlumber Loader\")\n",
    "print(\"   - High accuracy text and table extraction\")\n",
    "print(\"   - Requires: pdfplumber\")\n",
    "print(\"   - Install: pip install pdfplumber\")\n",
    "print(\"   Usage: PDFLoader('file.pdf', method='pdfplumber')\\n\")\n",
    "\n",
    "# Method 4: Using PyMuPDF for fast processing\n",
    "print(\"4. PyMuPDF Loader\")\n",
    "print(\"   - Fast PDF processing with fitz backend\")\n",
    "print(\"   - Requires: pymupdf\")\n",
    "print(\"   - Install: pip install pymupdf\")\n",
    "print(\"   Usage: PDFLoader('file.pdf', method='pymupdf')\\n\")\n",
    "\n",
    "# Method 5: Using PyMuPDF4LLM for LLM-optimized extraction\n",
    "print(\"5. PyMuPDF4LLM Loader\")\n",
    "print(\"   - Optimized for LLM consumption\")\n",
    "print(\"   - Preserves document structure for better context\")\n",
    "print(\"   - Requires: pymupdf4llm\")\n",
    "print(\"   - Install: pip install pymupdf4llm\")\n",
    "print(\"   Usage: PDFLoader('file.pdf', method='pymupdf4llm')\\n\")\n",
    "\n",
    "# Get installation command for any loader\n",
    "print(\"\\nTo check installation requirements:\")\n",
    "print(\"loader = PDFLoader('file.pdf', method='pdfplumber')\")\n",
    "print(\"print(loader.get_install_command())\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a51bb0",
   "metadata": {},
   "source": [
    "## 13. Practical Example: Choosing the Right Loader\n",
    "\n",
    "Here's a guide to help you choose the right loader for your use case:\n",
    "\n",
    "### For Text Files (.txt, .md):\n",
    "- Use `TextFileLoader` for simple text files\n",
    "- Supports custom encoding (UTF-8, Latin-1, etc.)\n",
    "\n",
    "### For Structured Data:\n",
    "- **CSV**: Use `CSVFileLoader` for tabular data\n",
    "- **JSON**: Use `JSONFileLoader` with optional jq schema filtering\n",
    "\n",
    "### For Documents:\n",
    "- **DOCX**: Use `DOCXFileLoader` for Microsoft Word documents\n",
    "- **HTML**: Use `HTMLFileLoader` for web pages\n",
    "\n",
    "### For PDFs:\n",
    "- **General text**: Use `pypdf` (default, no dependencies)\n",
    "- **Mathematical content**: Use `mathpix` (requires API key)\n",
    "- **High accuracy tables**: Use `pdfplumber`\n",
    "- **Fast processing**: Use `pymupdf`\n",
    "- **LLM-optimized**: Use `pymupdf4llm`\n",
    "- **Complex layouts**: Use `unstructured` or `amazon_textract`\n",
    "\n",
    "### Auto-detection:\n",
    "For most cases, just use `DocumentParser()` - it automatically selects the right loader based on file extension!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50de9cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available files in sample_docs:\n",
      "  - python_basics.txt (.txt)\n",
      "  - data.csv (.csv)\n",
      "  - machine_learning.txt (.txt)\n",
      "\n",
      "Parsing all files with auto-detection...\n",
      "\n",
      "âœ“ Successfully parsed 3 files\n",
      "  python_basics.txt (.txt): 1 document(s)\n",
      "  data.csv (.csv): 4 document(s)\n",
      "  machine_learning.txt (.txt): 1 document(s)\n",
      "\n",
      "Total files loaded: 3\n",
      "Parser automatically selected the right loader for each file type!\n"
     ]
    }
   ],
   "source": [
    "# Complete example: Mixed document types with auto-detection\n",
    "from automated_document_parser import DocumentParser\n",
    "\n",
    "# Initialize parser\n",
    "parser = DocumentParser()\n",
    "\n",
    "# Let's see what files we have\n",
    "print(\"Available files in sample_docs:\")\n",
    "for file in sample_docs_dir.glob(\"*\"):\n",
    "    if file.is_file():\n",
    "        print(f\"  - {file.name} ({file.suffix})\")\n",
    "\n",
    "# Parse all files automatically - parser detects file types\n",
    "print(\"\\nParsing all files with auto-detection...\")\n",
    "all_files = [str(f) for f in sample_docs_dir.glob(\"*\") if f.is_file()]\n",
    "results = parser.parse_multiple(all_files)\n",
    "\n",
    "print(f\"\\nSuccessfully parsed {len(results)} files\")\n",
    "for file_path, docs in results.items():\n",
    "    file_name = Path(file_path).name\n",
    "    file_type = Path(file_path).suffix\n",
    "    print(f\"  {file_name} ({file_type}): {len(docs)} document(s)\")\n",
    "\n",
    "# Show loaded files\n",
    "print(f\"\\nTotal files loaded: {len(parser.get_loaded_files())}\")\n",
    "print(\"Parser automatically selected the right loader for each file type!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80a2f78",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Congratulations! You've learned how to:\n",
    "\n",
    "1. Set up a complete RAG application with HuggingFace models\n",
    "2. Use the automated document parser to load various file types\n",
    "3. Create embeddings and build a vector store\n",
    "4. Implement question-answering with context retrieval\n",
    "5. Use specific file loaders for granular control\n",
    "6. Choose the right PDF loader for your use case\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "- **Modular Structure**: The parser now has `file_load/` and `pdf_load/` subdirectories for organized loaders\n",
    "- **Auto-Detection**: Use `DocumentParser()` for automatic file type detection\n",
    "- **Flexibility**: Import specific loaders when you need fine-grained control\n",
    "- **9 PDF Methods**: Choose from multiple PDF loading strategies based on your needs\n",
    "- **No API Keys Required**: This demo uses free HuggingFace models that run locally\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "1. Try adding your own documents to the `sample_docs/` folder\n",
    "2. Experiment with different PDF loading methods\n",
    "3. Use larger, more powerful models for production\n",
    "4. Integrate with your own applications\n",
    "\n",
    "Happy coding!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "document-parser",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
