"""MAWO –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å OpenCorpora –∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã.
"""

from __future__ import annotations

import logging
import pickle
from pathlib import Path
from typing import Any

try:
    from defusedxml.ElementTree import parse as defusedxml_parse  # type: ignore[import-not-found]

    ET_PARSE_SAFE = True
except ImportError:
    ET_PARSE_SAFE = False

# Rich –¥–ª—è –ø—Ä–æ–≥—Ä–µ—Å—Å –±–∞—Ä–∞ –∏ –∫—Ä–∞—Å–∏–≤–æ–≥–æ –≤—ã–≤–æ–¥–∞
try:
    from rich.console import Console
    from rich.panel import Panel

    RICH_AVAILABLE = True
except ImportError:
    RICH_AVAILABLE = False

# –ò—Å–ø—Ä–∞–≤–ª—è–µ–º getargspec –ø—Ä–æ–±–ª–µ–º—É –¥–ª—è Python 3.11+
import inspect

if not hasattr(inspect, "getargspec"):
    inspect.getargspec = inspect.getfullargspec  # type: ignore[assignment]

logger = logging.getLogger(__name__)

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π –∫—ç—à –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ OpenCorpora —Å–ª–æ–≤–∞—Ä—è
_GLOBAL_DICTIONARY_CACHE = None
_GLOBAL_PATTERNS_CACHE = None

# –°–∏–Ω–≥–ª—Ç–æ–Ω —ç–∫–∑–µ–º–ø–ª—è—Ä –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
_GLOBAL_ANALYZER_INSTANCE = None
_ANALYZER_LOCK = None

# –ò–º–ø–æ—Ä—Ç threading –µ—Å–ª–∏ –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω
try:
    import threading

    _ANALYZER_LOCK = threading.Lock()
except ImportError:
    _ANALYZER_LOCK = None


class MAWOTag:
    """–ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π —Ç–µ–≥ –≤ —Ñ–æ—Ä–º–∞—Ç–µ MAWO."""

    def __init__(self, pos: str = "UNKN", grammemes: set[str] | None = None) -> None:
        self.POS = pos
        self.grammemes = grammemes or set()

    @property
    def case(self) -> str | None:
        cases = {"nomn", "gent", "datv", "accs", "ablt", "loct", "voct"}
        return next((g for g in self.grammemes if g in cases), None)

    @property
    def number(self) -> str | None:
        numbers = {"sing", "plur"}
        return next((g for g in self.grammemes if g in numbers), None)

    @property
    def gender(self) -> str | None:
        genders = {"masc", "femn", "neut"}
        return next((g for g in self.grammemes if g in genders), None)

    @property
    def aspect(self) -> str | None:
        aspects = {"perf", "impf"}
        return next((g for g in self.grammemes if g in aspects), None)

    @property
    def tense(self) -> str | None:
        tenses = {"past", "pres", "futr"}
        return next((g for g in self.grammemes if g in tenses), None)

    def __contains__(self, item: Any) -> bool:
        return item in self.grammemes or item == self.POS

    def __str__(self) -> str:
        if not self.grammemes:
            return str(self.POS)
        return f"{self.POS} {','.join(sorted(self.grammemes))}"


class MAWOParse:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞."""

    def __init__(
        self,
        word: str,
        normal_form: str,
        tag: MAWOTag,
        score: float = 1.0,
        analyzer: Any | None = None,
    ) -> None:
        self.word = word
        self.normal_form = normal_form
        self.tag = tag
        self.score = score
        self._analyzer = analyzer

    def inflect(self, required_grammemes: set[str]) -> MAWOParse | None:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ª–æ–≤–æ—Ñ–æ—Ä–º—ã —Å –∑–∞–¥–∞–Ω–Ω—ã–º–∏ –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏.

        Args:
            required_grammemes: –ú–Ω–æ–∂–µ—Å—Ç–≤–æ —Ç—Ä–µ–±—É–µ–º—ã—Ö –≥—Ä–∞–º–º–µ–º (–Ω–∞–ø—Ä–∏–º–µ—Ä, {"sing", "femn"})

        Returns:
            MAWOParse —Å –Ω—É–∂–Ω—ã–º–∏ –≥—Ä–∞–º–º–µ–º–∞–º–∏ –∏–ª–∏ None –µ—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ
        """
        if not self._analyzer or not hasattr(self._analyzer, "dictionary"):
            # –ü—Ä–æ—Å—Ç–∞—è –∑–∞–≥–ª—É—à–∫–∞ –µ—Å–ª–∏ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
            logger.warning("Analyzer not available for inflection, returning None")
            return None

        # –ò—â–µ–º —Ñ–æ—Ä–º—ã –Ω–æ—Ä–º–∞–ª—å–Ω–æ–π —Ñ–æ—Ä–º—ã —Å–ª–æ–≤–∞
        normal_parses = self._analyzer.dictionary.get(self.normal_form, [])

        # –ò—â–µ–º —Ñ–æ—Ä–º—É —Å –Ω—É–∂–Ω—ã–º–∏ –≥—Ä–∞–º–º–µ–º–∞–º–∏
        for parse_item in normal_parses:
            if required_grammemes.issubset(parse_item.tag.grammemes):
                return parse_item  # type: ignore[no-any-return]

        # –ï—Å–ª–∏ —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –∏—â–µ–º —á–∞—Å—Ç–∏—á–Ω–æ–µ
        for parse_item in normal_parses:
            matching_grammemes = parse_item.tag.grammemes & required_grammemes
            if matching_grammemes:
                return parse_item  # type: ignore[no-any-return]

        return None

    def __repr__(self) -> str:
        return f"MAWOParse(word='{self.word}', normal_form='{self.normal_form}', tag='{self.tag}', score={self.score})"


class MAWOMorphAnalyzer:
    """–ì–ª–∞–≤–Ω—ã–π –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä MAWO
    –ü–æ–ª–Ω–∞—è –∑–∞–º–µ–Ω–∞ pymorphy2 —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º —Å–ª–æ–≤–∞—Ä–µ–º OpenCorpora.
    """

    def __init__(self, dict_path: str | None = None, use_dawg: bool = True) -> None:
        global _GLOBAL_DICTIONARY_CACHE, _GLOBAL_PATTERNS_CACHE

        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ DAWG —Å–ª–æ–≤–∞—Ä–∏ –∏–∑ dicts_ru
        self.dict_path = dict_path or str(Path(__file__).parent / "dicts_ru")
        self.use_dawg = use_dawg
        self._dawg_dict: Any = None

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–π DAWGDictionary –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω–æ
        if self.use_dawg and Path(self.dict_path).exists():
            try:
                from .dawg_dictionary import DAWGDictionary

                logger.info("‚ö° –ó–∞–≥—Ä—É–∑–∫–∞ DAWG —Å–ª–æ–≤–∞—Ä–µ–π...")
                self._dawg_dict = DAWGDictionary(self.dict_path)
                self.dictionary: dict[str, list[MAWOParse]] = {}

                logger.info("‚úÖ DAWG —Å–ª–æ–≤–∞—Ä–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ!")
                logger.info(f"   –ü—É—Ç—å –∫ —Å–ª–æ–≤–∞—Ä—è–º: {self.dict_path}")
                logger.info("   –ü–∞–º—è—Ç—å: ~15-20 –ú–ë (DAWG)")

                # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º patterns –¥–ª—è fallback –Ω–∞ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–µ —Å–ª–æ–≤–∞
                self.patterns: dict[str, Any] = {}
                self._init_patterns()

                # –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å —Ç–µ—Å—Ç–∞–º–∏
                self._analyzer = self
                self._production_analyzer = None

                logger.info(
                    "‚úÖ MAWO Morphological Analyzer initialized with DAWG dictionaries",
                )

                return  # –ì–æ—Ç–æ–≤–æ, –Ω–µ –Ω—É–∂–µ–Ω fallback

            except ImportError as e:
                logger.error(f"‚ö†Ô∏è dawg-python –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω: {e}, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback")
                logger.exception("–ü–æ–ª–Ω–∞—è —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∞ ImportError:")
                self.use_dawg = False
            except Exception as e:
                logger.error(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ DAWG: {e}, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback")
                logger.exception("–ü–æ–ª–Ω–∞—è —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∞ –æ—à–∏–±–∫–∏:")
                self.use_dawg = False

        # Fallback: –∑–∞–≥—Ä—É–∑–∫–∞ —á–µ—Ä–µ–∑ –∫—ç—à –∏–ª–∏ XML
        if _GLOBAL_DICTIONARY_CACHE is None:
            logger.info("üîÑ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è (fallback —Ä–µ–∂–∏–º) - –∑–∞–≥—Ä—É–∑–∫–∞ —Å–ª–æ–≤–∞—Ä—è...")

            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∫—Ä–∞—Å–∏–≤—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ –¢–û–õ–¨–ö–û –ø—Ä–∏ —Ä–µ–∞–ª—å–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–µ
            if RICH_AVAILABLE:
                console = Console()
                console.print(
                    Panel(
                        "[bold blue]üìö –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ MAWO[/bold blue]\n"
                        "[dim]–ó–∞–≥—Ä—É–∑–∫–∞ —Å–ª–æ–≤–∞—Ä—è OpenCorpora –¥–ª—è —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏...[/dim]",
                        title="OpenCorpora",
                    ),
                )

            self.dictionary: dict[str, list[MAWOParse]] = {}
            self._load_dictionary()
            _GLOBAL_DICTIONARY_CACHE = self.dictionary.copy()
            logger.info(
                f"üíæ OpenCorpora dictionary cached ({len(_GLOBAL_DICTIONARY_CACHE)} entries)",
            )
        else:
            logger.debug("‚ö° Using cached OpenCorpora dictionary - no reload needed!")  # type: ignore[unreachable]
            self.dictionary = _GLOBAL_DICTIONARY_CACHE.copy()

        if _GLOBAL_PATTERNS_CACHE is None:
            self.patterns: dict[str, Any] = {}
            self._init_patterns()
            _GLOBAL_PATTERNS_CACHE = self.patterns.copy()
        else:
            self.patterns = _GLOBAL_PATTERNS_CACHE.copy()  # type: ignore[unreachable]

        # –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å —Ç–µ—Å—Ç–∞–º–∏
        self._analyzer = self
        self._production_analyzer = None

        logger.info(
            f"‚úÖ MAWO Morphological Analyzer initialized with {len(self.dictionary)} entries",
        )

    def _get_cache_path(self, xml_path: Path) -> Path:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø—É—Ç–∏ –∫ pickle-–∫—ç—à—É —Å–ª–æ–≤–∞—Ä—è."""
        return xml_path.parent / f"{xml_path.stem}.pkl"

    def _is_cache_valid(self, xml_path: Path, cache_path: Path) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç–∏ –∫—ç—à–∞."""
        if not cache_path.exists():
            return False

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –∫—ç—à –Ω–æ–≤–µ–µ XML
        xml_mtime = xml_path.stat().st_mtime
        cache_mtime = cache_path.stat().st_mtime

        return bool(cache_mtime >= xml_mtime)

    def _load_from_cache(self, cache_path: Path) -> bool:
        """–ë—ã—Å—Ç—Ä–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ —Å–ª–æ–≤–∞—Ä—è –∏–∑ pickle-–∫—ç—à–∞."""
        try:
            logger.info(f"‚ö° Loading dictionary from cache: {cache_path.name}")

            if RICH_AVAILABLE:
                console = Console()
                console.print(
                    Panel(
                        "[bold cyan]‚ö° –ë—ã—Å—Ç—Ä–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –∏–∑ –∫—ç—à–∞[/bold cyan]\n"
                        "[dim]–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å...[/dim]",
                        title="OpenCorpora Cache",
                    ),
                )

            # Security: This is a locally-generated cache file, not user input
            # Validate file size before loading to prevent memory exhaustion
            import os

            cache_size = os.path.getsize(cache_path)
            if cache_size > 500_000_000:  # 500MB limit
                msg = f"Cache file too large: {cache_size} bytes"
                raise ValueError(msg)

            with open(cache_path, "rb") as f:
                # nosec B301 - This is a locally-generated cache file, not untrusted user input
                cached_data = pickle.load(f)  # nosec B301

            self.dictionary = cached_data["dictionary"]
            cache_info = cached_data.get("metadata", {})

            logger.info(f"‚úÖ Dictionary loaded from cache: {len(self.dictionary):,} entries")
            logger.info(f"üìä Cache created: {cache_info.get('created_at', 'unknown')}")
            logger.info(
                f"üöÄ Loading time: ~instant (vs {cache_info.get('original_parse_time', 'N/A')}s from XML)",
            )

            if RICH_AVAILABLE:
                console = Console()
                console.print(
                    Panel(
                        f"[bold green]‚úÖ –°–ª–æ–≤–∞—Ä—å –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ –∫—ç—à–∞[/bold green]\n"
                        f"[dim]{len(self.dictionary):,} –∑–∞–ø–∏—Å–µ–π ‚Ä¢ –ú–≥–Ω–æ–≤–µ–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞[/dim]",
                        title="–ì–æ—Ç–æ–≤–æ",
                    ),
                )

            return True

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Failed to load from cache: {e}, will parse XML")
            return False

    def _save_to_cache(self, cache_path: Path, parse_time: float) -> None:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è –≤ pickle-–∫—ç—à –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ –≤ –±—É–¥—É—â–µ–º."""
        try:
            from datetime import datetime

            logger.info(f"üíæ Saving dictionary to cache: {cache_path.name}")

            cache_data = {
                "dictionary": self.dictionary,
                "metadata": {
                    "created_at": datetime.now().isoformat(),
                    "entries_count": len(self.dictionary),
                    "original_parse_time": round(parse_time, 2),
                    "mawo_version": "2025.1",
                },
            }

            # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
            cache_path.parent.mkdir(parents=True, exist_ok=True)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º –ø—Ä–æ—Ç–æ–∫–æ–ª–æ–º pickle
            with open(cache_path, "wb") as f:
                pickle.dump(cache_data, f, protocol=pickle.HIGHEST_PROTOCOL)

            cache_size_mb = cache_path.stat().st_size / (1024 * 1024)
            logger.info(f"‚úÖ Cache saved: {cache_size_mb:.1f} MB")
            logger.info("üöÄ Future loads will be instant!")

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Failed to save cache (non-critical): {e}")

    def _load_dictionary(self) -> None:
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Å–ª–æ–≤–∞—Ä—è –∏–∑ OpenCorpora XML –∏–ª–∏ –∫—ç—à–∞."""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ä–µ–∂–∏–º–∞ (—Ç–µ—Å—Ç—ã)
        import os
        import time

        if os.environ.get("MAWO_FAST_MODE") == "1" or os.environ.get("PYTEST_CURRENT_TEST"):
            logger.info("Fast mode enabled, using basic dictionary")
            self._init_basic_dictionary()
            return

        # Use centralized path configuration
        try:
            from core.path_config import path_config  # type: ignore[import-not-found]

            opencorpora_path = (
                path_config.data_dir
                / "local_libs"
                / "opencorpora_2025"
                / "opencorpora_annot_2025.xml"
            )
        except ImportError:
            # Fallback if path_config is not available
            opencorpora_path = (
                Path(__file__).parent.parent.parent
                / "data"
                / "local_libs"
                / "opencorpora_2025"
                / "opencorpora_annot_2025.xml"
            )

        if opencorpora_path.exists():
            try:
                cache_path = self._get_cache_path(opencorpora_path)

                # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑ –∫—ç—à–∞
                if self._is_cache_valid(opencorpora_path, cache_path):
                    if self._load_from_cache(cache_path):
                        return  # –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ –∏–∑ –∫—ç—à–∞!

                # –ö—ç—à –Ω–µ –Ω–∞–π–¥–µ–Ω –∏–ª–∏ —É—Å—Ç–∞—Ä–µ–ª - –ø–∞—Ä—Å–∏–º XML
                logger.info("üìñ Cache not found or outdated, parsing XML (this will take time...)")

                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∑–∞–≥—Ä—É–∑–∫–∏
                if RICH_AVAILABLE:
                    console = Console()
                    console.print(
                        Panel(
                            "[bold yellow]üìñ –ü–µ—Ä–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ —Å–ª–æ–≤–∞—Ä—è OpenCorpora[/bold yellow]\n"
                            "[dim]–ü–∞—Ä—Å–∏–Ω–≥ XML –¥–∞–Ω–Ω—ã—Ö ‚Ä¢ –°–æ–∑–¥–∞–Ω–∏–µ –∫—ç—à–∞ –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –∑–∞–≥—Ä—É–∑–∫–∏...[/dim]",
                            title="OpenCorpora",
                        ),
                    )

                start_time = time.time()
                self._parse_opencorpora(opencorpora_path)
                parse_time = time.time() - start_time

                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∑–∞–≥—Ä—É–∑–∫–∏
                if RICH_AVAILABLE:
                    console = Console()
                    console.print(
                        Panel(
                            f"[bold green]‚úÖ –°–ª–æ–≤–∞—Ä—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω[/bold green]\n"
                            f"[dim]–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(self.dictionary):,} –∑–∞–ø–∏—Å–µ–π –∑–∞ {parse_time:.1f}—Å[/dim]",
                            title="–ì–æ—Ç–æ–≤–æ",
                        ),
                    )

                logger.info(
                    f"‚úÖ Loaded OpenCorpora dictionary: {len(self.dictionary):,} entries in {parse_time:.1f}s",
                )

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à –¥–ª—è –±—É–¥—É—â–∏—Ö –∑–∞–ø—É—Å–∫–æ–≤
                self._save_to_cache(cache_path, parse_time)

            except Exception as e:
                logger.warning(f"Failed to load OpenCorpora: {e}")
                self._init_basic_dictionary()
        else:
            logger.info("OpenCorpora not found, using basic dictionary")
            self._init_basic_dictionary()

    def _process_opencorpora_token(self, token: Any) -> None:
        """Process a single OpenCorpora token and add to dictionary."""
        word_attr = token.get("text")
        if not word_attr:
            return

        word = word_attr.lower()

        # Parse structure: token -> tfr -> v -> l
        for tfr in token.findall("tfr"):
            for v in tfr.findall("v"):
                for lemma in v.findall("l"):
                    normal_form = lemma.get("t", word)

                    # Extract POS from first g element
                    pos = "UNKN"
                    grammemes = set()
                    for gram in lemma.findall("g"):
                        gram_value = gram.get("v")
                        if gram_value:
                            if pos == "UNKN" and gram_value in {
                                "NOUN",
                                "VERB",
                                "ADJF",
                                "ADJS",
                                "COMP",
                                "INFN",
                                "PRTF",
                                "PRTS",
                                "GRND",
                                "NUMR",
                                "ADVB",
                                "NPRO",
                                "PRED",
                                "PREP",
                                "CONJ",
                                "PRCL",
                                "INTJ",
                                "PNCT",
                            }:
                                pos = gram_value
                            else:
                                grammemes.add(gram_value)

                    tag = MAWOTag(pos, grammemes)
                    parse_result = MAWOParse(word, normal_form, tag, 1.0, self)

                    if word not in self.dictionary:
                        self.dictionary[word] = []
                    self.dictionary[word].append(parse_result)

    def _parse_opencorpora(self, xml_path: Path) -> None:
        """–ü–∞—Ä—Å–∏–Ω–≥ OpenCorpora XML —Å –ø—Ä–æ–≥—Ä–µ—Å—Å –±–∞—Ä–æ–º."""
        try:
            if ET_PARSE_SAFE:
                tree = defusedxml_parse(xml_path)
            else:
                import xml.etree.ElementTree as ET  # noqa: N817

                tree = ET.parse(xml_path)  # nosec B314
            root = tree.getroot()

            all_tokens = root.findall(".//token")
            total_tokens = len(all_tokens)

            # –ò—Å–ø–æ–ª—å–∑—É–µ–º tqdm –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç—ã –≤ –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É
            try:
                from tqdm import tqdm

                if total_tokens > 0:
                    with tqdm(
                        total=total_tokens,
                        desc="üìñ –ó–∞–≥—Ä—É–∑–∫–∞ OpenCorpora —Å–ª–æ–≤–∞—Ä—è",
                        unit="token",
                        dynamic_ncols=True,
                        leave=False,  # –ù–µ –æ—Å—Ç–∞–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
                        disable=total_tokens < 1000,  # –û—Ç–∫–ª—é—á–∞–µ–º –¥–ª—è –º–∞–ª–µ–Ω—å–∫–∏—Ö —Å–ª–æ–≤–∞—Ä–µ–π
                    ) as pbar:
                        for i, token in enumerate(all_tokens):
                            self._process_opencorpora_token(token)
                            if (i + 1) % 100 == 0:
                                pbar.update(100)
                        # –û–±–Ω–æ–≤–ª—è–µ–º –¥–æ –∫–æ–Ω—Ü–∞ –µ—Å–ª–∏ –æ—Å—Ç–∞–ª–∏—Å—å –Ω–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
                        pbar.update(total_tokens - pbar.n)
                else:
                    for token in all_tokens:
                        self._process_opencorpora_token(token)
            except ImportError:
                # Fallback –±–µ–∑ –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–∞ –µ—Å–ª–∏ tqdm –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω
                for token in all_tokens:
                    self._process_opencorpora_token(token)

        except Exception as e:
            logger.exception(f"Error parsing OpenCorpora: {e}")
            raise

    def _init_basic_dictionary(self) -> None:
        """–ë–∞–∑–æ–≤—ã–π —Å–ª–æ–≤–∞—Ä—å –¥–ª—è fail_fast_mode."""
        basic_words = {
            # –ú–µ—Å—Ç–æ–∏–º–µ–Ω–∏—è
            "—è": [("—è", "NPRO", {"sing", "1per", "nomn"})],
            "—Ç—ã": [("—Ç—ã", "NPRO", {"sing", "2per", "nomn"})],
            "–æ–Ω": [("–æ–Ω", "NPRO", {"sing", "3per", "masc", "nomn"})],
            "–æ–Ω–∞": [("–æ–Ω–∞", "NPRO", {"sing", "3per", "femn", "nomn"})],
            "–æ–Ω–æ": [("–æ–Ω–æ", "NPRO", {"sing", "3per", "neut", "nomn"})],
            "–º—ã": [("–º—ã", "NPRO", {"plur", "1per", "nomn"})],
            "–≤—ã": [("–≤—ã", "NPRO", {"plur", "2per", "nomn"})],
            "–æ–Ω–∏": [("–æ–Ω–∏", "NPRO", {"plur", "3per", "nomn"})],
            # –ì–ª–∞–≥–æ–ª—ã
            "–±—ã—Ç—å": [("–±—ã—Ç—å", "INFN", {"impf"})],
            "–µ—Å—Ç—å": [("–±—ã—Ç—å", "VERB", {"pres", "3per", "sing"})],
            "–±—ã–ª": [("–±—ã—Ç—å", "VERB", {"past", "masc", "sing"})],
            "–±—ã–ª–∞": [("–±—ã—Ç—å", "VERB", {"past", "femn", "sing"})],
            "–±—ã–ª–æ": [("–±—ã—Ç—å", "VERB", {"past", "neut", "sing"})],
            "–±—ã–ª–∏": [("–±—ã—Ç—å", "VERB", {"past", "plur"})],
            # –°—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ
            "–¥–æ–º": [("–¥–æ–º", "NOUN", {"masc", "inan", "nomn", "sing"})],
            "–¥–æ–º–∞": [
                ("–¥–æ–º", "NOUN", {"masc", "inan", "gent", "sing"}),
                ("–¥–æ–º", "NOUN", {"masc", "inan", "nomn", "plur"}),
            ],
            "—à–∫–æ–ª–∞": [("—à–∫–æ–ª–∞", "NOUN", {"femn", "inan", "nomn", "sing"})],
            "—à–∫–æ–ª—ã": [
                ("—à–∫–æ–ª–∞", "NOUN", {"femn", "inan", "gent", "sing"}),
                ("—à–∫–æ–ª–∞", "NOUN", {"femn", "inan", "nomn", "plur"}),
            ],
        }

        for word, forms in basic_words.items():
            self.dictionary[word] = []
            for normal_form, pos, grammemes in forms:
                tag = MAWOTag(pos, grammemes)
                parse_result = MAWOParse(word, normal_form, tag, 0.8, self)
                self.dictionary[word].append(parse_result)

    def _init_patterns(self) -> None:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤."""
        self.patterns = {
            # –û–∫–æ–Ω—á–∞–Ω–∏—è —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö
            "noun_endings": {
                "–∞": ("femn", "nomn", "sing"),
                "—ã": ("femn", "nomn", "plur"),
                "–æ–π": ("femn", "gent", "sing"),
                "–µ–º": ("masc", "ablt", "sing"),
                "–∞–º–∏": ("femn", "ablt", "plur"),
            },
            # –û–∫–æ–Ω—á–∞–Ω–∏—è –≥–ª–∞–≥–æ–ª–æ–≤
            "verb_endings": {
                "—Ç—å": ("INFN", set()),
                "—Ç–∏": ("INFN", set()),
                "—á—å": ("INFN", set()),
                "–µ—Ç": ("VERB", {"3per", "sing", "pres"}),
                "—É—Ç": ("VERB", {"3per", "plur", "pres"}),
                "—é—Ç": ("VERB", {"3per", "plur", "pres"}),
            },
            # –û–∫–æ–Ω—á–∞–Ω–∏—è –ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω—ã—Ö
            "adj_endings": {
                "—ã–π": ("ADJF", {"masc", "nomn", "sing"}),
                "–∏–π": ("ADJF", {"masc", "nomn", "sing"}),
                "–æ–π": ("ADJF", {"masc", "nomn", "sing"}),
                "–∞—è": ("ADJF", {"femn", "nomn", "sing"}),
                "—è—è": ("ADJF", {"femn", "nomn", "sing"}),
                "–æ–µ": ("ADJF", {"neut", "nomn", "sing"}),
                "–µ–µ": ("ADJF", {"neut", "nomn", "sing"}),
            },
        }

    def parse(self, word: str) -> list[MAWOParse]:
        """–ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Å–ª–æ–≤–∞.

        Args:
            word: –°–ª–æ–≤–æ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞

        Returns:
            –°–ø–∏—Å–æ–∫ –≤–æ–∑–º–æ–∂–Ω—ã—Ö —Ä–∞–∑–±–æ—Ä–æ–≤ —Å–ª–æ–≤–∞

        """
        if not word or not word.strip():
            return []

        word_clean = word.lower().strip()

        # –ï—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º DAWG —á–µ—Ä–µ–∑ DAWGDictionary
        if self.use_dawg and self._dawg_dict:
            try:
                # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–±–æ—Ä—ã —Å–ª–æ–≤–∞ –∏–∑ DAWG
                word_parses = self._dawg_dict.get_word_parses(word_clean)

                mawo_parses = []
                for paradigm_id, word_idx in word_parses:
                    # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–∞—Ä–∞–¥–∏–≥–º–µ
                    paradigm_info = self._dawg_dict.get_paradigm(paradigm_id, word_idx)

                    if paradigm_info is None:
                        continue

                    suffix, tag_string, prefix = paradigm_info

                    # –†–∞–∑–±–∏—Ä–∞–µ–º —Ç–µ–≥
                    pos, grammemes = self._dawg_dict.parse_tag_string(tag_string)

                    # –í—ã—á–∏—Å–ª—è–µ–º –Ω–æ—Ä–º–∞–ª—å–Ω—É—é —Ñ–æ—Ä–º—É
                    # –ü–æ–ª—É—á–∞–µ–º –ø–µ—Ä–≤—É—é —Ñ–æ—Ä–º—É –ø–∞—Ä–∞–¥–∏–≥–º—ã (word_idx=0)
                    normal_form_info = self._dawg_dict.get_paradigm(paradigm_id, 0)
                    if normal_form_info:
                        normal_suffix, _, normal_prefix = normal_form_info
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Å–Ω–æ–≤—É (stem)
                        # –£–¥–∞–ª—è–µ–º –ø—Ä–µ—Ñ–∏–∫—Å –∏ —Å—É—Ñ—Ñ–∏–∫—Å –∏–∑ —Ç–µ–∫—É—â–µ–≥–æ —Å–ª–æ–≤–∞
                        stem = word_clean
                        if prefix and stem.startswith(prefix):
                            stem = stem[len(prefix) :]
                        if suffix and stem.endswith(suffix):
                            stem = stem[: -len(suffix)]

                        # –°–æ–±–∏—Ä–∞–µ–º –Ω–æ—Ä–º–∞–ª—å–Ω—É—é —Ñ–æ—Ä–º—É
                        normal_form = normal_prefix + stem + normal_suffix
                    else:
                        normal_form = word_clean

                    mawo_tag = MAWOTag(pos, grammemes)
                    mawo_parse = MAWOParse(
                        word=word_clean,
                        normal_form=normal_form,
                        tag=mawo_tag,
                        score=1.0,
                        analyzer=self,
                    )
                    mawo_parses.append(mawo_parse)

                if mawo_parses:
                    return mawo_parses

            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞–∑–±–æ—Ä–µ —á–µ—Ä–µ–∑ DAWG: {e}")
                # Fallback –∫ –æ–±—ã—á–Ω–æ–º—É –º–µ—Ç–æ–¥—É

        # Fallback: —Å–Ω–∞—á–∞–ª–∞ –∏—â–µ–º –≤ —Å–ª–æ–≤–∞—Ä–µ
        if word_clean in self.dictionary:
            # –î–æ–±–∞–≤–ª—è–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–ª—è inflect()
            parses = self.dictionary[word_clean]
            for parse in parses:
                parse._analyzer = self
            return parses

        # –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        return self._analyze_by_patterns(word_clean)

    def _analyze_by_patterns(self, word: str) -> list[MAWOParse]:
        """–ê–Ω–∞–ª–∏–∑ —Å–ª–æ–≤–∞ –ø–æ –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–º –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º."""
        results: list[Any] = []

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–∫–æ–Ω—á–∞–Ω–∏—è –≥–ª–∞–≥–æ–ª–æ–≤
        for ending, (pos, grammemes) in self.patterns["verb_endings"].items():
            if word.endswith(ending) and len(word) > len(ending):
                normal_form = word[: -len(ending)] + "—Ç—å" if pos == "INFN" else word
                tag = MAWOTag(pos, grammemes)
                results.append(MAWOParse(word, normal_form, tag, 0.6, self))

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–∫–æ–Ω—á–∞–Ω–∏—è –ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω—ã—Ö
        for ending, (pos, grammemes) in self.patterns["adj_endings"].items():
            if word.endswith(ending) and len(word) > len(ending):
                normal_form = word[: -len(ending)] + "—ã–π"
                tag = MAWOTag(pos, grammemes)
                results.append(MAWOParse(word, normal_form, tag, 0.6, self))

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–∫–æ–Ω—á–∞–Ω–∏—è —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö
        for ending, (gender, case, number) in self.patterns["noun_endings"].items():
            if word.endswith(ending) and len(word) > len(ending):
                normal_form = word  # –£–ø—Ä–æ—â–µ–Ω–Ω–æ - –Ω—É–∂–Ω–æ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ
                grammemes = {gender, case, number, "inan"}
                tag = MAWOTag("NOUN", grammemes)
                results.append(MAWOParse(word, normal_form, tag, 0.4, self))

        # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–µ —Å–ª–æ–≤–æ
        if not results:
            tag = MAWOTag("UNKN", set())
            results.append(MAWOParse(word, word, tag, 0.1, self))

        return results


class MAWOOptimizedMorphAnalyzer:
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–ª—è MAWO —Å–∏—Å—Ç–µ–º—ã
    –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –º–µ—Ç–æ–¥–∞–º–∏ –æ–±—É—á–µ–Ω–∏—è.
    """

    def __init__(self, dict_path: str | None = None) -> None:
        self.base_analyzer = create_analyzer(dict_path)
        self.cache: dict[str, list[dict[str, Any]]] = {}
        logger.info("‚úÖ MAWOOptimizedMorphAnalyzer initialized")

    def analyze(self, text: str) -> list[dict[str, Any]]:
        """–ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.

        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞

        Returns:
            –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞

        """
        if not text:
            return []

        if text in self.cache:
            return self.cache[text]

        words = text.split()
        results: list[Any] = []

        for word in words:
            if word.isalpha():
                parses = self.base_analyzer.parse(word)
                if parses:
                    best_parse = parses[0]  # –ë–µ—Ä–µ–º –ª—É—á—à–∏–π —Ä–∞–∑–±–æ—Ä
                    results.append(
                        {
                            "word": word,
                            "normal_form": best_parse.normal_form,
                            "pos": best_parse.tag.POS,
                            "case": best_parse.tag.case,
                            "number": best_parse.tag.number,
                            "gender": best_parse.tag.gender,
                            "aspect": best_parse.tag.aspect,
                            "tense": best_parse.tag.tense,
                            "score": best_parse.score,
                            "analysis_mode": "mawo_morphology",
                        },
                    )

        # –ö—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        self.cache[text] = results
        return results


def create_analyzer(dict_path: str | None = None, use_dawg: bool = True) -> MAWOMorphAnalyzer:
    """–°–æ–∑–¥–∞–µ—Ç –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä MAWO (—Å–∏–Ω–≥–ª—Ç–æ–Ω).

    –í–ê–ñ–ù–û: –ò—Å–ø–æ–ª—å–∑—É–µ—Ç DAWG —Å–ª–æ–≤–∞—Ä–∏ –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ –∏ –º–∞–ª–æ–≥–æ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—è –ø–∞–º—è—Ç–∏.
    - –ó–∞–≥—Ä—É–∑–∫–∞: ~1-2 —Å–µ–∫—É–Ω–¥—ã
    - –ü–∞–º—è—Ç—å: ~15-20 –ú–ë (–≤–º–µ—Å—Ç–æ ~500 –ú–ë)
    - Thread-safe —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è —Å double-checked locking

    Args:
        dict_path: –ü—É—Ç—å –∫ —Å–ª–æ–≤–∞—Ä—é (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é dicts_ru/)
        use_dawg: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å DAWG –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é True)

    Returns:
        –≠–∫–∑–µ–º–ø–ª—è—Ä MAWOMorphAnalyzer (—Å–∏–Ω–≥–ª—Ç–æ–Ω –≤ —Ä–∞–º–∫–∞—Ö –ø—Ä–æ—Ü–µ—Å—Å–∞)

    """
    global _GLOBAL_ANALYZER_INSTANCE, _ANALYZER_LOCK

    # –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –±–µ–∑ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏
    if _GLOBAL_ANALYZER_INSTANCE is not None:
        logger.debug("‚ö° Returning existing singleton analyzer instance (fast path)")  # type: ignore[unreachable]
        return _GLOBAL_ANALYZER_INSTANCE

    # –ú–µ–¥–ª–µ–Ω–Ω—ã–π –ø—É—Ç—å —Å –±–ª–æ–∫–∏—Ä–æ–≤–∫–æ–π
    if _ANALYZER_LOCK:
        with _ANALYZER_LOCK:
            # Double-checked locking: –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å–Ω–æ–≤–∞ –≤–Ω—É—Ç—Ä–∏ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏
            if _GLOBAL_ANALYZER_INSTANCE is None:
                logger.info("üîÑ Creating new singleton analyzer instance (thread-safe)")
                _GLOBAL_ANALYZER_INSTANCE = MAWOMorphAnalyzer(dict_path, use_dawg=use_dawg)
            else:
                logger.debug("‚ö° Another thread created instance, using it")  # type: ignore[unreachable]
    # PRODUCTION REQUIRED –±–µ–∑ threading (fallback –¥–ª—è —Å—Ç–∞—Ä—ã—Ö —Å–∏—Å—Ç–µ–º)
    elif _GLOBAL_ANALYZER_INSTANCE is None:
        logger.info("üîÑ Creating new singleton analyzer instance (no threading)")
        _GLOBAL_ANALYZER_INSTANCE = MAWOMorphAnalyzer(dict_path, use_dawg=use_dawg)

    return _GLOBAL_ANALYZER_INSTANCE


def get_global_analyzer() -> MAWOMorphAnalyzer:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ (—Å–∏–Ω–≥–ª—Ç–æ–Ω).

    –≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω–∞ create_analyzer() –±–µ–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤,
    –Ω–æ —è–≤–Ω–æ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–µ—Ç —á—Ç–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç—Å—è –≥–ª–æ–±–∞–ª—å–Ω—ã–π —Å–∏–Ω–≥–ª—Ç–æ–Ω.

    Returns:
        –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä MAWOMorphAnalyzer
    """
    return create_analyzer()


class MAWODictionaryManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è DAWG –∫—ç—à–µ–º –∏ —Å–ª–æ–≤–∞—Ä—è–º–∏ OpenCorpora."""

    def __init__(self, dict_path: Path | None = None) -> None:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–µ–Ω–µ–¥–∂–µ—Ä–∞ —Å–ª–æ–≤–∞—Ä–µ–π.

        Args:
            dict_path: –ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å–æ —Å–ª–æ–≤–∞—Ä—è–º–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        """
        if dict_path is None:
            dict_path = Path(__file__).parent / "dicts_ru"
        self.dict_path = Path(dict_path)
        self.dawg_cache_path = self.dict_path / "words.dawg"

    def is_dawg_cache_available(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ DAWG –∫—ç—à–∞.

        Returns:
            True –µ—Å–ª–∏ DAWG —Å–ª–æ–≤–∞—Ä—å —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        """
        return self.dawg_cache_path.exists()

    def build_dawg_cache(self) -> bool:
        """–°–æ–∑–¥–∞–µ—Ç DAWG –∫—ç—à –∏–∑ OpenCorpora XML.

        Returns:
            True –µ—Å–ª–∏ –∫—ç—à —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω
        """
        logger.info("üî® Building DAWG cache from OpenCorpora XML...")

        try:
            # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º DAWG optimizer
            from .dawg_optimizer import get_dawg_optimizer

            optimizer = get_dawg_optimizer()

            if not optimizer.is_available():
                logger.error("‚ùå DAWG library not available. Install with: pip install dawg-python")
                return False

            # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Å–ª–æ–≤–∞—Ä—è
            analyzer = MAWOMorphAnalyzer()

            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Å–ª–æ–≤–∞—Ä—å –≤ DAWG
            dawg_dict = optimizer.convert_dict_to_dawg(analyzer.dictionary)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º DAWG –∫—ç—à
            optimizer.save_dawg_cache(dawg_dict, self.dawg_cache_path)

            logger.info("‚úÖ DAWG cache built successfully!")
            return True

        except Exception as e:
            logger.exception(f"‚ùå Failed to build DAWG cache: {e}")
            return False

    def get_cache_info(self) -> dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫—ç—à–µ.

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –∫—ç—à–µ
        """
        info: dict[str, Any] = {
            "dict_path": str(self.dict_path),
            "dawg_available": self.is_dawg_cache_available(),
        }

        if self.is_dawg_cache_available():
            info["dawg_size_mb"] = self.dawg_cache_path.stat().st_size / (1024 * 1024)

        return info


# –≠–∫—Å–ø–æ—Ä—Ç –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å pymorphy3
MorphAnalyzer = MAWOMorphAnalyzer

# –û—Å–Ω–æ–≤–Ω—ã–µ —ç–∫—Å–ø–æ—Ä—Ç—ã
__all__ = [
    "MAWOMorphAnalyzer",
    "MAWOOptimizedMorphAnalyzer",
    "MAWOParse",
    "MAWOTag",
    "MorphAnalyzer",
    "create_analyzer",
    "get_global_analyzer",
    "MAWODictionaryManager",
]
