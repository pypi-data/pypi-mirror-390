"""MAWO –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç —É–ª—É—á—à–µ–Ω–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å OpenCorpora –∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã.
"""

from __future__ import annotations

import logging
import pickle
from pathlib import Path
from typing import Any

try:
    from defusedxml.ElementTree import parse as defusedxml_parse  # type: ignore[import-not-found]

    ET_PARSE_SAFE = True
except ImportError:
    ET_PARSE_SAFE = False

# Rich –¥–ª—è –ø—Ä–æ–≥—Ä–µ—Å—Å –±–∞—Ä–∞ –∏ –∫—Ä–∞—Å–∏–≤–æ–≥–æ –≤—ã–≤–æ–¥–∞
try:
    from rich.console import Console
    from rich.panel import Panel

    RICH_AVAILABLE = True
except ImportError:
    RICH_AVAILABLE = False

# –ò—Å–ø—Ä–∞–≤–ª—è–µ–º getargspec –ø—Ä–æ–±–ª–µ–º—É –¥–ª—è Python 3.11+
import inspect

if not hasattr(inspect, "getargspec"):
    inspect.getargspec = inspect.getfullargspec  # type: ignore[assignment]

logger = logging.getLogger(__name__)

# –ì–ª–æ–±–∞–ª—å–Ω—ã–π –∫—ç—à –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ OpenCorpora —Å–ª–æ–≤–∞—Ä—è
_GLOBAL_DICTIONARY_CACHE = None
_GLOBAL_PATTERNS_CACHE = None

# –°–∏–Ω–≥–ª—Ç–æ–Ω —ç–∫–∑–µ–º–ø–ª—è—Ä –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–π –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏
_GLOBAL_ANALYZER_INSTANCE = None
_ANALYZER_LOCK = None

# –ò–º–ø–æ—Ä—Ç threading –µ—Å–ª–∏ –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω
try:
    import threading

    _ANALYZER_LOCK = threading.Lock()
except ImportError:
    _ANALYZER_LOCK = None


class MAWOTag:
    """–ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π —Ç–µ–≥ –≤ —Ñ–æ—Ä–º–∞—Ç–µ MAWO."""

    # Mapping —Ä–µ–¥–∫–∏—Ö –ø–∞–¥–µ–∂–µ–π –Ω–∞ –æ–±—ã—á–Ω—ã–µ (–∫–∞–∫ –≤ pymorphy2)
    RARE_CASES = {
        "gen1": "gent",
        "gen2": "gent",
        "acc1": "accs",
        "acc2": "accs",
        "loc1": "loct",
        "loc2": "loct",
        "voct": "nomn",
    }

    def __init__(self, pos: str = "UNKN", grammemes: set[str] | None = None) -> None:
        self.POS = pos
        self.grammemes = grammemes or set()

    @classmethod
    def fix_rare_cases(cls, grammemes: set[str]) -> set[str]:
        """
        Replace rare cases (loc2/voct/...) with common ones (loct/nomn/...).
        –ö–∞–∫ –≤ pymorphy2.
        """
        return {cls.RARE_CASES.get(g, g) for g in grammemes}

    @property
    def case(self) -> str | None:
        cases = {"nomn", "gent", "datv", "accs", "ablt", "loct", "voct"}
        return next((g for g in self.grammemes if g in cases), None)

    @property
    def number(self) -> str | None:
        numbers = {"sing", "plur"}
        return next((g for g in self.grammemes if g in numbers), None)

    @property
    def gender(self) -> str | None:
        genders = {"masc", "femn", "neut"}
        return next((g for g in self.grammemes if g in genders), None)

    @property
    def aspect(self) -> str | None:
        aspects = {"perf", "impf"}
        return next((g for g in self.grammemes if g in aspects), None)

    @property
    def tense(self) -> str | None:
        tenses = {"past", "pres", "futr"}
        return next((g for g in self.grammemes if g in tenses), None)

    def __contains__(self, item: Any) -> bool:
        return item in self.grammemes or item == self.POS

    def __str__(self) -> str:
        if not self.grammemes:
            return str(self.POS)
        return f"{self.POS} {','.join(sorted(self.grammemes))}"

    def __eq__(self, other: Any) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–≤–µ–Ω—Å—Ç–≤–∞ —Ç–µ–≥–æ–≤."""
        if not isinstance(other, MAWOTag):
            return False
        return self.POS == other.POS and self.grammemes == other.grammemes

    def __hash__(self) -> int:
        """–•–µ—à —Ç–µ–≥–∞ –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –≤ set/dict."""
        return hash((self.POS, frozenset(self.grammemes)))


class MAWOParse:
    """–†–µ–∑—É–ª—å—Ç–∞—Ç –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞."""

    def __init__(
        self,
        word: str,
        normal_form: str,
        tag: MAWOTag,
        score: float = 1.0,
        analyzer: Any | None = None,
        paradigm_id: int | None = None,
        stem: str | None = None,
    ) -> None:
        self.word = word
        self.normal_form = normal_form
        self.tag = tag
        self.score = score
        self._analyzer = analyzer
        self._paradigm_id = paradigm_id
        self._stem = stem

    def inflect(self, required_grammemes: set[str]) -> MAWOParse | None:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ª–æ–≤–æ—Ñ–æ—Ä–º—ã —Å –∑–∞–¥–∞–Ω–Ω—ã–º–∏ –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏.

        –ê–ª–≥–æ—Ä–∏—Ç–º –∫–∞–∫ –≤ pymorphy2:
        1. –ü–æ–ª—É—á–∏—Ç—å –ª–µ–∫—Å–µ–º—É (–≤—Å–µ —Ñ–æ—Ä–º—ã —Å–ª–æ–≤–∞)
        2. –ù–∞–π—Ç–∏ —Ñ–æ—Ä–º—ã, —Å–æ–¥–µ—Ä–∂–∞—â–∏–µ —Ç—Ä–µ–±—É–µ–º—ã–µ –≥—Ä–∞–º–º–µ–º—ã
        3. –í—ã–±—Ä–∞—Ç—å –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ—Ö–æ–∂—É—é —Ñ–æ—Ä–º—É

        Args:
            required_grammemes: –ú–Ω–æ–∂–µ—Å—Ç–≤–æ —Ç—Ä–µ–±—É–µ–º—ã—Ö –≥—Ä–∞–º–º–µ–º (–Ω–∞–ø—Ä–∏–º–µ—Ä, {"sing", "femn"})

        Returns:
            MAWOParse —Å –Ω—É–∂–Ω—ã–º–∏ –≥—Ä–∞–º–º–µ–º–∞–º–∏ –∏–ª–∏ None –µ—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ
        """
        if not self._analyzer:
            logger.warning("Analyzer not available for inflection")
            return None

        # –ü–æ–ª—É—á–∞–µ–º –ª–µ–∫—Å–µ–º—É (–≤—Å–µ —Ñ–æ—Ä–º—ã —Å–ª–æ–≤–∞)
        lexeme = self.lexeme

        # –ò—â–µ–º —Ñ–æ—Ä–º—ã, —Å–æ–¥–µ—Ä–∂–∞—â–∏–µ —Ç—Ä–µ–±—É–µ–º—ã–µ –≥—Ä–∞–º–º–µ–º—ã (—Å–Ω–∞—á–∞–ª–∞ —Å –∏—Å—Ö–æ–¥–Ω—ã–º–∏ —Ä–µ–¥–∫–∏–º–∏ –ø–∞–¥–µ–∂–∞–º–∏)
        possible_results = []
        for form in lexeme:
            form_tags = form.tag.grammemes | {form.tag.POS}
            if required_grammemes.issubset(form_tags):
                possible_results.append(form)

        # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ —Å —Ä–µ–¥–∫–∏–º–∏ –ø–∞–¥–µ–∂–∞–º–∏, –ø—Ä–æ–±—É–µ–º –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞—Ç—å
        if not possible_results:
            normalized_grammemes = MAWOTag.fix_rare_cases(required_grammemes)
            # –ï—Å–ª–∏ –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è —á—Ç–æ-—Ç–æ –∏–∑–º–µ–Ω–∏–ª–∞, –ø—Ä–æ–±—É–µ–º –µ—â–µ —Ä–∞–∑
            if normalized_grammemes != required_grammemes:
                for form in lexeme:
                    form_tags = form.tag.grammemes | {form.tag.POS}
                    if normalized_grammemes.issubset(form_tags):
                        possible_results.append(form)

        # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º None
        if not possible_results:
            return None

        # –ï—Å–ª–∏ –Ω–∞–π–¥–µ–Ω–∞ –æ–¥–Ω–∞ —Ñ–æ—Ä–º–∞, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –µ–µ
        if len(possible_results) == 1:
            return possible_results[0]

        # –ï—Å–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ñ–æ—Ä–º, –≤—ã–±–∏—Ä–∞–µ–º –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ—Ö–æ–∂—É—é
        # –õ–æ–≥–∏–∫–∞ similarity –∏–∑ pymorphy2:
        # similarity = len(common_grammemes) - 0.1 * len(symmetric_difference)
        source_grammemes = self.tag.grammemes | {self.tag.POS}

        def similarity(form: MAWOParse) -> float:
            form_grammemes = form.tag.grammemes | {form.tag.POS}
            common = source_grammemes & form_grammemes
            diff = source_grammemes ^ form_grammemes
            return len(common) - 0.1 * len(diff)

        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ñ–æ—Ä–º—É —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π similarity
        return max(possible_results, key=similarity)

    def _inflect_legacy(self, required_grammemes: set[str]) -> MAWOParse | None:
        """–°—Ç–∞—Ä—ã–π –º–µ—Ç–æ–¥ inflect (–æ—Å—Ç–∞–≤–ª–µ–Ω –¥–ª—è —Å–ø—Ä–∞–≤–∫–∏).

        –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–ª—Å—è –¥–æ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ lexeme-based –ø–æ–¥—Ö–æ–¥–∞.
        """
        if not self._analyzer:
            logger.warning("Analyzer not available for inflection")
            return None

        # –ï—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è DAWG –∏ –µ—Å—Ç—å paradigm_id
        if (
            hasattr(self._analyzer, "_dawg_dict")
            and self._analyzer._dawg_dict
            and self._paradigm_id is not None
            and self._stem is not None
        ):
            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ —Ñ–æ—Ä–º—ã –ø–∞—Ä–∞–¥–∏–≥–º—ã
            paradigm_forms = self._analyzer._dawg_dict.get_all_paradigm_forms(self._paradigm_id)

            # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –≥—Ä–∞–º–º–µ–º—ã –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
            def get_grammeme_groups():
                return {
                    "number": {"sing", "plur"},
                    "tense": {"past", "pres", "futr"},
                    "gender": {"masc", "femn", "neut"},
                    "case": {
                        "nomn",
                        "gent",
                        "datv",
                        "accs",
                        "ablt",
                        "loct",
                        "voct",
                        "gen2",
                        "acc2",
                        "loc2",
                    },
                    "person": {"1per", "2per", "3per"},
                    "aspect": {"perf", "impf"},
                    "voice": {"actv", "pssv"},
                    "animacy": {"anim", "inan"},
                }

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, –∫–∞–∫–∏–µ –≥—Ä–∞–º–º–µ–º—ã –∏–∑ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ–≤–∞ –Ω—É–∂–Ω–æ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å
            source_grammemes_to_preserve = set()
            grammeme_groups = get_grammeme_groups()

            # –î–ª—è –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö —á–∞—Å—Ç–µ–π —Ä–µ—á–∏ –Ω–µ –Ω—É–∂–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–µ –≥—Ä–∞–º–º–µ–º—ã
            # GRND (–¥–µ–µ–ø—Ä–∏—á–∞—Å—Ç–∏–µ) - –Ω–µ –∏–º–µ–µ—Ç —Ä–æ–¥–∞, —á–∏—Å–ª–∞, –ª–∏—Ü–∞, –ø–∞–¥–µ–∂–∞ (–Ω–æ –ò–ú–ï–ï–¢ –≤—Ä–µ–º—è: past/pres)
            # INFN (–∏–Ω—Ñ–∏–Ω–∏—Ç–∏–≤) - –Ω–µ –∏–º–µ–µ—Ç –≤—Ä–µ–º–µ–Ω–∏, —Ä–æ–¥–∞, —á–∏—Å–ª–∞, –ª–∏—Ü–∞, –ø–∞–¥–µ–∂–∞
            # COMP (—Å—Ä–∞–≤–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Å—Ç–µ–ø–µ–Ω—å) - –Ω–µ –∏–º–µ–µ—Ç —Ä–æ–¥–∞, —á–∏—Å–ª–∞, –ø–∞–¥–µ–∂–∞
            pos_incompatible_groups = {
                "GRND": {"gender", "number", "person", "case"},
                "INFN": {"tense", "gender", "number", "person", "case"},
                "COMP": {"gender", "number", "case"},
                "PRTS": {"case"},  # –ö—Ä–∞—Ç–∫–æ–µ –ø—Ä–∏—á–∞—Å—Ç–∏–µ - –Ω–µ—Ç –ø–∞–¥–µ–∂–∞
                "ADJS": {"case"},  # –ö—Ä–∞—Ç–∫–æ–µ –ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω–æ–µ - –Ω–µ—Ç –ø–∞–¥–µ–∂–∞
            }

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ü–µ–ª–µ–≤–æ–π POS (–∏–∑ required_grammemes –∏–ª–∏ –∏–∑ —Ç–µ–∫—É—â–µ–≥–æ —Ç–µ–≥–∞)
            target_pos = None
            pos_set = {
                "NOUN",
                "VERB",
                "ADJF",
                "ADJS",
                "COMP",
                "INFN",
                "PRTF",
                "PRTS",
                "GRND",
                "NUMR",
                "ADVB",
                "NPRO",
                "PRED",
                "PREP",
                "CONJ",
                "PRCL",
                "INTJ",
            }
            for pos_candidate in required_grammemes & pos_set:
                target_pos = pos_candidate
                break

            for group_name, group_grammemes in grammeme_groups.items():
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –≥—Ä—É–ø–ø—ã, –Ω–µ—Å–æ–≤–º–µ—Å—Ç–∏–º—ã–µ —Å —Ü–µ–ª–µ–≤—ã–º POS
                if target_pos and target_pos in pos_incompatible_groups:
                    if group_name in pos_incompatible_groups[target_pos]:
                        continue

                # –ï—Å–ª–∏ –≤ required_grammemes –Ω–µ—Ç –≥—Ä–∞–º–º–µ–º —ç—Ç–æ–π –≥—Ä—É–ø–ø—ã, –±–µ—Ä–µ–º –∏–∑ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ
                if not (required_grammemes & group_grammemes):
                    source_grammemes_to_preserve.update(self.tag.grammemes & group_grammemes)

            # –û–±—ä–µ–¥–∏–Ω—è–µ–º —Ç—Ä–µ–±—É–µ–º—ã–µ –≥—Ä–∞–º–º–µ–º—ã —Å —Å–æ—Ö—Ä–∞–Ω—è–µ–º—ã–º–∏ –∏–∑ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Å–ª–æ–≤–∞
            target_grammemes = required_grammemes | source_grammemes_to_preserve

            # –ò—â–µ–º —Ñ–æ—Ä–º—É —Å –Ω—É–∂–Ω—ã–º–∏ –≥—Ä–∞–º–º–µ–º–∞–º–∏
            for suffix, tag_string, prefix in paradigm_forms:
                pos, grammemes = self._analyzer._dawg_dict.parse_tag_string(tag_string)

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –≥—Ä–∞–º–º–µ–º –ò/–ò–õ–ò POS
                all_tags = grammemes | {pos}
                if target_grammemes.issubset(all_tags):
                    # –°–æ–±–∏—Ä–∞–µ–º —Å–ª–æ–≤–æ
                    inflected_word = prefix + self._stem + suffix

                    # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π parse
                    new_tag = MAWOTag(pos, grammemes)
                    return MAWOParse(
                        word=inflected_word,
                        normal_form=self.normal_form,
                        tag=new_tag,
                        score=self.score,
                        analyzer=self._analyzer,
                        paradigm_id=self._paradigm_id,
                        stem=self._stem,
                    )

            # –ï—Å–ª–∏ —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –∏—â–µ–º —á–∞—Å—Ç–∏—á–Ω–æ–µ
            best_match = None
            best_match_score = 0

            for suffix, tag_string, prefix in paradigm_forms:
                pos, grammemes = self._analyzer._dawg_dict.parse_tag_string(tag_string)

                all_tags = grammemes | {pos}
                matching_grammemes = all_tags & target_grammemes
                match_score = len(matching_grammemes)

                if match_score > best_match_score:
                    best_match_score = match_score
                    inflected_word = prefix + self._stem + suffix
                    new_tag = MAWOTag(pos, grammemes)
                    best_match = MAWOParse(
                        word=inflected_word,
                        normal_form=self.normal_form,
                        tag=new_tag,
                        score=self.score,
                        analyzer=self._analyzer,
                        paradigm_id=self._paradigm_id,
                        stem=self._stem,
                    )

            return best_match

        # Fallback –¥–ª—è –æ–±—ã—á–Ω–æ–≥–æ —Å–ª–æ–≤–∞—Ä—è
        if hasattr(self._analyzer, "dictionary"):
            # –ò—â–µ–º —Ñ–æ—Ä–º—ã –Ω–æ—Ä–º–∞–ª—å–Ω–æ–π —Ñ–æ—Ä–º—ã —Å–ª–æ–≤–∞
            normal_parses = self._analyzer.dictionary.get(self.normal_form, [])

            # –ò—â–µ–º —Ñ–æ—Ä–º—É —Å –Ω—É–∂–Ω—ã–º–∏ –≥—Ä–∞–º–º–µ–º–∞–º–∏
            for parse_item in normal_parses:
                if required_grammemes.issubset(parse_item.tag.grammemes):
                    return parse_item  # type: ignore[no-any-return]

            # –ï—Å–ª–∏ —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –∏—â–µ–º —á–∞—Å—Ç–∏—á–Ω–æ–µ
            for parse_item in normal_parses:
                matching_grammemes = parse_item.tag.grammemes & required_grammemes
                if matching_grammemes:
                    return parse_item  # type: ignore[no-any-return]

        return None

    @property
    def is_known(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∏—Ç—å, –∏–∑–≤–µ—Å—Ç–Ω–æ –ª–∏ —Å–ª–æ–≤–æ —Å–ª–æ–≤–∞—Ä—é.

        Returns:
            True –µ—Å–ª–∏ —Å–ª–æ–≤–æ –Ω–∞–π–¥–µ–Ω–æ –≤ —Å–ª–æ–≤–∞—Ä–µ, False –µ—Å–ª–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–æ
        """
        # –ï—Å–ª–∏ —É –Ω–∞—Å –µ—Å—Ç—å paradigm_id, –∑–Ω–∞—á–∏—Ç —Å–ª–æ–≤–æ –∏–∑ DAWG —Å–ª–æ–≤–∞—Ä—è
        if self._paradigm_id is not None:
            return True

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á–µ—Ä–µ–∑ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä
        if self._analyzer and hasattr(self._analyzer, "_dawg_dict") and self._analyzer._dawg_dict:
            return self._analyzer._dawg_dict.word_is_known(self.word)

        # Fallback: –ø—Ä–æ–≤–µ—Ä—è–µ–º score (–ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ —Å–ª–æ–≤–∞ –∏–º–µ—é—Ç score < 1.0)
        return self.score >= 1.0

    @property
    def normalized(self) -> MAWOParse:
        """–ü–æ–ª—É—á–∏—Ç—å —Ä–∞–∑–±–æ—Ä –Ω–æ—Ä–º–∞–ª—å–Ω–æ–π —Ñ–æ—Ä–º—ã —Å–ª–æ–≤–∞.

        Returns:
            MAWOParse –¥–ª—è –Ω–æ—Ä–º–∞–ª—å–Ω–æ–π —Ñ–æ—Ä–º—ã
        """
        if self.word == self.normal_form:
            return self

        # –ü–∞—Ä—Å–∏–º –Ω–æ—Ä–º–∞–ª—å–Ω—É—é —Ñ–æ—Ä–º—É
        if self._analyzer:
            parses = self._analyzer.parse(self.normal_form)
            if parses:
                # –ò—â–µ–º —Ä–∞–∑–±–æ—Ä —Å —Ç–µ–º –∂–µ POS
                for p in parses:
                    if p.tag.POS == self.tag.POS:
                        return p
                # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ —Å —Ç–µ–º –∂–µ POS, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–µ—Ä–≤—ã–π
                return parses[0]

        # Fallback: —Å–æ–∑–¥–∞–µ–º parse –¥–ª—è –Ω–æ—Ä–º–∞–ª—å–Ω–æ–π —Ñ–æ—Ä–º—ã
        return MAWOParse(
            word=self.normal_form,
            normal_form=self.normal_form,
            tag=self.tag,
            score=self.score,
            analyzer=self._analyzer,
            paradigm_id=self._paradigm_id,
            stem=self._stem,
        )

    def make_agree_with_number(self, num: int) -> MAWOParse | None:
        """–°–æ–≥–ª–∞—Å–æ–≤–∞—Ç—å —Å–ª–æ–≤–æ —Å —á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–º.

        Args:
            num: –ß–∏—Å–ª–æ –¥–ª—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–∏—è

        Returns:
            –°–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–∞—è —Ñ–æ—Ä–º–∞ –∏–ª–∏ None
        """
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω—É–∂–Ω–æ–µ —á–∏—Å–ª–æ (–µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ –∏–ª–∏ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ)
        # –ü—Ä–∞–≤–∏–ª–∞ —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞:
        # 1 - sing (1 –¥–æ–º)
        # 2,3,4 - sing + gent (2 –¥–æ–º–∞, –Ω–æ –≤ –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö —Å–ª—É—á–∞—è—Ö —ç—Ç–æ –æ—Å–æ–±–∞—è —Ñ–æ—Ä–º–∞)
        # 5+ - plur + gent (5 –¥–æ–º–æ–≤)
        # 11-14 - –æ—Å–æ–±—ã–π —Å–ª—É—á–∞–π, –≤—Å–µ–≥–¥–∞ plur + gent

        if num % 10 == 1 and num % 100 != 11:
            # 1, 21, 31, ... - –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ —á–∏—Å–ª–æ, –∏–º–µ–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø–∞–¥–µ–∂
            return self.inflect({"sing", "nomn"})
        elif 2 <= num % 10 <= 4 and (num % 100 < 10 or num % 100 >= 20):
            # 2,3,4, 22,23,24, ... - –µ–¥–∏–Ω—Å—Ç–≤–µ–Ω–Ω–æ–µ —á–∏—Å–ª–æ, —Ä–æ–¥–∏—Ç–µ–ª—å–Ω—ã–π –ø–∞–¥–µ–∂
            return self.inflect({"sing", "gent"})
        else:
            # 5-20, 25-30, ... - –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ —á–∏—Å–ª–æ, —Ä–æ–¥–∏—Ç–µ–ª—å–Ω—ã–π –ø–∞–¥–µ–∂
            return self.inflect({"plur", "gent"})

    @property
    def methods_stack(self) -> tuple:
        """–°—Ç–µ–∫ –º–µ—Ç–æ–¥–æ–≤ —Ä–∞–∑–±–æ—Ä–∞ (–¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å pymorphy2).

        Returns:
            –ü—É—Å—Ç–æ–π –∫–æ—Ä—Ç–µ–∂ (–∑–∞–≥–ª—É—à–∫–∞)
        """
        # –í pymorphy2 —ç—Ç–æ —Å–ø–∏—Å–æ–∫ –º–µ—Ç–æ–¥–æ–≤, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–∞–∑–±–æ—Ä–∞
        # –î–ª—è –Ω–∞—Å —ç—Ç–æ –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–æ, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π tuple
        return ()

    @property
    def lexeme(self) -> list[MAWOParse]:
        """–ü–æ–ª—É—á–∏—Ç—å –≤—Å–µ —Ñ–æ—Ä–º—ã —Å–ª–æ–≤–∞ (–ª–µ–∫—Å–µ–º—É/–ø–∞—Ä–∞–¥–∏–≥–º—É).

        Returns:
            –°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —Å–ª–æ–≤–æ—Ñ–æ—Ä–º –¥–∞–Ω–Ω–æ–≥–æ —Å–ª–æ–≤–∞
        """
        if not self._analyzer:
            return [self]

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ —á–∞—Å—Ç–∏—Ü–∞ –≤ normal_form (–¥–ª—è —Å–ª–æ–≤ —Ç–∏–ø–∞ "—Å–∫–∞–∑–∞—Ç—å-–∫–∞")
        particle_suffix = None
        if "-" in self.normal_form:
            particles = ["–∫–∞", "—Ç–æ", "—Ç–∞–∫–∏", "–¥–µ", "—Ç–∫–æ", "—Ç–∫–∞", "—Å", "—Å—Ç–∞"]
            parts = self.normal_form.rsplit("-", 1)
            if len(parts) == 2 and parts[1] in particles:
                particle_suffix = "-" + parts[1]

        # –ï—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è DAWG –∏ –µ—Å—Ç—å paradigm_id
        if (
            hasattr(self._analyzer, "_dawg_dict")
            and self._analyzer._dawg_dict
            and self._paradigm_id is not None
            and self._stem is not None
        ):
            lexeme_forms = []
            paradigm_forms = self._analyzer._dawg_dict.get_all_paradigm_forms(self._paradigm_id)

            for suffix, tag_string, prefix in paradigm_forms:
                pos, grammemes = self._analyzer._dawg_dict.parse_tag_string(tag_string)
                inflected_word = prefix + self._stem + suffix

                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –Ω–æ—Ä–º–∞–ª—å–Ω—É—é —Ñ–æ—Ä–º—É (–ø–µ—Ä–≤–∞—è —Ñ–æ—Ä–º–∞ –ø–∞—Ä–∞–¥–∏–≥–º—ã)
                normal_form_info = self._analyzer._dawg_dict.get_paradigm(self._paradigm_id, 0)
                if normal_form_info:
                    normal_suffix, _, normal_prefix = normal_form_info
                    normal_form = normal_prefix + self._stem + normal_suffix
                else:
                    normal_form = inflected_word

                # –î–æ–±–∞–≤–ª—è–µ–º —á–∞—Å—Ç–∏—Ü—É –æ–±—Ä–∞—Ç–Ω–æ –µ—Å–ª–∏ –æ–Ω–∞ –±—ã–ª–∞
                if particle_suffix:
                    inflected_word = inflected_word + particle_suffix
                    normal_form = normal_form + particle_suffix

                new_tag = MAWOTag(pos, grammemes)
                lexeme_forms.append(
                    MAWOParse(
                        word=inflected_word,
                        normal_form=normal_form,
                        tag=new_tag,
                        score=self.score,
                        analyzer=self._analyzer,
                        paradigm_id=self._paradigm_id,
                        stem=self._stem,
                    )
                )

            return lexeme_forms

        # Fallback –¥–ª—è –æ–±—ã—á–Ω–æ–≥–æ —Å–ª–æ–≤–∞—Ä—è
        if hasattr(self._analyzer, "dictionary"):
            normal_forms = self._analyzer.dictionary.get(self.normal_form, [])
            return normal_forms if normal_forms else [self]

        return [self]

    def __repr__(self) -> str:
        return f"MAWOParse(word='{self.word}', normal_form='{self.normal_form}', tag='{self.tag}', score={self.score})"


class MAWOMorphAnalyzer:
    """–ì–ª–∞–≤–Ω—ã–π –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä MAWO
    –ü–æ–ª–Ω–∞—è –∑–∞–º–µ–Ω–∞ pymorphy2 —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º —Å–ª–æ–≤–∞—Ä–µ–º OpenCorpora.
    """

    def __init__(self, dict_path: str | None = None, use_dawg: bool = True) -> None:
        global _GLOBAL_DICTIONARY_CACHE, _GLOBAL_PATTERNS_CACHE

        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ DAWG —Å–ª–æ–≤–∞—Ä–∏ –∏–∑ dicts_ru
        self.dict_path = dict_path or str(Path(__file__).parent / "dicts_ru")
        self.use_dawg = use_dawg
        self._dawg_dict: Any = None

        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–π DAWGDictionary –µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω–æ
        if self.use_dawg and Path(self.dict_path).exists():
            try:
                from .dawg_dictionary import DAWGDictionary

                logger.info("‚ö° –ó–∞–≥—Ä—É–∑–∫–∞ DAWG —Å–ª–æ–≤–∞—Ä–µ–π...")
                self._dawg_dict = DAWGDictionary(self.dict_path)
                self.dictionary: dict[str, list[MAWOParse]] = {}

                logger.info("‚úÖ DAWG —Å–ª–æ–≤–∞—Ä–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ!")
                logger.info(f"   –ü—É—Ç—å –∫ —Å–ª–æ–≤–∞—Ä—è–º: {self.dict_path}")
                logger.info("   –ü–∞–º—è—Ç—å: ~15-20 –ú–ë (DAWG)")

                # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º patterns –¥–ª—è fallback –Ω–∞ –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–µ —Å–ª–æ–≤–∞
                self.patterns: dict[str, Any] = {}
                self._init_patterns()

                # –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å —Ç–µ—Å—Ç–∞–º–∏
                self._analyzer = self
                self._production_analyzer = None

                logger.info(
                    "‚úÖ MAWO Morphological Analyzer initialized with DAWG dictionaries",
                )

                return  # –ì–æ—Ç–æ–≤–æ, –Ω–µ –Ω—É–∂–µ–Ω fallback

            except ImportError as e:
                logger.error(f"‚ö†Ô∏è dawg-python –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω: {e}, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback")
                logger.exception("–ü–æ–ª–Ω–∞—è —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∞ ImportError:")
                self.use_dawg = False
            except Exception as e:
                logger.error(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ DAWG: {e}, –∏—Å–ø–æ–ª—å–∑—É–µ–º fallback")
                logger.exception("–ü–æ–ª–Ω–∞—è —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∞ –æ—à–∏–±–∫–∏:")
                self.use_dawg = False

        # Fallback: –∑–∞–≥—Ä—É–∑–∫–∞ —á–µ—Ä–µ–∑ –∫—ç—à –∏–ª–∏ XML
        if _GLOBAL_DICTIONARY_CACHE is None:
            logger.info("üîÑ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è (fallback —Ä–µ–∂–∏–º) - –∑–∞–≥—Ä—É–∑–∫–∞ —Å–ª–æ–≤–∞—Ä—è...")

            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∫—Ä–∞—Å–∏–≤—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ –¢–û–õ–¨–ö–û –ø—Ä–∏ —Ä–µ–∞–ª—å–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–µ
            if RICH_AVAILABLE:
                console = Console()
                console.print(
                    Panel(
                        "[bold blue]üìö –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ MAWO[/bold blue]\n"
                        "[dim]–ó–∞–≥—Ä—É–∑–∫–∞ —Å–ª–æ–≤–∞—Ä—è OpenCorpora –¥–ª—è —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏...[/dim]",
                        title="OpenCorpora",
                    ),
                )

            self.dictionary: dict[str, list[MAWOParse]] = {}
            self._load_dictionary()
            _GLOBAL_DICTIONARY_CACHE = self.dictionary.copy()
            logger.info(
                f"üíæ OpenCorpora dictionary cached ({len(_GLOBAL_DICTIONARY_CACHE)} entries)",
            )
        else:
            logger.debug("‚ö° Using cached OpenCorpora dictionary - no reload needed!")  # type: ignore[unreachable]
            self.dictionary = _GLOBAL_DICTIONARY_CACHE.copy()

        if _GLOBAL_PATTERNS_CACHE is None:
            self.patterns: dict[str, Any] = {}
            self._init_patterns()
            _GLOBAL_PATTERNS_CACHE = self.patterns.copy()
        else:
            self.patterns = _GLOBAL_PATTERNS_CACHE.copy()  # type: ignore[unreachable]

        # –î–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å —Ç–µ—Å—Ç–∞–º–∏
        self._analyzer = self
        self._production_analyzer = None

        logger.info(
            f"‚úÖ MAWO Morphological Analyzer initialized with {len(self.dictionary)} entries",
        )

    def _get_cache_path(self, xml_path: Path) -> Path:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –ø—É—Ç–∏ –∫ pickle-–∫—ç—à—É —Å–ª–æ–≤–∞—Ä—è."""
        return xml_path.parent / f"{xml_path.stem}.pkl"

    def _is_cache_valid(self, xml_path: Path, cache_path: Path) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç–∏ –∫—ç—à–∞."""
        if not cache_path.exists():
            return False

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ –∫—ç—à –Ω–æ–≤–µ–µ XML
        xml_mtime = xml_path.stat().st_mtime
        cache_mtime = cache_path.stat().st_mtime

        return bool(cache_mtime >= xml_mtime)

    def _load_from_cache(self, cache_path: Path) -> bool:
        """–ë—ã—Å—Ç—Ä–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ —Å–ª–æ–≤–∞—Ä—è –∏–∑ pickle-–∫—ç—à–∞."""
        try:
            logger.info(f"‚ö° Loading dictionary from cache: {cache_path.name}")

            if RICH_AVAILABLE:
                console = Console()
                console.print(
                    Panel(
                        "[bold cyan]‚ö° –ë—ã—Å—Ç—Ä–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –∏–∑ –∫—ç—à–∞[/bold cyan]\n"
                        "[dim]–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–π —Å–ª–æ–≤–∞—Ä—å...[/dim]",
                        title="OpenCorpora Cache",
                    ),
                )

            # Security: This is a locally-generated cache file, not user input
            # Validate file size before loading to prevent memory exhaustion
            import os

            cache_size = os.path.getsize(cache_path)
            if cache_size > 500_000_000:  # 500MB limit
                msg = f"Cache file too large: {cache_size} bytes"
                raise ValueError(msg)

            with open(cache_path, "rb") as f:
                # nosec B301 - This is a locally-generated cache file, not untrusted user input
                cached_data = pickle.load(f)  # nosec B301

            self.dictionary = cached_data["dictionary"]
            cache_info = cached_data.get("metadata", {})

            logger.info(f"‚úÖ Dictionary loaded from cache: {len(self.dictionary):,} entries")
            logger.info(f"üìä Cache created: {cache_info.get('created_at', 'unknown')}")
            logger.info(
                f"üöÄ Loading time: ~instant (vs {cache_info.get('original_parse_time', 'N/A')}s from XML)",
            )

            if RICH_AVAILABLE:
                console = Console()
                console.print(
                    Panel(
                        f"[bold green]‚úÖ –°–ª–æ–≤–∞—Ä—å –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ –∫—ç—à–∞[/bold green]\n"
                        f"[dim]{len(self.dictionary):,} –∑–∞–ø–∏—Å–µ–π ‚Ä¢ –ú–≥–Ω–æ–≤–µ–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞[/dim]",
                        title="–ì–æ—Ç–æ–≤–æ",
                    ),
                )

            return True

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Failed to load from cache: {e}, will parse XML")
            return False

    def _save_to_cache(self, cache_path: Path, parse_time: float) -> None:
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å–ª–æ–≤–∞—Ä—è –≤ pickle-–∫—ç—à –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ –≤ –±—É–¥—É—â–µ–º."""
        try:
            from datetime import datetime

            logger.info(f"üíæ Saving dictionary to cache: {cache_path.name}")

            cache_data = {
                "dictionary": self.dictionary,
                "metadata": {
                    "created_at": datetime.now().isoformat(),
                    "entries_count": len(self.dictionary),
                    "original_parse_time": round(parse_time, 2),
                    "mawo_version": "2025.1",
                },
            }

            # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
            cache_path.parent.mkdir(parents=True, exist_ok=True)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º –ø—Ä–æ—Ç–æ–∫–æ–ª–æ–º pickle
            with open(cache_path, "wb") as f:
                pickle.dump(cache_data, f, protocol=pickle.HIGHEST_PROTOCOL)

            cache_size_mb = cache_path.stat().st_size / (1024 * 1024)
            logger.info(f"‚úÖ Cache saved: {cache_size_mb:.1f} MB")
            logger.info("üöÄ Future loads will be instant!")

        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Failed to save cache (non-critical): {e}")

    def _load_dictionary(self) -> None:
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Å–ª–æ–≤–∞—Ä—è –∏–∑ OpenCorpora XML –∏–ª–∏ –∫—ç—à–∞."""
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ä–µ–∂–∏–º–∞ (—Ç–µ—Å—Ç—ã)
        import os
        import time

        if os.environ.get("MAWO_FAST_MODE") == "1" or os.environ.get("PYTEST_CURRENT_TEST"):
            logger.info("Fast mode enabled, using basic dictionary")
            self._init_basic_dictionary()
            return

        # Use centralized path configuration
        try:
            from core.path_config import path_config  # type: ignore[import-not-found]

            opencorpora_path = (
                path_config.data_dir
                / "local_libs"
                / "opencorpora_2025"
                / "opencorpora_annot_2025.xml"
            )
        except ImportError:
            # Fallback if path_config is not available
            opencorpora_path = (
                Path(__file__).parent.parent.parent
                / "data"
                / "local_libs"
                / "opencorpora_2025"
                / "opencorpora_annot_2025.xml"
            )

        if opencorpora_path.exists():
            try:
                cache_path = self._get_cache_path(opencorpora_path)

                # –ü—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑ –∫—ç—à–∞
                if self._is_cache_valid(opencorpora_path, cache_path):
                    if self._load_from_cache(cache_path):
                        return  # –£—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ –∏–∑ –∫—ç—à–∞!

                # –ö—ç—à –Ω–µ –Ω–∞–π–¥–µ–Ω –∏–ª–∏ —É—Å—Ç–∞—Ä–µ–ª - –ø–∞—Ä—Å–∏–º XML
                logger.info("üìñ Cache not found or outdated, parsing XML (this will take time...)")

                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∑–∞–≥—Ä—É–∑–∫–∏
                if RICH_AVAILABLE:
                    console = Console()
                    console.print(
                        Panel(
                            "[bold yellow]üìñ –ü–µ—Ä–≤–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ —Å–ª–æ–≤–∞—Ä—è OpenCorpora[/bold yellow]\n"
                            "[dim]–ü–∞—Ä—Å–∏–Ω–≥ XML –¥–∞–Ω–Ω—ã—Ö ‚Ä¢ –°–æ–∑–¥–∞–Ω–∏–µ –∫—ç—à–∞ –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –∑–∞–≥—Ä—É–∑–∫–∏...[/dim]",
                            title="OpenCorpora",
                        ),
                    )

                start_time = time.time()
                self._parse_opencorpora(opencorpora_path)
                parse_time = time.time() - start_time

                # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∑–∞–≥—Ä—É–∑–∫–∏
                if RICH_AVAILABLE:
                    console = Console()
                    console.print(
                        Panel(
                            f"[bold green]‚úÖ –°–ª–æ–≤–∞—Ä—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω[/bold green]\n"
                            f"[dim]–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(self.dictionary):,} –∑–∞–ø–∏—Å–µ–π –∑–∞ {parse_time:.1f}—Å[/dim]",
                            title="–ì–æ—Ç–æ–≤–æ",
                        ),
                    )

                logger.info(
                    f"‚úÖ Loaded OpenCorpora dictionary: {len(self.dictionary):,} entries in {parse_time:.1f}s",
                )

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à –¥–ª—è –±—É–¥—É—â–∏—Ö –∑–∞–ø—É—Å–∫–æ–≤
                self._save_to_cache(cache_path, parse_time)

            except Exception as e:
                logger.warning(f"Failed to load OpenCorpora: {e}")
                self._init_basic_dictionary()
        else:
            logger.info("OpenCorpora not found, using basic dictionary")
            self._init_basic_dictionary()

    def _process_opencorpora_token(self, token: Any) -> None:
        """Process a single OpenCorpora token and add to dictionary."""
        word_attr = token.get("text")
        if not word_attr:
            return

        word = word_attr.lower()

        # Parse structure: token -> tfr -> v -> l
        for tfr in token.findall("tfr"):
            for v in tfr.findall("v"):
                for lemma in v.findall("l"):
                    normal_form = lemma.get("t", word)

                    # Extract POS from first g element
                    pos = "UNKN"
                    grammemes = set()
                    for gram in lemma.findall("g"):
                        gram_value = gram.get("v")
                        if gram_value:
                            if pos == "UNKN" and gram_value in {
                                "NOUN",
                                "VERB",
                                "ADJF",
                                "ADJS",
                                "COMP",
                                "INFN",
                                "PRTF",
                                "PRTS",
                                "GRND",
                                "NUMR",
                                "ADVB",
                                "NPRO",
                                "PRED",
                                "PREP",
                                "CONJ",
                                "PRCL",
                                "INTJ",
                                "PNCT",
                            }:
                                pos = gram_value
                            else:
                                grammemes.add(gram_value)

                    tag = MAWOTag(pos, grammemes)
                    parse_result = MAWOParse(word, normal_form, tag, 1.0, self)

                    if word not in self.dictionary:
                        self.dictionary[word] = []
                    self.dictionary[word].append(parse_result)

    def _parse_opencorpora(self, xml_path: Path) -> None:
        """–ü–∞—Ä—Å–∏–Ω–≥ OpenCorpora XML —Å –ø—Ä–æ–≥—Ä–µ—Å—Å –±–∞—Ä–æ–º."""
        try:
            if ET_PARSE_SAFE:
                tree = defusedxml_parse(xml_path)
            else:
                import xml.etree.ElementTree as ET  # noqa: N817

                tree = ET.parse(xml_path)  # nosec B314
            root = tree.getroot()

            all_tokens = root.findall(".//token")
            total_tokens = len(all_tokens)

            # –ò—Å–ø–æ–ª—å–∑—É–µ–º tqdm –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ–π —Ä–∞–±–æ—Ç—ã –≤ –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É
            try:
                from tqdm import tqdm

                if total_tokens > 0:
                    with tqdm(
                        total=total_tokens,
                        desc="üìñ –ó–∞–≥—Ä—É–∑–∫–∞ OpenCorpora —Å–ª–æ–≤–∞—Ä—è",
                        unit="token",
                        dynamic_ncols=True,
                        leave=False,  # –ù–µ –æ—Å—Ç–∞–≤–ª—è–µ–º –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è
                        disable=total_tokens < 1000,  # –û—Ç–∫–ª—é—á–∞–µ–º –¥–ª—è –º–∞–ª–µ–Ω—å–∫–∏—Ö —Å–ª–æ–≤–∞—Ä–µ–π
                    ) as pbar:
                        for i, token in enumerate(all_tokens):
                            self._process_opencorpora_token(token)
                            if (i + 1) % 100 == 0:
                                pbar.update(100)
                        # –û–±–Ω–æ–≤–ª—è–µ–º –¥–æ –∫–æ–Ω—Ü–∞ –µ—Å–ª–∏ –æ—Å—Ç–∞–ª–∏—Å—å –Ω–µ–æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
                        pbar.update(total_tokens - pbar.n)
                else:
                    for token in all_tokens:
                        self._process_opencorpora_token(token)
            except ImportError:
                # Fallback –±–µ–∑ –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä–∞ –µ—Å–ª–∏ tqdm –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω
                for token in all_tokens:
                    self._process_opencorpora_token(token)

        except Exception as e:
            logger.exception(f"Error parsing OpenCorpora: {e}")
            raise

    def _init_basic_dictionary(self) -> None:
        """–ë–∞–∑–æ–≤—ã–π —Å–ª–æ–≤–∞—Ä—å –¥–ª—è fail_fast_mode."""
        basic_words = {
            # –ú–µ—Å—Ç–æ–∏–º–µ–Ω–∏—è
            "—è": [("—è", "NPRO", {"sing", "1per", "nomn"})],
            "—Ç—ã": [("—Ç—ã", "NPRO", {"sing", "2per", "nomn"})],
            "–æ–Ω": [("–æ–Ω", "NPRO", {"sing", "3per", "masc", "nomn"})],
            "–æ–Ω–∞": [("–æ–Ω–∞", "NPRO", {"sing", "3per", "femn", "nomn"})],
            "–æ–Ω–æ": [("–æ–Ω–æ", "NPRO", {"sing", "3per", "neut", "nomn"})],
            "–º—ã": [("–º—ã", "NPRO", {"plur", "1per", "nomn"})],
            "–≤—ã": [("–≤—ã", "NPRO", {"plur", "2per", "nomn"})],
            "–æ–Ω–∏": [("–æ–Ω–∏", "NPRO", {"plur", "3per", "nomn"})],
            # –ì–ª–∞–≥–æ–ª—ã
            "–±—ã—Ç—å": [("–±—ã—Ç—å", "INFN", {"impf"})],
            "–µ—Å—Ç—å": [("–±—ã—Ç—å", "VERB", {"pres", "3per", "sing"})],
            "–±—ã–ª": [("–±—ã—Ç—å", "VERB", {"past", "masc", "sing"})],
            "–±—ã–ª–∞": [("–±—ã—Ç—å", "VERB", {"past", "femn", "sing"})],
            "–±—ã–ª–æ": [("–±—ã—Ç—å", "VERB", {"past", "neut", "sing"})],
            "–±—ã–ª–∏": [("–±—ã—Ç—å", "VERB", {"past", "plur"})],
            # –°—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ
            "–¥–æ–º": [("–¥–æ–º", "NOUN", {"masc", "inan", "nomn", "sing"})],
            "–¥–æ–º–∞": [
                ("–¥–æ–º", "NOUN", {"masc", "inan", "gent", "sing"}),
                ("–¥–æ–º", "NOUN", {"masc", "inan", "nomn", "plur"}),
            ],
            "—à–∫–æ–ª–∞": [("—à–∫–æ–ª–∞", "NOUN", {"femn", "inan", "nomn", "sing"})],
            "—à–∫–æ–ª—ã": [
                ("—à–∫–æ–ª–∞", "NOUN", {"femn", "inan", "gent", "sing"}),
                ("—à–∫–æ–ª–∞", "NOUN", {"femn", "inan", "nomn", "plur"}),
            ],
        }

        for word, forms in basic_words.items():
            self.dictionary[word] = []
            for normal_form, pos, grammemes in forms:
                tag = MAWOTag(pos, grammemes)
                parse_result = MAWOParse(word, normal_form, tag, 0.8, self)
                self.dictionary[word].append(parse_result)

    def _init_patterns(self) -> None:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤."""
        self.patterns = {
            # –û–∫–æ–Ω—á–∞–Ω–∏—è —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö
            "noun_endings": {
                "–∞": ("femn", "nomn", "sing"),
                "—ã": ("femn", "nomn", "plur"),
                "–æ–π": ("femn", "gent", "sing"),
                "–µ–º": ("masc", "ablt", "sing"),
                "–∞–º–∏": ("femn", "ablt", "plur"),
            },
            # –û–∫–æ–Ω—á–∞–Ω–∏—è –≥–ª–∞–≥–æ–ª–æ–≤
            "verb_endings": {
                "—Ç—å": ("INFN", set()),
                "—Ç–∏": ("INFN", set()),
                "—á—å": ("INFN", set()),
                "–µ—Ç": ("VERB", {"3per", "sing", "pres"}),
                "—É—Ç": ("VERB", {"3per", "plur", "pres"}),
                "—é—Ç": ("VERB", {"3per", "plur", "pres"}),
            },
            # –û–∫–æ–Ω—á–∞–Ω–∏—è –ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω—ã—Ö
            "adj_endings": {
                "—ã–π": ("ADJF", {"masc", "nomn", "sing"}),
                "–∏–π": ("ADJF", {"masc", "nomn", "sing"}),
                "–æ–π": ("ADJF", {"masc", "nomn", "sing"}),
                "–∞—è": ("ADJF", {"femn", "nomn", "sing"}),
                "—è—è": ("ADJF", {"femn", "nomn", "sing"}),
                "–æ–µ": ("ADJF", {"neut", "nomn", "sing"}),
                "–µ–µ": ("ADJF", {"neut", "nomn", "sing"}),
            },
        }

    def parse(self, word: str) -> list[MAWOParse]:
        """–ú–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ —Å–ª–æ–≤–∞.

        Args:
            word: –°–ª–æ–≤–æ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞

        Returns:
            –°–ø–∏—Å–æ–∫ –≤–æ–∑–º–æ–∂–Ω—ã—Ö —Ä–∞–∑–±–æ—Ä–æ–≤ —Å–ª–æ–≤–∞

        """
        if not word or not word.strip():
            return []

        word_clean = word.lower().strip()

        # –ü—ã—Ç–∞–µ–º—Å—è —Å–Ω–∞—á–∞–ª–∞ —Å –∏—Å—Ö–æ–¥–Ω—ã–º —Å–ª–æ–≤–æ–º
        result = self._parse_word(word_clean)

        # –ï/–Å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è (best practice NLP 2024-2025):
        # –î–ª—è –Ω–µ–∫–æ—Ç–æ—Ä—ã—Ö —Å–ª–æ–≤ –µ/—ë –¥–∞–µ—Ç –†–ê–ó–ù–´–ï —Å–ª–æ–≤–∞ (–æ–∑–µ—Ä–∞ vs –æ–∑—ë—Ä–∞)
        # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è: –ø—Ä–æ–±—É–µ–º —Ç–æ–ª—å–∫–æ –¥–ª—è –∫–æ—Ä–æ—Ç–∫–∏—Ö —Å–ª–æ–≤ (<=6 —Å–∏–º–≤–æ–ª–æ–≤) –∏–ª–∏ –µ—Å–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –ø–ª–æ—Ö–æ–π
        should_try_eo_norm = False
        if "–µ" in word_clean or "—ë" in word_clean:
            if not result or result[0].tag.POS == "UNKN" or result[0].score < 1.0:
                # –ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞ / –ø–ª–æ—Ö–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç - –≤—Å–µ–≥–¥–∞ –ø—Ä–æ–±—É–µ–º
                should_try_eo_norm = True
            elif len(word_clean) <= 6:
                # –ö–æ—Ä–æ—Ç–∫–∏–µ —Å–ª–æ–≤–∞ (–æ–∑–µ—Ä–∞, –¥–æ–º –∏ —Ç.–¥.) - –ø—Ä–æ–±—É–µ–º –¥–ª—è –ø–æ–ª–Ω–æ—Ç—ã
                should_try_eo_norm = True

        if should_try_eo_norm:
            # –ó–∞–º–µ–Ω—è–µ–º –µ ‚Üî —ë
            word_normalized = word_clean.replace("–µ", "\x00").replace("—ë", "–µ").replace("\x00", "—ë")
            result_normalized = self._parse_word(word_normalized)

            if result_normalized:
                # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ –Ω–∞–ø–∏—Å–∞–Ω–∏–µ
                for parse in result_normalized:
                    parse.word = word_clean

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–∞—é—Ç –ª–∏ –≤–∞—Ä–∏–∞–Ω—Ç—ã –†–ê–ó–ù–´–ï parse –≤–∞—Ä–∏–∞–Ω—Ç—ã
                # (–æ–∑–µ—Ä–∞: gent,sing vs plur,nomn - —Ä–∞–∑–Ω—ã–µ –≥—Ä–∞–º–º–µ–º—ã –ø—Ä–∏ –æ–¥–Ω–æ–π normal form)
                original_keys = {(p.normal_form, str(p.tag)) for p in result} if result else set()
                normalized_keys = {(p.normal_form, str(p.tag)) for p in result_normalized}

                if original_keys != normalized_keys:
                    # –†–∞–∑–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã - –æ–±—ä–µ–¥–∏–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                    seen = set()
                    combined = []
                    for parse in result + result_normalized:
                        key = (parse.normal_form, str(parse.tag))
                        if key not in seen:
                            seen.add(key)
                            combined.append(parse)
                    result = combined
                # –ï—Å–ª–∏ –≤–∞—Ä–∏–∞–Ω—Ç—ã –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ - –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –æ—Ä–∏–≥–∏–Ω–∞–ª (result)

        # –ü—Ä–∏–º–µ–Ω—è–µ–º —ç–≤—Ä–∏—Å—Ç–∏–∫—É –¥–ª—è —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏ —Ä–∞–∑–±–æ—Ä–æ–≤ (–ª—É—á—à–∏–µ –ø–µ—Ä–≤—ã–º–∏)
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: —Å–ª–æ–≤–∞—Ä–Ω—ã–µ > POS (NOUN > VERB > ADJF) > nominative > –º–µ–Ω—å—à–µ –≥—Ä–∞–º–º–µ–º
        if result and len(result) > 1:
            result = self._rank_parses(result)

        return result if result else []

    def _rank_parses(self, parses: list[MAWOParse]) -> list[MAWOParse]:
        """–†–∞–Ω–∂–∏—Ä–æ–≤–∞—Ç—å —Ä–∞–∑–±–æ—Ä—ã –ø–æ —ç–≤—Ä–∏—Å—Ç–∏–∫–∞–º (–ª—É—á—à–∏–µ –ø–µ—Ä–≤—ã–º–∏).

        –≠–≤—Ä–∏—Å—Ç–∏–∫–∏ (–≤ –ø–æ—Ä—è–¥–∫–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞ - –ø–æ best practices NLP 2024-2025):
        1. –°–ª–æ–≤–∞—Ä–Ω—ã–µ —Å–ª–æ–≤–∞ (score >= 1.0) –í–°–ï–ì–î–ê –ª—É—á—à–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö (score < 1.0)
        2. –°—Ä–µ–¥–∏ —Å–ª–æ–≤–∞—Ä–Ω—ã—Ö: –ß–∞—Å—Ç—å —Ä–µ—á–∏: NOUN > VERB > ADJF > –æ—Å—Ç–∞–ª—å–Ω—ã–µ
        3. –ò–º–µ–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø–∞–¥–µ–∂ (nomn) –ø—Ä–µ–¥–ø–æ—á—Ç–∏—Ç–µ–ª—å–Ω–µ–µ –∫–æ—Å–≤–µ–Ω–Ω—ã—Ö
        4. –ú–µ–Ω—å—à–µ –≥—Ä–∞–º–º–µ–º = –ø—Ä–æ—â–µ —Ñ–æ—Ä–º–∞ = –≤–µ—Ä–æ—è—Ç–Ω–µ–µ

        Args:
            parses: –°–ø–∏—Å–æ–∫ —Ä–∞–∑–±–æ—Ä–æ–≤

        Returns:
            –û—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Ä–∞–∑–±–æ—Ä–æ–≤
        """

        def parse_rank(p: MAWOParse) -> tuple[int, int, int, int]:
            # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 0: —Å–ª–æ–≤–∞—Ä–Ω—ã–µ —Å–ª–æ–≤–∞ vs –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ
            # –≠—Ç–æ –ö–†–ò–¢–ò–ß–ù–û: —Å–ª–æ–≤–∞—Ä–Ω—ã–µ –í–°–ï–ì–î–ê –ª—É—á—à–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö
            is_predicted = 1 if p.score < 1.0 else 0

            # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 1: POS (—Ç–æ–ª—å–∫–æ —Å—Ä–µ–¥–∏ —Å–ª–æ–≤–∞—Ä–Ω—ã—Ö –∏–ª–∏ —Å—Ä–µ–¥–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö)
            # –û—Å–Ω–æ–≤–∞–Ω–æ –Ω–∞ —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç–∏ —á–∞—Å—Ç–µ–π —Ä–µ—á–∏ –≤ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ
            pos_priority = {
                "NOUN": 0,  # –°—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ - —Å–∞–º—ã–π —á–∞—Å—Ç—ã–π –∫–ª–∞—Å—Å
                "VERB": 1,  # –ì–ª–∞–≥–æ–ª—ã
                "INFN": 1,  # –ò–Ω—Ñ–∏–Ω–∏—Ç–∏–≤—ã (—Ç–æ–∂–µ –≥–ª–∞–≥–æ–ª—ã)
                "ADJF": 2,  # –ü—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª–Ω—ã–µ
                "NUMR": 2,  # –ß–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ
                "NPRO": 2,  # –ú–µ—Å—Ç–æ–∏–º–µ–Ω–∏—è-—Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ
                "ADJS": 3,  # –ü—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω—ã–µ –∫—Ä–∞—Ç–∫–∏–µ
                "ADVB": 3,  # –ù–∞—Ä–µ—á–∏—è
                "PRTF": 3,  # –ü—Ä–∏—á–∞—Å—Ç–∏—è –ø–æ–ª–Ω—ã–µ
                "PRTS": 4,  # –ü—Ä–∏—á–∞—Å—Ç–∏—è –∫—Ä–∞—Ç–∫–∏–µ
                "GRND": 4,  # –î–µ–µ–ø—Ä–∏—á–∞—Å—Ç–∏—è
                "PRED": 4,  # –ü—Ä–µ–¥–∏–∫–∞—Ç–∏–≤—ã
            }
            pos_rank = pos_priority.get(p.tag.POS, 10)

            # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 2: nominative case (–∏–º–µ–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø–∞–¥–µ–∂ —á–∞—â–µ –≤—Å—Ç—Ä–µ—á–∞–µ—Ç—Å—è)
            nomn_penalty = 0 if "nomn" in p.tag.grammemes else 1

            # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 3: –º–µ–Ω—å—à–µ –≥—Ä–∞–º–º–µ–º = –ø—Ä–æ—â–µ —Ñ–æ—Ä–º–∞
            grammemes_count = len(p.tag.grammemes)

            return (is_predicted, pos_rank, nomn_penalty, grammemes_count)

        return sorted(parses, key=parse_rank)

    def tag(self, word: str) -> list[MAWOTag]:
        """–ü–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –≤–æ–∑–º–æ–∂–Ω—ã—Ö —Ç–µ–≥–æ–≤ –¥–ª—è —Å–ª–æ–≤–∞.

        Args:
            word: –°–ª–æ–≤–æ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞

        Returns:
            –°–ø–∏—Å–æ–∫ –≤–æ–∑–º–æ–∂–Ω—ã—Ö —Ç–µ–≥–æ–≤ (MAWOTag)
        """
        parses = self.parse(word)
        return [p.tag for p in parses]

    def _parse_word(self, word_clean: str) -> list[MAWOParse]:
        """–í–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –º–µ—Ç–æ–¥ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å–ª–æ–≤–∞ –±–µ–∑ –µ/—ë –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏."""

        # ========== PATTERN-BASED ANALYZERS (NLP Best Practice 2024-2025) ==========

        # 1. Superlative adjectives with –ù–ê–ò- prefix (–Ω–∞–∏–Ω–µ–≤–µ—Ä–æ—è—Ç–Ω–µ–π—à–∏–π ‚Üí –≤–µ—Ä–æ—è—Ç–Ω—ã–π)
        if word_clean.startswith("–Ω–∞–∏") and len(word_clean) > 6:
            superlative_result = self._analyze_superlative(word_clean)
            if superlative_result:
                return superlative_result

        # 2. Adverbs with –ü–û- prefix (–ø–æ-—Ç–µ–∞—Ç—Ä–∞–ª—å–Ω–æ–º—É, –ø–æ-–≤–æ—Ä–æ–±—å–∏–Ω–æ–º—É)
        if word_clean.startswith("–ø–æ-") and len(word_clean) > 4:
            po_adverb_result = self._analyze_po_adverb(word_clean)
            if po_adverb_result:
                return po_adverb_result

        # 3. Reduplicated words (–±—ã—Å—Ç—Ä–æ-–±—ã—Å—Ç—Ä–æ, —Ç–∏—Ö–æ-—Ç–∏—Ö–æ)
        if "-" in word_clean:
            parts = word_clean.split("-")
            if len(parts) == 2 and parts[0] == parts[1]:
                # –ü–æ–≤—Ç–æ—Ä –æ–¥–Ω–æ–≥–æ –∏ —Ç–æ–≥–æ –∂–µ —Å–ª–æ–≤–∞
                single_word_parse = self._parse_word_base(parts[0])
                if single_word_parse and single_word_parse[0].tag.POS != "UNKN":
                    # –°–æ–∑–¥–∞–µ–º parse —Å —É–¥–≤–æ–µ–Ω–Ω–æ–π —Ñ–æ—Ä–º–æ–π
                    return [
                        MAWOParse(
                            word=word_clean,
                            normal_form=word_clean,  # –Ω–æ—Ä–º–∞–ª—å–Ω–∞—è —Ñ–æ—Ä–º–∞ = —Å–∞–º–æ —Å–ª–æ–≤–æ
                            tag=single_word_parse[0].tag,
                            score=1.0,
                            analyzer=self,
                        )
                    ]

        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥
        result = self._parse_word_base(word_clean)

        # –ï—Å–ª–∏ –ø–æ–ª—É—á–∏–ª–∏ —Ö–æ—Ä–æ—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç (–Ω–µ UNKN –∏ –Ω–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ), –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –µ–≥–æ
        # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏–º–µ—é—Ç score < 1.0
        if result and result[0].tag.POS != "UNKN" and result[0].score >= 1.0:
            return result

        # 4. Compound words with hyphen (–∫–æ–º–∞–Ω–¥-—É—á–∞—Å—Ç–Ω–∏—Ü, pdf-–¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)
        if "-" in word_clean:
            compound_result = self._analyze_compound_word(word_clean)
            if compound_result:
                return compound_result

        # 5. HyphenSeparatedParticleAnalyzer: –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ª–æ–≤ —Å —á–∞—Å—Ç–∏—Ü–∞–º–∏ –ø–æ—Å–ª–µ –¥–µ—Ñ–∏—Å–∞
        # –¢–æ–ª—å–∫–æ –µ—Å–ª–∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π –ø–∞—Ä—Å–∏–Ω–≥ –¥–∞–ª UNKN –∏–ª–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
        if "-" in word_clean:
            particles = ["-–∫–∞", "-—Ç–æ", "-—Ç–∞–∫–∏", "-–¥–µ", "-—Ç–∫–æ", "-—Ç–∫–∞", "-—Å", "-—Å—Ç–∞"]
            for particle in particles:
                if word_clean.endswith(particle):
                    # –ü–∞—Ä—Å–∏–º —Å–ª–æ–≤–æ –±–µ–∑ —á–∞—Å—Ç–∏—Ü—ã
                    word_without_particle = word_clean[: -len(particle)]
                    if word_without_particle:
                        parses = self._parse_word_base(word_without_particle)
                        if parses and parses[0].tag.POS != "UNKN":
                            # –î–æ–±–∞–≤–ª—è–µ–º —á–∞—Å—Ç–∏—Ü—É –æ–±—Ä–∞—Ç–Ω–æ
                            particle_result = []
                            for p in parses:
                                new_parse = MAWOParse(
                                    word=word_clean,  # —Å —á–∞—Å—Ç–∏—Ü–µ–π
                                    normal_form=p.normal_form + particle,
                                    tag=p.tag,
                                    score=p.score * 0.9,  # score_multiplier
                                    analyzer=self,
                                    paradigm_id=p._paradigm_id,
                                    stem=p._stem,
                                )
                                particle_result.append(new_parse)
                            return particle_result
                    break  # –¢–æ–ª—å–∫–æ –æ–¥–Ω–∞ —á–∞—Å—Ç–∏—Ü–∞ –º–æ–∂–µ—Ç –±—ã—Ç—å

        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ –ø–∞—Ä—Å–∏–Ω–≥–∞ (–º–æ–∂–µ—Ç –±—ã—Ç—å UNKN)
        return result if result else []

    def _parse_word_base(self, word_clean: str) -> list[MAWOParse]:
        """–ë–∞–∑–æ–≤—ã–π –ø–∞—Ä—Å–∏–Ω–≥ —Å–ª–æ–≤–∞ –±–µ–∑ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —á–∞—Å—Ç–∏—Ü."""

        # ========== SPECIAL ANALYZERS (Best Practice NLP 2024-2025) ==========
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –î–û –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        # –ü–æ–¥—Ö–æ–¥ –æ—Å–Ω–æ–≤–∞–Ω –Ω–∞ spaCy tokenizer special cases

        # 1. PunctuationAnalyzer - –∑–Ω–∞–∫–∏ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏
        if self._is_punctuation(word_clean):
            return [
                MAWOParse(
                    word=word_clean,
                    normal_form=word_clean,
                    tag=MAWOTag("PNCT", set()),
                    score=1.0,
                    analyzer=self,
                )
            ]

        # 2. NumberAnalyzer - —á–∏—Å–ª–∞
        number_result = self._analyze_number(word_clean)
        if number_result:
            return number_result

        # 3. RomanNumeralAnalyzer - —Ä–∏–º—Å–∫–∏–µ —Ü–∏—Ñ—Ä—ã
        # 4. LatinAnalyzer - –ª–∞—Ç–∏–Ω—Å–∫–∏–π —Ç–µ–∫—Å—Ç (–Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç –∫–∏—Ä–∏–ª–ª–∏—Ü—É)
        # –ù–µ–∫–æ—Ç–æ—Ä—ã–µ —Ç–æ–∫–µ–Ω—ã –º–æ–≥—É—Ç –±—ã—Ç—å –∏ —Ä–∏–º—Å–∫–∏–º–∏ —Ü–∏—Ñ—Ä–∞–º–∏, –∏ –ª–∞—Ç–∏–Ω—Å–∫–∏–º —Ç–µ–∫—Å—Ç–æ–º
        # (–Ω–∞–ø—Ä–∏–º–µ—Ä, "I", "V", "X", "L", "C", "D", "M")
        special_results = []

        roman_result = self._analyze_roman(word_clean)
        if roman_result:
            special_results.extend(roman_result)

        if self._is_latin(word_clean):
            # –î–æ–±–∞–≤–ª—è–µ–º LATN –≤–∞—Ä–∏–∞–Ω—Ç
            special_results.append(
                MAWOParse(
                    word=word_clean,
                    normal_form=word_clean.lower(),
                    tag=MAWOTag("LATN", set()),
                    score=1.0,
                    analyzer=self,
                )
            )

        if special_results:
            return special_results

        # ========== MORPHOLOGICAL ANALYSIS ==========
        # –ï—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º DAWG —á–µ—Ä–µ–∑ DAWGDictionary
        if self.use_dawg and self._dawg_dict:
            try:
                # –ü–æ–ª—É—á–∞–µ–º —Ä–∞–∑–±–æ—Ä—ã —Å–ª–æ–≤–∞ –∏–∑ DAWG
                word_parses = self._dawg_dict.get_word_parses(word_clean)

                mawo_parses = []
                for paradigm_id, word_idx in word_parses:
                    # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–∞—Ä–∞–¥–∏–≥–º–µ
                    paradigm_info = self._dawg_dict.get_paradigm(paradigm_id, word_idx)

                    if paradigm_info is None:
                        continue

                    suffix, tag_string, prefix = paradigm_info

                    # –†–∞–∑–±–∏—Ä–∞–µ–º —Ç–µ–≥
                    pos, grammemes = self._dawg_dict.parse_tag_string(tag_string)

                    # –í—ã—á–∏—Å–ª—è–µ–º –Ω–æ—Ä–º–∞–ª—å–Ω—É—é —Ñ–æ—Ä–º—É
                    # –ü–æ–ª—É—á–∞–µ–º –ø–µ—Ä–≤—É—é —Ñ–æ—Ä–º—É –ø–∞—Ä–∞–¥–∏–≥–º—ã (word_idx=0)
                    normal_form_info = self._dawg_dict.get_paradigm(paradigm_id, 0)
                    if normal_form_info:
                        normal_suffix, _, normal_prefix = normal_form_info
                        # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Å–Ω–æ–≤—É (stem)
                        # –£–¥–∞–ª—è–µ–º –ø—Ä–µ—Ñ–∏–∫—Å –∏ —Å—É—Ñ—Ñ–∏–∫—Å –∏–∑ —Ç–µ–∫—É—â–µ–≥–æ —Å–ª–æ–≤–∞
                        stem = word_clean
                        if prefix and stem.startswith(prefix):
                            stem = stem[len(prefix) :]
                        if suffix and stem.endswith(suffix):
                            stem = stem[: -len(suffix)]

                        # –°–æ–±–∏—Ä–∞–µ–º –Ω–æ—Ä–º–∞–ª—å–Ω—É—é —Ñ–æ—Ä–º—É
                        normal_form = normal_prefix + stem + normal_suffix
                    else:
                        normal_form = word_clean

                    mawo_tag = MAWOTag(pos, grammemes)
                    mawo_parse = MAWOParse(
                        word=word_clean,
                        normal_form=normal_form,
                        tag=mawo_tag,
                        score=1.0,
                        analyzer=self,
                        paradigm_id=paradigm_id,
                        stem=stem if normal_form_info else None,
                    )
                    mawo_parses.append(mawo_parse)

                if mawo_parses:
                    return mawo_parses

            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞–∑–±–æ—Ä–µ —á–µ—Ä–µ–∑ DAWG: {e}")
                # Fallback –∫ –æ–±—ã—á–Ω–æ–º—É –º–µ—Ç–æ–¥—É

        # Fallback: —Å–Ω–∞—á–∞–ª–∞ –∏—â–µ–º –≤ —Å–ª–æ–≤–∞—Ä–µ
        if word_clean in self.dictionary:
            # –î–æ–±–∞–≤–ª—è–µ–º —Å—Å—ã–ª–∫—É –Ω–∞ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–ª—è inflect()
            parses = self.dictionary[word_clean]
            for parse in parses:
                parse._analyzer = self
            return parses

        # –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –ø—Ä–æ–±—É–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –ø–æ —Å—É—Ñ—Ñ–∏–∫—Å–∞–º (KnownSuffixAnalyzer)
        if self.use_dawg and self._dawg_dict:
            predicted = self._predict_by_suffix(word_clean)
            if predicted:
                # –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–µ–º –∞—Å–ø–µ–∫—Ç –≥–ª–∞–≥–æ–ª–æ–≤ —Å perfectivizing prefixes (NLP Best Practice 2025)
                # –ü—Ä–∏—Å—Ç–∞–≤–∫–∏ –≤–∑-/–≤—Å-, –≤—ã-, –¥–æ-, –∑–∞-, –∏–∑-/–∏—Å-, –Ω–∞-, –æ-/–æ–±-, –æ—Ç-, –ø–µ—Ä–µ-, –ø–æ-, –ø–æ–¥-,
                # –ø—Ä–∏-, –ø—Ä–æ-, —Ä–∞–∑-/—Ä–∞—Å-, —Å-, —É- –æ–±—ã—á–Ω–æ –æ–±—Ä–∞–∑—É—é—Ç —Å–æ–≤–µ—Ä—à–µ–Ω–Ω—ã–π –≤–∏–¥
                predicted = self._correct_verb_aspect(predicted, word_clean)
                return predicted

        # –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        return self._analyze_by_patterns(word_clean)

    def _predict_by_suffix(self, word: str) -> list[MAWOParse]:
        """–ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Ñ–æ—Ä–º —Å–ª–æ–≤–∞ –ø–æ –∏–∑–≤–µ—Å—Ç–Ω—ã–º —Å—É—Ñ—Ñ–∏–∫—Å–∞–º (—É–ø—Ä–æ—â—ë–Ω–Ω—ã–π KnownSuffixAnalyzer)."""
        if len(word) < 4:
            return []

        results = []

        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –¥–ª–∏–Ω—ã —Å—É—Ñ—Ñ–∏–∫—Å–æ–≤ (–æ—Ç 4 –¥–æ 2 —Å–∏–º–≤–æ–ª–æ–≤)
        for suffix_len in [4, 3, 2]:
            if len(word) <= suffix_len:
                continue

            suffix = word[-suffix_len:]

            # –ò—â–µ–º —Å–ª–æ–≤–∞ —Å —Ç–∞–∫–∏–º –∂–µ —Å—É—Ñ—Ñ–∏–∫—Å–æ–º –≤ DAWG
            similar_words = []
            try:
                # –ü—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫: –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤–∞—Ä–∏–∞–Ω—Ç–æ–≤
                for test_stem in [
                    "–∫",
                    "–º",
                    "–ø",
                    "—Ç",
                    "–ª",
                    "–Ω",
                    "—Ä",
                    "—Å",
                    "–≤",
                    "–¥",
                    "–±",
                    "–≥",
                    "–∑",
                    "–∂",
                    "—Ö",
                ]:
                    test_word = test_stem + suffix
                    word_parses = self._dawg_dict.get_word_parses(test_word)
                    if word_parses:
                        similar_words.append((test_word, word_parses[0]))
                        if len(similar_words) >= 3:
                            break
            except Exception:
                pass

            # –ï—Å–ª–∏ –Ω–∞—à–ª–∏ –ø–æ—Ö–æ–∂–∏–µ —Å–ª–æ–≤–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∏—Ö –ø–∞—Ä–∞–¥–∏–≥–º—É
            if similar_words:
                # –ë–µ—Ä—ë–º –ø–µ—Ä–≤–æ–µ –ø–æ—Ö–æ–∂–µ–µ —Å–ª–æ–≤–æ
                similar_word, (paradigm_id, word_idx) = similar_words[0]

                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º stem –Ω–æ–≤–æ–≥–æ —Å–ª–æ–≤–∞
                # –ù—É–∂–Ω–æ –≤—ã—á–µ—Å—Ç—å —Å—É—Ñ—Ñ–∏–∫—Å –∏–∑ –ø–æ—Ö–æ–∂–µ–≥–æ —Å–ª–æ–≤–∞ –∏ –∑–∞–º–µ–Ω–∏—Ç—å –Ω–∞ –Ω–∞—à stem
                paradigm_info = self._dawg_dict.get_paradigm(paradigm_id, word_idx)
                if paradigm_info:
                    suffix_old, tag_string, prefix = paradigm_info

                    # –í—ã—á–∏—Å–ª—è–µ–º stem –ø–æ—Ö–æ–∂–µ–≥–æ —Å–ª–æ–≤–∞
                    similar_stem = similar_word
                    if prefix and similar_stem.startswith(prefix):
                        similar_stem = similar_stem[len(prefix) :]
                    if suffix_old and similar_stem.endswith(suffix_old):
                        similar_stem = similar_stem[: -len(suffix_old)]

                    # –ù–∞—à stem = –Ω–∞—à–µ —Å–ª–æ–≤–æ –º–∏–Ω—É—Å —Å—É—Ñ—Ñ–∏–∫—Å –º–∏–Ω—É—Å –ø—Ä–µ—Ñ–∏–∫—Å
                    our_stem = word
                    if prefix and our_stem.startswith(prefix):
                        our_stem = our_stem[len(prefix) :]
                    if suffix_old and our_stem.endswith(suffix_old):
                        our_stem = our_stem[: -len(suffix_old)]

                    # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–≥
                    pos, grammemes = self._dawg_dict.parse_tag_string(tag_string)

                    # –ü–æ–ª—É—á–∞–µ–º –Ω–æ—Ä–º–∞–ª—å–Ω—É—é —Ñ–æ—Ä–º—É
                    normal_form_info = self._dawg_dict.get_paradigm(paradigm_id, 0)
                    if normal_form_info:
                        normal_suffix, _, normal_prefix = normal_form_info
                        normal_form = normal_prefix + our_stem + normal_suffix
                    else:
                        normal_form = word

                    mawo_tag = MAWOTag(pos, grammemes)
                    mawo_parse = MAWOParse(
                        word=word,
                        normal_form=normal_form,
                        tag=mawo_tag,
                        score=0.5,  # –ü–æ–Ω–∏–∂–µ–Ω–Ω—ã–π score –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö
                        analyzer=self,
                        paradigm_id=paradigm_id,
                        stem=our_stem,
                    )
                    results.append(mawo_parse)
                    return results  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–µ—Ä–≤—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç

        return results

    def _correct_verb_aspect(
        self, parses: list[MAWOParse], word: str
    ) -> list[MAWOParse]:
        """–ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ –∞—Å–ø–µ–∫—Ç–∞ –≥–ª–∞–≥–æ–ª–æ–≤ —Å perfectivizing prefixes (NLP Best Practice 2025).

        –ü—Ä–æ–±–ª–µ–º–∞: Prediction –º–æ–∂–µ—Ç –Ω–µ–ø—Ä–∞–≤–∏–ª—å–Ω–æ –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –∞—Å–ø–µ–∫—Ç –≥–ª–∞–≥–æ–ª–∞ —Å –ø—Ä–∏—Å—Ç–∞–≤–∫–æ–π.
        –†–µ—à–µ–Ω–∏–µ: –î–ª—è –≥–ª–∞–≥–æ–ª–æ–≤ —Å –∏–∑–≤–µ—Å—Ç–Ω—ã–º–∏ perfectivizing prefixes –º–µ–Ω—è–µ–º impf ‚Üí perf.

        Args:
            parses: –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ prediction
            word: –ò—Å—Ö–æ–¥–Ω–æ–µ —Å–ª–æ–≤–æ

        Returns:
            –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –ø–∞—Ä—Å–æ–≤
        """
        # –ü—Ä–∏—Å—Ç–∞–≤–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –æ–±—ã—á–Ω–æ –æ–±—Ä–∞–∑—É—é—Ç —Å–æ–≤–µ—Ä—à–µ–Ω–Ω—ã–π –≤–∏–¥ (perf)
        # –û—Å–Ω–æ–≤–∞–Ω–æ –Ω–∞: –†—É—Å—Å–∫–∞—è –≥—Ä–∞–º–º–∞—Ç–∏–∫–∞ (1980), –ó–∞–ª–∏–∑–Ω—è–∫ (2003)
        # Web research 2025: —Å—Ç–∞—Ç—å–∏ –ø–æ –∞—Å–ø–µ–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç–∏ —Ä—É—Å—Å–∫–æ–≥–æ –≥–ª–∞–≥–æ–ª–∞
        perfectivizing_prefixes = {
            "–≤–∑",
            "–≤—Å",  # –≤–∑–ª–µ—Ç–µ—Ç—å, –≤—Å–∫–∏–ø–µ—Ç—å
            "–≤—ã",  # –≤—ã–±–µ–∂–∞—Ç—å, –≤—ã–ø–∏—Ç—å
            "–¥–æ",  # –¥–æ–±–µ–∂–∞—Ç—å, –¥–æ–ø–∏—Å–∞—Ç—å
            "–∑–∞",  # –∑–∞–±–µ–∂–∞—Ç—å, –∑–∞–ø–∏—Å–∞—Ç—å
            "–∏–∑",
            "–∏—Å",  # –∏–∑–±–µ–∂–∞—Ç—å, –∏–∑–º–µ–Ω–∏—Ç—å, –∏—Å–ø–µ—á—å
            "–Ω–∞",  # –Ω–∞–±—Ä–∞—Ç—å, –Ω–∞–ø–∏—Å–∞—Ç—å
            "–æ",
            "–æ–±",  # –æ–ø–∏—Å–∞—Ç—å, –æ–±–æ–π—Ç–∏
            "–æ—Ç",  # –æ—Ç–±–µ–∂–∞—Ç—å, –æ—Ç–ø–∏—Å–∞—Ç—å
            "–ø–µ—Ä–µ",  # –ø–µ—Ä–µ–±–µ–∂–∞—Ç—å, –ø–µ—Ä–µ–ø–∏—Å–∞—Ç—å
            "–ø–æ",  # –ø–æ–±–µ–∂–∞—Ç—å, –ø–æ–ø–∏—Ç—å (–ù–û: –º–æ–∂–µ—Ç –±—ã—Ç—å –∏ impf!)
            "–ø–æ–¥",  # –ø–æ–¥–±–µ–∂–∞—Ç—å, –ø–æ–¥–ø–∏—Å–∞—Ç—å
            "–ø—Ä–∏",  # –ø—Ä–∏–±–µ–∂–∞—Ç—å, –ø—Ä–∏–ø–∏—Å–∞—Ç—å
            "–ø—Ä–æ",  # –ø—Ä–æ–±–µ–∂–∞—Ç—å, –ø—Ä–æ–ø–∏—Å–∞—Ç—å
            "—Ä–∞–∑",
            "—Ä–∞—Å",  # —Ä–∞–∑–±–µ–∂–∞—Ç—å—Å—è, —Ä–∞—Å–ø–∏—Å–∞—Ç—å
            "—Å",  # —Å–±–µ–∂–∞—Ç—å, —Å–ø–∏—Å–∞—Ç—å
            "—É",  # —É–±–µ–∂–∞—Ç—å, —É–ø–∏—Å–∞—Ç—å
        }

        corrected_parses = []

        for parse in parses:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º: —ç—Ç–æ –≥–ª–∞–≥–æ–ª –ò –æ–Ω –∏–º–µ–µ—Ç impf
            if parse.tag.POS in ("VERB", "INFN") and "impf" in parse.tag.grammemes:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ perfectivizing prefix
                has_perf_prefix = False
                for prefix in perfectivizing_prefixes:
                    if word.startswith(prefix) and len(word) > len(prefix) + 2:
                        has_perf_prefix = True
                        break

                if has_perf_prefix:
                    # –ú–µ–Ω—è–µ–º impf ‚Üí perf
                    new_grammemes = parse.tag.grammemes.copy()
                    new_grammemes.discard("impf")
                    new_grammemes.add("perf")

                    # –°–æ–∑–¥–∞—ë–º –Ω–æ–≤—ã–π parse —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω—ã–º –∞—Å–ø–µ–∫—Ç–æ–º
                    corrected_parse = MAWOParse(
                        word=parse.word,
                        normal_form=parse.normal_form,
                        tag=MAWOTag(parse.tag.POS, new_grammemes),
                        score=parse.score,
                        analyzer=parse._analyzer,
                        paradigm_id=parse.paradigm_id if hasattr(parse, "paradigm_id") else None,
                        stem=parse.stem if hasattr(parse, "stem") else None,
                    )
                    corrected_parses.append(corrected_parse)
                else:
                    # –ù–µ—Ç perfectivizing prefix, –æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å
                    corrected_parses.append(parse)
            else:
                # –ù–µ –≥–ª–∞–≥–æ–ª –∏–ª–∏ —É–∂–µ perf, –æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å
                corrected_parses.append(parse)

        return corrected_parses

    def _analyze_by_patterns(self, word: str) -> list[MAWOParse]:
        """–ê–Ω–∞–ª–∏–∑ —Å–ª–æ–≤–∞ –ø–æ –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–º –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º."""
        results: list[Any] = []

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–∫–æ–Ω—á–∞–Ω–∏—è –≥–ª–∞–≥–æ–ª–æ–≤
        for ending, (pos, grammemes) in self.patterns["verb_endings"].items():
            if word.endswith(ending) and len(word) > len(ending):
                normal_form = word[: -len(ending)] + "—Ç—å" if pos == "INFN" else word
                tag = MAWOTag(pos, grammemes)
                results.append(MAWOParse(word, normal_form, tag, 0.6, self))

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–∫–æ–Ω—á–∞–Ω–∏—è –ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω—ã—Ö
        for ending, (pos, grammemes) in self.patterns["adj_endings"].items():
            if word.endswith(ending) and len(word) > len(ending):
                normal_form = word[: -len(ending)] + "—ã–π"
                tag = MAWOTag(pos, grammemes)
                results.append(MAWOParse(word, normal_form, tag, 0.6, self))

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–∫–æ–Ω—á–∞–Ω–∏—è —Å—É—â–µ—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã—Ö
        for ending, (gender, case, number) in self.patterns["noun_endings"].items():
            if word.endswith(ending) and len(word) > len(ending):
                normal_form = word  # –£–ø—Ä–æ—â–µ–Ω–Ω–æ - –Ω—É–∂–Ω–æ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ
                grammemes = {gender, case, number, "inan"}
                tag = MAWOTag("NOUN", grammemes)
                results.append(MAWOParse(word, normal_form, tag, 0.4, self))

        # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–æ–µ —Å–ª–æ–≤–æ
        if not results:
            tag = MAWOTag("UNKN", set())
            results.append(MAWOParse(word, word, tag, 0.1, self))

        return results

    # ========== SPECIAL ANALYZERS HELPER METHODS ==========

    def _is_punctuation(self, word: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–æ–∫–µ–Ω –ø—É–Ω–∫—Ç—É–∞—Ü–∏–µ–π."""
        import string

        # –†–∞—Å—à–∏—Ä–µ–Ω–Ω–∞—è –ø—É–Ω–∫—Ç—É–∞—Ü–∏—è (–≤–∫–ª—é—á–∞—è Unicode)
        punct_chars = set(string.punctuation + "‚Ä¶‚Äî‚Äì")
        return all(c in punct_chars for c in word) and len(word) > 0

    def _analyze_number(self, word: str) -> list[MAWOParse] | None:
        """–ê–Ω–∞–ª–∏–∑ —á–∏—Å–ª–æ–≤—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤."""
        import re

        # Integer: 123, 0
        if re.match(r"^\d+$", word):
            return [
                MAWOParse(
                    word=word,
                    normal_form=word,
                    tag=MAWOTag("NUMB", {"intg"}),
                    score=1.0,
                    analyzer=self,
                )
            ]

        # Real number: 123.1 or 123,1
        if re.match(r"^\d+[.,]\d+$", word):
            return [
                MAWOParse(
                    word=word,
                    normal_form=word,
                    tag=MAWOTag("NUMB", {"real"}),
                    score=1.0,
                    analyzer=self,
                )
            ]

        return None

    def _analyze_roman(self, word: str) -> list[MAWOParse] | None:
        """–ê–Ω–∞–ª–∏–∑ —Ä–∏–º—Å–∫–∏—Ö —Ü–∏—Ñ—Ä."""
        import re

        # –†–∏–º—Å–∫–∏–µ —Ü–∏—Ñ—Ä—ã: I, V, X, L, C, D, M (case insensitive)
        if re.match(r"^[IVXLCDM]+$", word.upper()) and len(word) > 0:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —ç—Ç–æ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ –ø–æ—Ö–æ–∂–µ –Ω–∞ —Ä–∏–º—Å–∫—É—é —Ü–∏—Ñ—Ä—É
            # (–Ω–µ –ø—Ä–æ—Å—Ç–æ —Å–ª—É—á–∞–π–Ω–∞—è –∫–æ–º–±–∏–Ω–∞—Ü–∏—è –±—É–∫–≤)
            upper_word = word.upper()
            # –ë–∞–∑–æ–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ —Ä–∏–º—Å–∫–æ–π —Ü–∏—Ñ—Ä—ã
            if self._is_valid_roman(upper_word):
                return [
                    MAWOParse(
                        word=word,
                        normal_form=word.lower(),
                        tag=MAWOTag("ROMN", set()),
                        score=1.0,
                        analyzer=self,
                    )
                ]

        return None

    def _is_valid_roman(self, word: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç–∏ —Ä–∏–º—Å–∫–æ–π —Ü–∏—Ñ—Ä—ã."""
        # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞: —Ä–∏–º—Å–∫–∏–µ —Ü–∏—Ñ—Ä—ã –æ–±—ã—á–Ω–æ —Å–æ–¥–µ—Ä–∂–∞—Ç I, V, X
        # –∏ –Ω–µ —Å–æ–¥–µ—Ä–∂–∞—Ç –±–æ–ª–µ–µ 3 –æ–¥–∏–Ω–∞–∫–æ–≤—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤ –ø–æ–¥—Ä—è–¥ (–∫—Ä–æ–º–µ M)
        valid_chars = set("IVXLCDM")
        if not set(word).issubset(valid_chars):
            return False

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –ø–æ–≤—Ç–æ—Ä—ã (–Ω–µ –±–æ–ª–µ–µ 3-4 –ø–æ–¥—Ä—è–¥)
        for char in "IVXLCD":
            if char * 4 in word:
                return False

        return True

    def _is_latin(self, word: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Ç–µ–∫—Å—Ç –ª–∞—Ç–∏–Ω—Å–∫–∏–º (–Ω–µ –∫–∏—Ä–∏–ª–ª–∏—Ü–∞)."""
        # –õ–∞—Ç–∏–Ω—Å–∫–∏–π —Ç–µ–∫—Å—Ç - —Å–æ–¥–µ—Ä–∂–∏—Ç —Ö–æ—Ç—è –±—ã –æ–¥–Ω—É –ª–∞—Ç–∏–Ω—Å–∫—É—é –±—É–∫–≤—É
        has_latin = any("a" <= c.lower() <= "z" for c in word)
        has_cyrillic = any("–∞" <= c.lower() <= "—è" or c.lower() == "—ë" for c in word)

        if "-" in word and has_latin and has_cyrillic:
            # Compound word —Å –ª–∞—Ç–∏–Ω–∏—Ü–µ–π –∏ –∫–∏—Ä–∏–ª–ª–∏—Ü–µ–π
            # –ü—Ä–∏–º–µ—Ä—ã:
            # - "–†–µ—Ç—Ä–æ-FM" ‚Üí LATN (FM - –Ω–µ–∏–∑–º–µ–Ω—è–µ–º–∞—è –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä–∞)
            # - "pdf-–¥–æ–∫—É–º–µ–Ω—Ç–æ–≤" ‚Üí NOUN (–¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ - —Å–∫–ª–æ–Ω—è–µ—Ç—Å—è)

            parts = word.split("-")
            latin_parts = []
            cyrillic_parts = []

            for part in parts:
                part_has_latin = any("a" <= c.lower() <= "z" for c in part)
                part_has_cyrillic = any("–∞" <= c.lower() <= "—è" or c.lower() == "—ë" for c in part)

                if part_has_latin:
                    latin_parts.append(part)
                if part_has_cyrillic:
                    cyrillic_parts.append(part)

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—ã–≥–ª—è–¥–∏—Ç –ª–∏ –∫–∏—Ä–∏–ª–ª–∏—á–µ—Å–∫–∞—è —á–∞—Å—Ç—å –∫–∞–∫ —Å–∫–ª–æ–Ω—è–µ–º–æ–µ —Å–ª–æ–≤–æ
            # –ï—Å–ª–∏ –¥–∞ ‚Üí compound word ‚Üí –Ω–µ LATN
            # –ï—Å–ª–∏ –Ω–µ—Ç ‚Üí —Å–∫–æ—Ä–µ–µ LATN
            inflection_endings = ["–æ–≤", "–∞–º", "–∞–º–∏", "–∞—Ö", "—è–º–∏", "—è—Ö", "–µ–π", "–æ–π", "—É—é", "–æ–º"]
            for cyrillic_part in cyrillic_parts:
                for ending in inflection_endings:
                    if cyrillic_part.endswith(ending) and len(cyrillic_part) > len(ending) + 2:
                        # –ö–∏—Ä–∏–ª–ª–∏—á–µ—Å–∫–∞—è —á–∞—Å—Ç—å —Å–∫–ª–æ–Ω—è–µ—Ç—Å—è ‚Üí compound word ‚Üí –Ω–µ LATN
                        return False

            # –ï—Å–ª–∏ –∫–∏—Ä–∏–ª–ª–∏—á–µ—Å–∫–∞—è —á–∞—Å—Ç—å –Ω–µ —Å–∫–ª–æ–Ω—è–µ—Ç—Å—è –∏ –µ—Å—Ç—å –ª–∞—Ç–∏–Ω–∏—Ü–∞ ‚Üí LATN
            return True
        else:
            # –û–±—ã—á–Ω–æ–µ —Å–ª–æ–≤–æ - –ª–∞—Ç–∏–Ω–∏—Ü–∞ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –ù–ï–¢ –∫–∏—Ä–∏–ª–ª–∏—Ü—ã
            return has_latin and not has_cyrillic

    # ========== PATTERN ANALYZERS (2024-2025) ==========

    def _analyze_superlative(self, word: str) -> list[MAWOParse] | None:
        """–ê–Ω–∞–ª–∏–∑ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω–æ–π —Å—Ç–µ–ø–µ–Ω–∏ –ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω—ã—Ö —Å –ø—Ä–µ—Ñ–∏–∫—Å–æ–º –ù–ê–ò-.

        –ü—Ä–∏–º–µ—Ä—ã: –Ω–∞–∏–Ω–µ–≤–µ—Ä–æ—è—Ç–Ω–µ–π—à–∏–π ‚Üí –≤–µ—Ä–æ—è—Ç–Ω—ã–π, –Ω–∞–∏—Å—Ç–∞—Ä–µ–π—à–∏–π ‚Üí —Å—Ç–∞—Ä—ã–π
        """
        if not word.startswith("–Ω–∞–∏"):
            return None

        # –£–±–∏—Ä–∞–µ–º –ø—Ä–µ—Ñ–∏–∫—Å "–Ω–∞–∏"
        word_without_nai = word[3:]

        # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –±–∞–∑–æ–≤—É—é —Ñ–æ—Ä–º—É —á–µ—Ä–µ–∑ suffixes -–µ–π—à/-–∞–π—à
        for superlative_suffix in [
            "–µ–π—à–∏–π",
            "–µ–π—à–∞—è",
            "–µ–π—à–µ–µ",
            "–µ–π—à–∏–µ",
            "–µ–π—à–µ–≥–æ",
            "–µ–π—à–µ–º—É",
            "–µ–π—à–∏–º",
            "–µ–π—à–∏—Ö",
            "–∞–π—à–∏–π",
            "–∞–π—à–∞—è",
            "–∞–π—à–µ–µ",
            "–∞–π—à–∏–µ",
            "–∞–π—à–µ–≥–æ",
            "–∞–π—à–µ–º—É",
            "–∞–π—à–∏–º",
            "–∞–π—à–∏—Ö",
        ]:
            if word_without_nai.endswith(superlative_suffix):
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –æ—Å–Ω–æ–≤—É
                stem = word_without_nai[: -len(superlative_suffix)]

                # –ü—Ä–æ–±—É–µ–º —É–±—Ä–∞—Ç—å –ø—Ä–µ—Ñ–∏–∫—Å –ù–ï- –µ—Å–ª–∏ –µ—Å—Ç—å
                # –Ω–∞–∏–Ω–µ–≤–µ—Ä–æ—è—Ç–Ω–µ–π—à–∏–π ‚Üí –≤–µ—Ä–æ—è—Ç–Ω—ã–π (–∞ –Ω–µ –Ω–µ–≤–µ—Ä–æ—è—Ç–Ω—ã–π)
                # –í–ê–ñ–ù–û: —Å–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–±—É–µ–º –ë–ï–ó "–Ω–µ-", –ø–æ—Ç–æ–º —Å "–Ω–µ-"
                stem_variants = []
                if stem.startswith("–Ω–µ") and len(stem) > 3:
                    stem_without_ne = stem[2:]
                    stem_variants.append(stem_without_ne)  # –°–ù–ê–ß–ê–õ–ê –±–µ–∑ –Ω–µ-
                stem_variants.append(stem)  # –ü–û–¢–û–ú —Å –Ω–µ-

                # –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –±–∞–∑–æ–≤–æ–µ –ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω–æ–µ
                # –î–æ–±–∞–≤–ª—è–µ–º –æ–∫–æ–Ω—á–∞–Ω–∏–µ -—ã–π/-–Ω—ã–π/-–∏–π
                for stem_variant in stem_variants:
                    for base_ending in ["–Ω—ã–π", "—ã–π", "–∏–π"]:
                        base_word = stem_variant + base_ending
                        base_parses = self._dawg_dict.get_word_parses(base_word) if self._dawg_dict else []

                        if base_parses:
                            # –ù–∞—à–ª–∏ –±–∞–∑–æ–≤–æ–µ –ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω–æ–µ –≤ —Å–ª–æ–≤–∞—Ä–µ
                            paradigm_id, word_idx = base_parses[0]
                            paradigm_info = self._dawg_dict.get_paradigm(paradigm_id, word_idx)

                            if paradigm_info:
                                suffix, tag_string, prefix = paradigm_info
                                pos, grammemes = self._dawg_dict.parse_tag_string(tag_string)

                                # –ü–æ–ª—É—á–∞–µ–º –Ω–æ—Ä–º–∞–ª—å–Ω—É—é —Ñ–æ—Ä–º—É –±–∞–∑–æ–≤–æ–≥–æ –ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω–æ–≥–æ
                                normal_form_info = self._dawg_dict.get_paradigm(paradigm_id, 0)
                                if normal_form_info:
                                    normal_suffix, _, normal_prefix = normal_form_info
                                    # –°–æ–±–∏—Ä–∞–µ–º –æ—Å–Ω–æ–≤—É –∏–∑ –±–∞–∑–æ–≤–æ–≥–æ —Å–ª–æ–≤–∞
                                    base_stem = base_word
                                    if prefix and base_stem.startswith(prefix):
                                        base_stem = base_stem[len(prefix) :]
                                    if suffix and base_stem.endswith(suffix):
                                        base_stem = base_stem[: -len(suffix)]

                                    normal_form = normal_prefix + base_stem + normal_suffix

                                    # –î–æ–±–∞–≤–ª—è–µ–º –≥—Ä–∞–º–º–µ–º—É Supr (–ø—Ä–µ–≤–æ—Å—Ö–æ–¥–Ω–∞—è —Å—Ç–µ–ø–µ–Ω—å)
                                    grammemes_with_supr = grammemes | {"Supr"}
                                    tag = MAWOTag(pos, grammemes_with_supr)

                                    return [
                                        MAWOParse(
                                            word=word,
                                            normal_form=normal_form,
                                            tag=tag,
                                            score=1.0,
                                            analyzer=self,
                                        )
                                    ]

        return None

    def _analyze_po_adverb(self, word: str) -> list[MAWOParse] | None:
        """–ê–Ω–∞–ª–∏–∑ –Ω–∞—Ä–µ—á–∏–π —Å –ø—Ä–µ—Ñ–∏–∫—Å–æ–º –ü–û-.

        –ü—Ä–∏–º–µ—Ä—ã: –ø–æ-—Ç–µ–∞—Ç—Ä–∞–ª—å–Ω–æ–º—É, –ø–æ-–≤–æ—Ä–æ–±—å–∏–Ω–æ–º—É, –ø–æ-—Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–∏
        –ü—Ä–∞–≤–∏–ª–æ: –ü–û- + adjective(-–æ–º—É/-–µ–º—É) –∏–ª–∏ –ü–û- + adjective(-—Å–∫–∏/-—Ü–∫–∏/-—å–∏)
        """
        if not word.startswith("–ø–æ-"):
            return None

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã –Ω–∞—Ä–µ—á–∏–π —Å –ü–û-
        # 1. –ø–æ- + -–æ–º—É/-–µ–º—É (–ø–æ-—Ç–µ–∞—Ç—Ä–∞–ª—å–Ω–æ–º—É, –ø–æ-–Ω–æ–≤–æ–º—É)
        if word.endswith("–æ–º—É") or word.endswith("–µ–º—É"):
            # –ù–æ—Ä–º–∞–ª—å–Ω–∞—è —Ñ–æ—Ä–º–∞ = —Å–∞–º–æ —Å–ª–æ–≤–æ (–Ω–∞—Ä–µ—á–∏—è –Ω–µ —Å–∫–ª–æ–Ω—è—é—Ç—Å—è)
            return [
                MAWOParse(
                    word=word,
                    normal_form=word,
                    tag=MAWOTag("ADVB", set()),
                    score=1.0,
                    analyzer=self,
                )
            ]

        # 2. –ø–æ- + -—Å–∫–∏/-—Ü–∫–∏ (–ø–æ-—Ñ—Ä–∞–Ω—Ü—É–∑—Å–∫–∏, –ø–æ-–Ω–µ–º–µ—Ü–∫–∏)
        if word.endswith("—Å–∫–∏") or word.endswith("—Ü–∫–∏"):
            return [
                MAWOParse(
                    word=word,
                    normal_form=word,
                    tag=MAWOTag("ADVB", set()),
                    score=1.0,
                    analyzer=self,
                )
            ]

        # 3. –ø–æ- + -—å–∏ (–ø–æ-–ª–∏—Å—å–∏, –ø–æ-–∑–∞—è—á—å–∏)
        if word.endswith("—å–∏"):
            return [
                MAWOParse(
                    word=word,
                    normal_form=word,
                    tag=MAWOTag("ADVB", set()),
                    score=1.0,
                    analyzer=self,
                )
            ]

        return None

    def _analyze_compound_word(self, word: str) -> list[MAWOParse] | None:
        """–ê–Ω–∞–ª–∏–∑ —Å–æ—Å—Ç–∞–≤–Ω—ã—Ö —Å–ª–æ–≤ —Å –¥–µ—Ñ–∏—Å–æ–º (CompoundWordAnalyzer).

        –¢–∏–ø—ã:
        1. Immutable left + mutable right: –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–º–∞–≥–∞–∑–∏–Ω–∞, pdf-–¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        2. Both parts mutable: –∫–æ–º–∞–Ω–¥-—É—á–∞—Å—Ç–Ω–∏—Ü, –ø–æ–µ–∑–¥–æ–≤-—ç–∫—Å–ø—Ä–µ—Å—Å–æ–≤
        3. Adverbs: –±—ã—Å—Ç—Ä–æ-–±—ã—Å—Ç—Ä–æ (—É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ –æ—Ç–¥–µ–ª—å–Ω–æ)
        """
        if "-" not in word:
            return None

        parts = word.split("-", 1)  # –†–∞–∑–±–∏–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–æ –ø–µ—Ä–≤–æ–º—É –¥–µ—Ñ–∏—Å—É
        if len(parts) != 2:
            return None

        left_part, right_part = parts

        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–æ–±—Ä–∞—Ç—å –ø—Ä–∞–≤—É—é —á–∞—Å—Ç—å
        right_parses = self._parse_word_base(right_part)

        if not right_parses or right_parses[0].tag.POS == "UNKN":
            return None

        # –ü—Ä–æ–≤–µ—Ä–∫–∞: –µ—Å–ª–∏ –ø—Ä–∞–≤–∞—è —á–∞—Å—Ç—å - —á–∞—Å—Ç–∏—Ü–∞ (PRCL), —ç—Ç–æ –Ω–µ compound word
        # –∞ —Å–ª–æ–≤–æ —Å —ç–Ω–∫–ª–∏—Ç–∏—á–µ—Å–∫–æ–π —á–∞—Å—Ç–∏—Ü–µ–π (—Å–∫–∞–∂–∏-–∫–∞, –≥–¥–µ-—Ç–æ –∏ —Ç.–¥.)
        # –¢–∞–∫–∏–µ —Å–ª–æ–≤–∞ –¥–æ–ª–∂–Ω—ã –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å—Å—è HyphenSeparatedParticleAnalyzer
        if right_parses[0].tag.POS == "PRCL":
            return None

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ª–µ–≤—É—é —á–∞—Å—Ç—å
        left_parses = self._parse_word_base(left_part)

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –ª–µ–≤–∞—è —á–∞—Å—Ç—å immutable prefix
        # –ü—Ä–∏–º–µ—Ä—ã: –∞–º–º–∏–∞—á–Ω–æ-—Å–µ–ª–∏—Ç—Ä–æ–≤—ã–π, –ø–æ—á—Ç–æ–≤–æ-–±–∞–Ω–∫–æ–≤—Å–∫–∏–π
        # –ü—Ä–∏–∑–Ω–∞–∫–∏: –∫—Ä–∞—Ç–∫–æ–µ –ø—Ä–∏–ª–∞–≥–∞—Ç–µ–ª—å–Ω–æ–µ (ADJS) –Ω–∞ -–æ, –∏–ª–∏ –Ω–∞—Ä–µ—á–∏–µ (ADVB) –Ω–∞ -–æ
        is_immutable_left = False
        if left_parses and left_parses[0].tag.POS in ("ADJS", "ADVB"):
            if left_part.endswith("–æ") or left_part.endswith("–µ"):
                is_immutable_left = True

        if left_parses and left_parses[0].tag.POS != "UNKN" and left_parses[0].score >= 1.0 and not is_immutable_left:
            # ====== BOTH PARTS MUTABLE ======
            # –û–±–µ —á–∞—Å—Ç–∏ –µ—Å—Ç—å –≤ —Å–ª–æ–≤–∞—Ä–µ ‚Üí –æ–±–µ —Å–∫–ª–æ–Ω—è—é—Ç—Å—è
            # –∫–æ–º–∞–Ω–¥-—É—á–∞—Å—Ç–Ω–∏—Ü: –∫–æ–º–∞–Ω–¥–∞(gent,plur) + —É—á–∞—Å—Ç–Ω–∏—Ü–∞(gent,plur)
            # –¥—É–ª-–Ω–∞–¥—É–≤–∞–ª—Å—è: –¥—É—Ç—å(VERB) + –Ω–∞–¥—É–≤–∞—Ç—å—Å—è(VERB)

            right_tag = right_parses[0].tag

            # –ï—Å–ª–∏ –ø—Ä–∞–≤–∞—è —á–∞—Å—Ç—å - –≥–ª–∞–≥–æ–ª, –∏—â–µ–º –≥–ª–∞–≥–æ–ª –∏ –≤ –ª–µ–≤–æ–π
            # (–¥—É–ª –º–æ–∂–µ—Ç –±—ã—Ç—å –∏ "–¥—É–ª–æ" NOUN, –∏ "–¥—É—Ç—å" VERB)
            left_parse_to_use = left_parses[0]
            if right_tag.POS in ("VERB", "INFN") and len(left_parses) > 1:
                for left_p in left_parses:
                    if left_p.tag.POS in ("VERB", "INFN"):
                        left_parse_to_use = left_p
                        break

            # –ë–µ—Ä–µ–º —Ç–µ–≥–∏ –∏–∑ –æ–±–µ–∏—Ö —á–∞—Å—Ç–µ–π
            left_tag = left_parse_to_use.tag

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –æ–±—â–∏–π POS (–æ–±—ã—á–Ω–æ –æ—Ç –ø—Ä–∞–≤–æ–π —á–∞—Å—Ç–∏)
            pos = right_tag.POS

            # –ì—Ä–∞–º–º–µ–º—ã: –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –∏–∑ –ø—Ä–∞–≤–æ–π —á–∞—Å—Ç–∏ (–æ–Ω–∞ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç —Å–∫–ª–æ–Ω–µ–Ω–∏–µ)
            # –ù–û: animacy (anim/inan) –∏ transitivity (tran/intr) - –∏–∑ –õ–ï–í–û–ô —á–∞—Å—Ç–∏!
            grammemes = right_tag.grammemes.copy()

            # –ó–∞–º–µ–Ω—è–µ–º animacy –∏–∑ –ª–µ–≤–æ–π —á–∞—Å—Ç–∏
            left_animacy = left_tag.grammemes & {"anim", "inan"}
            if left_animacy:
                # –£–±–∏—Ä–∞–µ–º animacy –∏–∑ –ø—Ä–∞–≤–æ–π
                grammemes = grammemes - {"anim", "inan"}
                # –î–æ–±–∞–≤–ª—è–µ–º animacy –∏–∑ –ª–µ–≤–æ–π
                grammemes = grammemes | left_animacy

            # –ó–∞–º–µ–Ω—è–µ–º transitivity –∏–∑ –ª–µ–≤–æ–π —á–∞—Å—Ç–∏ (–¥–ª—è –≥–ª–∞–≥–æ–ª–æ–≤)
            left_trans = left_tag.grammemes & {"tran", "intr"}
            if left_trans:
                # –£–±–∏—Ä–∞–µ–º transitivity –∏–∑ –ø—Ä–∞–≤–æ–π
                grammemes = grammemes - {"tran", "intr"}
                # –î–æ–±–∞–≤–ª—è–µ–º transitivity –∏–∑ –ª–µ–≤–æ–π
                grammemes = grammemes | left_trans

            # –ù–æ—Ä–º–∞–ª—å–Ω–∞—è —Ñ–æ—Ä–º–∞ = normal_form –ª–µ–≤–æ–π + –¥–µ—Ñ–∏—Å + normal_form –ø—Ä–∞–≤–æ–π
            normal_form = left_parse_to_use.normal_form + "-" + right_parses[0].normal_form

            return [
                MAWOParse(
                    word=word,
                    normal_form=normal_form,
                    tag=MAWOTag(pos, grammemes),
                    score=1.0,
                    analyzer=self,
                )
            ]
        else:
            # ====== IMMUTABLE LEFT + MUTABLE RIGHT ======
            # –õ–µ–≤–∞—è —á–∞—Å—Ç—å –Ω–µ –≤ —Å–ª–æ–≤–∞—Ä–µ –∏–ª–∏ UNKN ‚Üí –æ–Ω–∞ –Ω–µ —Å–∫–ª–æ–Ω—è–µ—Ç—Å—è
            # –∏–Ω—Ç–µ—Ä–Ω–µ—Ç-–º–∞–≥–∞–∑–∏–Ω–∞, pdf-–¥–æ–∫—É–º–µ–Ω—Ç–æ–≤, –∞–º–º–∏–∞—á–Ω–æ-—Å–µ–ª–∏—Ç—Ä–æ–≤–æ–≥–æ

            # –õ–µ–≤–∞—è —á–∞—Å—Ç—å –æ—Å—Ç–∞–µ—Ç—Å—è –∫–∞–∫ –µ—Å—Ç—å (immutable)
            # –ü—Ä–∞–≤–∞—è —á–∞—Å—Ç—å –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –≤—Å–µ –≥—Ä–∞–º–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏

            right_tag = right_parses[0].tag

            # –ù–æ—Ä–º–∞–ª—å–Ω–∞—è —Ñ–æ—Ä–º–∞ = –ª–µ–≤–∞—è —á–∞—Å—Ç—å + –¥–µ—Ñ–∏—Å + normal_form –ø—Ä–∞–≤–æ–π
            normal_form = left_part + "-" + right_parses[0].normal_form

            return [
                MAWOParse(
                    word=word,
                    normal_form=normal_form,
                    tag=MAWOTag(right_tag.POS, right_tag.grammemes),
                    score=1.0,
                    analyzer=self,
                )
            ]


class MAWOOptimizedMorphAnalyzer:
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–ª—è MAWO —Å–∏—Å—Ç–µ–º—ã
    –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –º–µ—Ç–æ–¥–∞–º–∏ –æ–±—É—á–µ–Ω–∏—è.
    """

    def __init__(self, dict_path: str | None = None) -> None:
        self.base_analyzer = create_analyzer(dict_path)
        self.cache: dict[str, list[dict[str, Any]]] = {}
        logger.info("‚úÖ MAWOOptimizedMorphAnalyzer initialized")

    def analyze(self, text: str) -> list[dict[str, Any]]:
        """–ê–Ω–∞–ª–∏–∑ —Ç–µ–∫—Å—Ç–∞ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.

        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞

        Returns:
            –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞

        """
        if not text:
            return []

        if text in self.cache:
            return self.cache[text]

        words = text.split()
        results: list[Any] = []

        for word in words:
            if word.isalpha():
                parses = self.base_analyzer.parse(word)
                if parses:
                    best_parse = parses[0]  # –ë–µ—Ä–µ–º –ª—É—á—à–∏–π —Ä–∞–∑–±–æ—Ä
                    results.append(
                        {
                            "word": word,
                            "normal_form": best_parse.normal_form,
                            "pos": best_parse.tag.POS,
                            "case": best_parse.tag.case,
                            "number": best_parse.tag.number,
                            "gender": best_parse.tag.gender,
                            "aspect": best_parse.tag.aspect,
                            "tense": best_parse.tag.tense,
                            "score": best_parse.score,
                            "analysis_mode": "mawo_morphology",
                        },
                    )

        # –ö—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        self.cache[text] = results
        return results


def create_analyzer(dict_path: str | None = None, use_dawg: bool = True) -> MAWOMorphAnalyzer:
    """–°–æ–∑–¥–∞–µ—Ç –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä MAWO (—Å–∏–Ω–≥–ª—Ç–æ–Ω).

    –í–ê–ñ–ù–û: –ò—Å–ø–æ–ª—å–∑—É–µ—Ç DAWG —Å–ª–æ–≤–∞—Ä–∏ –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ –∏ –º–∞–ª–æ–≥–æ –ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏—è –ø–∞–º—è—Ç–∏.
    - –ó–∞–≥—Ä—É–∑–∫–∞: ~1-2 —Å–µ–∫—É–Ω–¥—ã
    - –ü–∞–º—è—Ç—å: ~15-20 –ú–ë (–≤–º–µ—Å—Ç–æ ~500 –ú–ë)
    - Thread-safe —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è —Å double-checked locking

    Args:
        dict_path: –ü—É—Ç—å –∫ —Å–ª–æ–≤–∞—Ä—é (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é dicts_ru/)
        use_dawg: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å DAWG –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—é (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é True)

    Returns:
        –≠–∫–∑–µ–º–ø–ª—è—Ä MAWOMorphAnalyzer (—Å–∏–Ω–≥–ª—Ç–æ–Ω –≤ —Ä–∞–º–∫–∞—Ö –ø—Ä–æ—Ü–µ—Å—Å–∞)

    """
    global _GLOBAL_ANALYZER_INSTANCE, _ANALYZER_LOCK

    # –ë—ã—Å—Ç—Ä–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –±–µ–∑ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏
    if _GLOBAL_ANALYZER_INSTANCE is not None:
        logger.debug("‚ö° Returning existing singleton analyzer instance (fast path)")  # type: ignore[unreachable]
        return _GLOBAL_ANALYZER_INSTANCE

    # –ú–µ–¥–ª–µ–Ω–Ω—ã–π –ø—É—Ç—å —Å –±–ª–æ–∫–∏—Ä–æ–≤–∫–æ–π
    if _ANALYZER_LOCK:
        with _ANALYZER_LOCK:
            # Double-checked locking: –ø—Ä–æ–≤–µ—Ä—è–µ–º —Å–Ω–æ–≤–∞ –≤–Ω—É—Ç—Ä–∏ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∏
            if _GLOBAL_ANALYZER_INSTANCE is None:
                logger.info("üîÑ Creating new singleton analyzer instance (thread-safe)")
                _GLOBAL_ANALYZER_INSTANCE = MAWOMorphAnalyzer(dict_path, use_dawg=use_dawg)
            else:
                logger.debug("‚ö° Another thread created instance, using it")  # type: ignore[unreachable]
    # PRODUCTION REQUIRED –±–µ–∑ threading (fallback –¥–ª—è —Å—Ç–∞—Ä—ã—Ö —Å–∏—Å—Ç–µ–º)
    elif _GLOBAL_ANALYZER_INSTANCE is None:
        logger.info("üîÑ Creating new singleton analyzer instance (no threading)")
        _GLOBAL_ANALYZER_INSTANCE = MAWOMorphAnalyzer(dict_path, use_dawg=use_dawg)

    return _GLOBAL_ANALYZER_INSTANCE


def get_global_analyzer() -> MAWOMorphAnalyzer:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –≥–ª–æ–±–∞–ª—å–Ω–æ–≥–æ —ç–∫–∑–µ–º–ø–ª—è—Ä–∞ –º–æ—Ä—Ñ–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞ (—Å–∏–Ω–≥–ª—Ç–æ–Ω).

    –≠—Ç–∞ —Ñ—É–Ω–∫—Ü–∏—è —ç–∫–≤–∏–≤–∞–ª–µ–Ω—Ç–Ω–∞ create_analyzer() –±–µ–∑ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤,
    –Ω–æ —è–≤–Ω–æ –ø–æ–¥—á–µ—Ä–∫–∏–≤–∞–µ—Ç —á—Ç–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç—Å—è –≥–ª–æ–±–∞–ª—å–Ω—ã–π —Å–∏–Ω–≥–ª—Ç–æ–Ω.

    Returns:
        –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä MAWOMorphAnalyzer
    """
    return create_analyzer()


class MAWODictionaryManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è DAWG –∫—ç—à–µ–º –∏ —Å–ª–æ–≤–∞—Ä—è–º–∏ OpenCorpora."""

    def __init__(self, dict_path: Path | None = None) -> None:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–µ–Ω–µ–¥–∂–µ—Ä–∞ —Å–ª–æ–≤–∞—Ä–µ–π.

        Args:
            dict_path: –ü—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å–æ —Å–ª–æ–≤–∞—Ä—è–º–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        """
        if dict_path is None:
            dict_path = Path(__file__).parent / "dicts_ru"
        self.dict_path = Path(dict_path)
        self.dawg_cache_path = self.dict_path / "words.dawg"

    def is_dawg_cache_available(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ DAWG –∫—ç—à–∞.

        Returns:
            True –µ—Å–ª–∏ DAWG —Å–ª–æ–≤–∞—Ä—å —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        """
        return self.dawg_cache_path.exists()

    def build_dawg_cache(self) -> bool:
        """–°–æ–∑–¥–∞–µ—Ç DAWG –∫—ç—à –∏–∑ OpenCorpora XML.

        Returns:
            True –µ—Å–ª–∏ –∫—ç—à —É—Å–ø–µ—à–Ω–æ —Å–æ–∑–¥–∞–Ω
        """
        logger.info("üî® Building DAWG cache from OpenCorpora XML...")

        try:
            # –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º DAWG optimizer
            from .dawg_optimizer import get_dawg_optimizer

            optimizer = get_dawg_optimizer()

            if not optimizer.is_available():
                logger.error("‚ùå DAWG library not available. Install with: pip install dawg-python")
                return False

            # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ —Å–ª–æ–≤–∞—Ä—è
            analyzer = MAWOMorphAnalyzer()

            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º —Å–ª–æ–≤–∞—Ä—å –≤ DAWG
            dawg_dict = optimizer.convert_dict_to_dawg(analyzer.dictionary)

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º DAWG –∫—ç—à
            optimizer.save_dawg_cache(dawg_dict, self.dawg_cache_path)

            logger.info("‚úÖ DAWG cache built successfully!")
            return True

        except Exception as e:
            logger.exception(f"‚ùå Failed to build DAWG cache: {e}")
            return False

    def get_cache_info(self) -> dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –∫—ç—à–µ.

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –∫—ç—à–µ
        """
        info: dict[str, Any] = {
            "dict_path": str(self.dict_path),
            "dawg_available": self.is_dawg_cache_available(),
        }

        if self.is_dawg_cache_available():
            info["dawg_size_mb"] = self.dawg_cache_path.stat().st_size / (1024 * 1024)

        return info


# –≠–∫—Å–ø–æ—Ä—Ç –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å pymorphy3
MorphAnalyzer = MAWOMorphAnalyzer

# –û—Å–Ω–æ–≤–Ω—ã–µ —ç–∫—Å–ø–æ—Ä—Ç—ã
__all__ = [
    "MAWOMorphAnalyzer",
    "MAWOOptimizedMorphAnalyzer",
    "MAWOParse",
    "MAWOTag",
    "MorphAnalyzer",
    "create_analyzer",
    "get_global_analyzer",
    "MAWODictionaryManager",
]
