name: Claude CLI QA Tests

on:
  pull_request:
    types: [opened, synchronize, reopened, ready_for_review]
    branches: [main]
    paths:
      - 'llmsbrieftxt/**'
      - 'tests/**'
      - 'pyproject.toml'
      - 'uv.lock'
      - '.github/workflows/claude-cli-qa.yml'
  workflow_dispatch:
    inputs:
      branch:
        description: 'Branch to test (leave empty for current branch)'
        required: false
        type: string
      test_scope:
        description: 'Test scope (quick, standard, thorough)'
        required: false
        default: 'standard'
        type: choice
        options:
          - quick
          - standard
          - thorough

jobs:
  cli-qa-tests:
    name: Agent-Based CLI QA
    runs-on: ubuntu-latest
    timeout-minutes: 20
    permissions:
      contents: read
      pull-requests: write
      issues: read
      id-token: write
    env:
      OLLAMA_MODEL: gemma3:270m

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          ref: ${{ github.event.inputs.branch || github.head_ref || github.ref }}
          fetch-depth: 1

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install uv
        run: |
          curl -LsSf https://astral.sh/uv/install.sh | sh
          echo "$HOME/.cargo/bin" >> $GITHUB_PATH

      - name: Install package with dependencies
        run: |
          uv sync --all-groups

      - name: Install Ollama
        run: |
          echo "Installing Ollama for local LLM testing..."
          curl -fsSL https://ollama.com/install.sh | sh

          # Verify installation
          ollama --version

      - name: Start Ollama service
        run: |
          echo "Starting Ollama service..."
          ollama serve > ollama.log 2>&1 &
          echo $! > ollama.pid

          # Wait for Ollama to be ready
          echo "Waiting for Ollama to be ready..."
          timeout 60 bash -c 'until curl -s http://localhost:11434/api/tags > /dev/null; do sleep 2; done'

          echo "‚úì Ollama service is running"

      - name: Pull Ollama model
        run: |
          echo "Pulling ${{ env.OLLAMA_MODEL }} model for testing..."
          ollama pull ${{ env.OLLAMA_MODEL }}

          echo "‚úì Model pulled successfully"

      - name: Verify Ollama setup
        run: |
          echo "Verifying Ollama models..."
          ollama list

          echo "‚úì Model ready"

      - name: Verify CLI installation
        run: |
          # Ensure CLI is accessible
          uv run llmtxt --help

          echo "‚úì CLI installed and accessible"

      - name: Determine test scope
        id: test-scope
        run: |
          SCOPE="${{ github.event.inputs.test_scope || 'standard' }}"
          echo "scope=$SCOPE" >> $GITHUB_OUTPUT

          case "$SCOPE" in
            quick)
              echo "max_urls=2" >> $GITHUB_OUTPUT
              echo "depth=1" >> $GITHUB_OUTPUT
              ;;
            thorough)
              echo "max_urls=5" >> $GITHUB_OUTPUT
              echo "depth=1" >> $GITHUB_OUTPUT
              ;;
            *)
              echo "max_urls=3" >> $GITHUB_OUTPUT
              echo "depth=1" >> $GITHUB_OUTPUT
              ;;
          esac

      - name: Run Claude CLI QA Agent
        id: claude-cli-qa
        uses: anthropics/claude-code-action@v1
        with:
          claude_code_oauth_token: ${{ secrets.CLAUDE_CODE_OAUTH_TOKEN }}

          # Allow Claude bot to trigger this workflow
          allowed_bots: '*'

          # Use Haiku model for faster/cheaper testing
          claude_args: |
            --model claude-haiku-4-5-20251001
            --allowedTools "Bash,Read,Write,Glob,Grep,TodoWrite,Task"

          # Enable progress tracking for PR events only
          track_progress: ${{ github.event_name == 'pull_request' }}

          # Show full output in workflow logs for debugging
          show_full_output: true

          prompt: |
            You are the QA coordinator orchestrating comprehensive CLI tests for llmsbrieftxt.

            **Test Environment (Ready)**:
            - Python 3.11 with uv package manager
            - Ollama service running at http://localhost:11434
            - Model: ${{ env.OLLAMA_MODEL }} (fast, lightweight)
            - CLI: Use `uv run llmtxt` to invoke the CLI

            **Environment Variables for Ollama**:
            ```bash
            export OPENAI_BASE_URL="http://localhost:11434/v1"
            export OPENAI_API_KEY="ollama-dummy-key"
            ```

            **Test Scope**: ${{ steps.test-scope.outputs.scope }}
            - quick: TS-001, TS-002, TS-007, TS-011 (critical tests)
            - standard: quick + TS-003, TS-004A, TS-005, TS-006 (critical + high priority)
            - thorough: all scenarios TS-001 through TS-012

            **Workflow**:
            1. Read docs/USER_JOURNEYS.md to understand test scenarios
            2. **IMPORTANT - Launch ALL sub-agents IN PARALLEL** using the Task tool. Do not wait for one to complete before launching the next. Create all Task tool calls in a single message.
            3. Each sub-agent executes ONE test scenario in isolation and reports pass/fail with evidence
            4. Collect results from all sub-agents (they will complete asynchronously)
            5. Create aggregated summary with:
               - Total tests: count
               - Passed: count + list
               - Failed: count + list
               - Critical tests status
               - High priority tests status

            **Sub-Agent Assignment**:
            Assign test scenarios based on scope:
            - TS-001: Fresh Install (ALWAYS include)
            - TS-002: Preview Mode (ALWAYS include)
            - TS-003: Depth/Max-URLs (standard+ scope)
            - TS-004A: Cache-Only (standard+ scope)
            - TS-004B: Force Refresh (thorough scope only)
            - TS-005: Custom Paths (standard+ scope)
            - TS-006: Output Format (standard+ scope)
            - TS-007: Partial Failures (ALWAYS include)
            - TS-008: Alternative Model (thorough scope only)
            - TS-009: Depth Validation (thorough scope only)
            - TS-010: Cache Reuse (thorough scope only)
            - TS-011: Ollama Not Running (ALWAYS include)
            - TS-012: Invalid Model (thorough scope only)

            For each test scenario, launch a sub-agent with clear test task:
            ```
            Use the Task tool to launch a specialized QA sub-agent:

            Prompt template (customize for each test):
            "Execute test scenario [TEST-ID] from docs/USER_JOURNEYS.md.

            Test: [Test Name]
            Priority: [Priority Level]

            **IMPORTANT - Directory Isolation**:
            Since multiple tests run in parallel, use unique directories to avoid conflicts:
            - Cache directory: /tmp/llmtxt-cache-[TEST-ID] (e.g., /tmp/llmtxt-cache-ts001)
            - Output directory: /tmp/llmtxt-output-[TEST-ID] (e.g., /tmp/llmtxt-output-ts001)
            - Use --cache-dir and --output flags in all CLI commands to specify these paths
            - This prevents parallel tests from interfering with each other

            Read the detailed test steps and expected results from docs/USER_JOURNEYS.md.
            Adapt the commands to use the isolated directories listed above.
            Execute all verification commands.
            Report:
            - Test Status: PASS or FAIL
            - Evidence: Which specific checks passed/failed
            - Details: Any observations or issues

            Format output as:
            TEST-ID: [PASS/FAIL]
            Evidence: [Bullet points of verification results]"
            ```

            **Aggregation Task**:
            After all sub-agents complete, create final summary:
            - Count total tests by priority
            - Report pass/fail percentages
            - Highlight any critical failures
            - Provide clear GO/NO-GO recommendation

      - name: Validate test execution
        id: validate-tests
        if: always()
        run: |
          echo "=== Validating CLI QA Test Results ==="

          # Check if CLI is accessible via uv run
          if ! uv run llmtxt --help &> /dev/null; then
            echo "‚ùå ERROR: llmtxt CLI not accessible via 'uv run'"
            exit 1
          fi

          echo "‚úÖ CLI validation passed (accessible via 'uv run llmtxt')"

      - name: Evaluate test results
        id: evaluate-results
        if: always()
        run: |
          echo "=== Evaluating Test Results ==="

          # Check Claude agent execution status
          if [ "${{ steps.claude-cli-qa.outcome }}" = "failure" ]; then
            echo "‚ùå Claude QA agent failed to complete"
            echo "status=failure" >> $GITHUB_OUTPUT
            exit 1
          fi

          # Parse output for test failures (these patterns match the prompt output)
          CLAUDE_OUTPUT="${{ steps.claude-cli-qa.outputs.text }}"

          # Check for critical failures in output
          if echo "$CLAUDE_OUTPUT" | grep -q "NO-GO\|‚ùå FAIL\|CRITICAL"; then
            echo "‚ùå Test failures detected in QA results"
            echo "status=failure" >> $GITHUB_OUTPUT
            exit 1
          fi

          # Check for test summary with failures
          if echo "$CLAUDE_OUTPUT" | grep -q "FAIL.*\|Failed:.*[1-9]"; then
            echo "‚ö†Ô∏è  Some tests failed but not critical - checking severity"
            if echo "$CLAUDE_OUTPUT" | grep -q "Exit Code Bug\|returns exit code 0\|CRITICAL"; then
              echo "‚ùå Critical test failures detected"
              echo "status=failure" >> $GITHUB_OUTPUT
              exit 1
            fi
          fi

          echo "‚úÖ All critical tests passed"
          echo "status=success" >> $GITHUB_OUTPUT

      - name: Generate test summary
        if: always()
        env:
          CLAUDE_STATUS: ${{ steps.claude-cli-qa.outcome }}
          EVALUATION_STATUS: ${{ steps.evaluate-results.outputs.status }}
          GITHUB_SERVER_URL: ${{ github.server_url }}
          GITHUB_REPOSITORY: ${{ github.repository }}
          GITHUB_RUN_ID: ${{ github.run_id }}
        run: |
          cat <<'EOF' >> $GITHUB_STEP_SUMMARY
          ## llmtxt CLI QA Test Summary

          **Test Scope**: ${{ steps.test-scope.outputs.scope }}
          **Status**: ${{ steps.evaluate-results.outputs.status == 'success' && '‚úÖ All Tests Passed' || '‚ùå Tests Failed' }}

          ### Environment
          - Python: 3.11
          - Package Manager: uv
          - LLM Provider: Ollama (${{ env.OLLAMA_MODEL }})
          - Max URLs: ${{ steps.test-scope.outputs.max_urls }}
          - Crawl Depth: ${{ steps.test-scope.outputs.depth }}

          ### Test Categories
          - Basic Functionality Tests
          - CLI Flags and Options Tests
          - Cache Behavior Tests
          - Error Handling Tests
          - Output Format Validation

          ### Results
          The CLI QA agent executed comprehensive tests using the `cli-qa-tester` agent.
          Detailed test report and findings are available in the workflow logs.

          **View full logs**: [${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}](${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }})

          ---

          ü§ñ **Agent-Based Testing**: This workflow uses Claude Code agents to perform intelligent,
          adaptive CLI testing without traditional E2E test frameworks.
          EOF

      - name: Upload test artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: cli-qa-artifacts
          path: |
            ~/.claude/docs/
            .llmsbrieftxt_cache/
            ollama.log
            **/test-*.txt
            **/test-*.log
          if-no-files-found: warn
          retention-days: 7

      - name: Fail workflow if tests failed
        if: always() && steps.evaluate-results.outputs.status == 'failure'
        run: |
          echo "‚ùå CLI QA tests have critical failures - blocking PR"
          exit 1

      - name: Cleanup
        if: always()
        run: |
          if [ -f ollama.pid ]; then
            kill $(cat ollama.pid) || true
          fi
          echo "‚úì Cleanup complete"
