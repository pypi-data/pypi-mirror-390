"""Multimodal RAG with directory scanning and ChromaDB integration.

Extends MemoryRAG to support automatic indexing of multimodal content
(images, audio, video, PDFs) from directories.

Features (v4.1.0+):
- Semantic chunking for long PDF/text extractions
- Automatic content processing via Gemini
"""

import logging
from pathlib import Path
from typing import TYPE_CHECKING, Any, Optional

from kagura.core.memory.rag import MemoryRAG
from kagura.loaders.cache import LoaderCache
from kagura.loaders.directory import DirectoryScanner
from kagura.loaders.file_types import FileType

if TYPE_CHECKING:
    from kagura.config.memory_config import ChunkingConfig, EmbeddingConfig

# Try to import GeminiLoader
try:
    from kagura.loaders.gemini import GEMINI_AVAILABLE, GeminiLoader

    MULTIMODAL_AVAILABLE = GEMINI_AVAILABLE
except ImportError:
    MULTIMODAL_AVAILABLE = False
    if not TYPE_CHECKING:
        GeminiLoader = None  # type: ignore

logger = logging.getLogger(__name__)


class MultimodalRAG(MemoryRAG):
    """Multimodal RAG with directory scanning.

    Extends MemoryRAG to automatically scan directories, process multimodal
    content (images, audio, video, PDFs) using Gemini, and index them in
    ChromaDB for semantic search.

    Example:
        >>> # Initialize with directory
        >>> rag = MultimodalRAG(
        ...     directory=Path("./project"),
        ...     collection_name="project_docs"
        ... )
        >>> # Build index from directory content
        >>> await rag.build_index()
        >>> # Query across all content types
        >>> results = rag.query("How does authentication work?")
        >>> print(results[0]["content"])
    """

    def __init__(
        self,
        directory: Path,
        collection_name: str = "multimodal_memory",
        persist_dir: Optional[Path] = None,
        gemini_api_key: Optional[str] = None,
        enable_cache: bool = True,
        cache_size_mb: int = 100,
        respect_gitignore: bool = True,
        chunking_config: Optional["ChunkingConfig"] = None,
        embedding_config: Optional["EmbeddingConfig"] = None,
    ):
        """Initialize MultimodalRAG with semantic chunking and E5 embeddings support.

        Args:
            directory: Directory to scan for content
            collection_name: ChromaDB collection name
            persist_dir: Directory for ChromaDB storage
            gemini_api_key: Gemini API key (optional, uses env var if None)
            enable_cache: Enable file content caching
            cache_size_mb: Cache size limit in megabytes
            respect_gitignore: Respect .gitignore/.kaguraignore patterns
            chunking_config: Semantic chunking configuration (v4.1.0+)
            embedding_config: Embedding configuration (v4.2.0+, Quick Win #1)

        Raises:
            ImportError: If Gemini or ChromaDB not available
            FileNotFoundError: If directory doesn't exist
        """
        # Initialize parent RAG (with chunking and E5 embeddings support)
        super().__init__(
            collection_name=collection_name,
            persist_dir=persist_dir,
            chunking_config=chunking_config,
            embedding_config=embedding_config,
        )

        if not MULTIMODAL_AVAILABLE:
            raise ImportError(
                "Multimodal support requires google-generativeai. "
                "Install with: pip install kagura-ai[multimodal]"
            )

        self.directory = Path(directory)
        if not self.directory.exists():
            raise FileNotFoundError(f"Directory not found: {self.directory}")

        # Initialize Gemini loader
        self.gemini = GeminiLoader(api_key=gemini_api_key)

        # Initialize directory scanner
        self.scanner = DirectoryScanner(
            directory=self.directory,
            gemini=self.gemini,
            respect_gitignore=respect_gitignore,
        )

        # Initialize cache
        self.cache: Optional[LoaderCache] = None
        if enable_cache:
            self.cache = LoaderCache(max_size_mb=cache_size_mb)

        self._indexed_files: set[Path] = set()

    async def build_index(
        self,
        max_concurrent: int = 3,
        language: str = "en",
        force_rebuild: bool = False,
    ) -> dict[str, Any]:
        """Build vector index from directory content.

        Scans directory, processes all supported files (text + multimodal),
        and adds them to ChromaDB for semantic search.

        Args:
            max_concurrent: Maximum concurrent file processing
            language: Language for Gemini responses (en/ja)
            force_rebuild: Force rebuild even if files already indexed

        Returns:
            Statistics dictionary with:
            - total_files: Total files processed
            - text_files: Number of text files
            - multimodal_files: Number of multimodal files
            - failed_files: Number of failed files
            - cache_hit_rate: Cache hit rate (if enabled)

        Example:
            >>> stats = await rag.build_index(max_concurrent=5)
            >>> print(f"Indexed {stats['total_files']} files")
        """
        logger.info(f"Building index from {self.directory}")

        # Clear existing index if force rebuild
        if force_rebuild:
            logger.info("Force rebuild: clearing existing index")
            self.delete_all()
            self._indexed_files.clear()

        # Scan and load all files
        contents = await self.scanner.load_all(
            max_concurrent=max_concurrent, language=language
        )

        # Statistics
        stats = {
            "total_files": 0,
            "text_files": 0,
            "multimodal_files": 0,
            "failed_files": 0,
            "cache_hit_rate": 0.0,
        }

        # Index each file
        for content in contents:
            # Skip if already indexed (unless force rebuild)
            if not force_rebuild and content.path in self._indexed_files:
                continue

            try:
                # Create metadata
                metadata = {
                    "file_path": str(content.path),
                    "file_type": content.file_type.value,
                    "file_size": content.size,
                }

                # Store in ChromaDB
                # Note: user_id should be provided by caller context
                # For now, use "default_user" (will be fixed in caller methods)
                self.store(
                    content=content.content,
                    user_id="default_user",  # TODO: Pass from indexing context
                    metadata=metadata,
                )

                # Mark as indexed
                self._indexed_files.add(content.path)

                # Update statistics
                stats["total_files"] += 1
                if content.file_type == FileType.TEXT:
                    stats["text_files"] += 1
                else:
                    stats["multimodal_files"] += 1

                # Cache the content
                if self.cache:
                    self.cache.put(content.path, content)

            except Exception as e:
                logger.error(f"Failed to index {content.path}: {e}")
                stats["failed_files"] += 1

        # Add cache statistics
        if self.cache:
            stats["cache_hit_rate"] = self.cache.stats().hit_rate

        logger.info(
            f"Index built: {stats['total_files']} files "
            f"({stats['text_files']} text, {stats['multimodal_files']} multimodal)"
        )

        return stats

    async def incremental_update(
        self, max_concurrent: int = 3, language: str = "en"
    ) -> dict[str, Any]:
        """Incrementally update index with new/modified files.

        Only processes files that haven't been indexed or have been modified
        since last indexing.

        Args:
            max_concurrent: Maximum concurrent file processing
            language: Language for Gemini responses

        Returns:
            Statistics dictionary (same as build_index)

        Example:
            >>> stats = await rag.incremental_update()
            >>> print(f"Updated {stats['total_files']} files")
        """
        logger.info("Performing incremental update")

        # Scan directory
        file_infos = await self.scanner.scan()

        # Find new/modified files
        files_to_process = []
        for file_info in file_infos:
            # Skip if already indexed and not modified
            if file_info.path in self._indexed_files:
                # Check cache validity (will detect modifications)
                if self.cache and self.cache.get(file_info.path):
                    continue  # File cached and not modified
            files_to_process.append(file_info.path)

        if not files_to_process:
            logger.info("No files to update")
            return {
                "total_files": 0,
                "text_files": 0,
                "multimodal_files": 0,
                "failed_files": 0,
            }

        # Load only new/modified files
        logger.info(f"Processing {len(files_to_process)} new/modified files")
        # TODO (v3.1): Implement selective loading in DirectoryScanner
        # This would optimize incremental updates by only processing changed files
        # instead of rebuilding the entire index. Requires tracking file metadata
        # (modification times, checksums) for delta detection.
        # For now, rebuild entire index
        return await self.build_index(
            max_concurrent=max_concurrent, language=language, force_rebuild=False
        )

    def query(
        self,
        query_text: str,
        user_id: str = "default_user",
        n_results: int = 5,
        file_type: Optional[FileType] = None,
    ) -> list[dict[str, Any]]:
        """Semantic search across multimodal content.

        Args:
            query_text: Search query
            user_id: User identifier (defaults to "default_user" for backward compat)
            n_results: Number of results to return
            file_type: Optional file type filter

        Returns:
            List of result dictionaries with content, distance, and metadata

        Example:
            >>> # Search all content
            >>> results = rag.query("authentication flow", user_id="jfk")
            >>> # Search only images
            >>> results = rag.query("diagram", user_id="jfk", file_type=FileType.IMAGE)
        """
        # Use parent recall method
        results = self.recall(query=query_text, user_id=user_id, top_k=n_results)

        # Filter by file type if specified
        if file_type:
            results = [
                r
                for r in results
                if r.get("metadata", {}).get("file_type") == file_type.value
            ]

        return results

    def get_indexed_files(self) -> list[Path]:
        """Get list of indexed file paths.

        Returns:
            List of indexed file paths
        """
        return list(self._indexed_files)

    def clear_cache(self) -> None:
        """Clear file content cache."""
        if self.cache:
            self.cache.clear()
            logger.info("Cache cleared")

    def __repr__(self) -> str:
        """String representation."""
        return (
            f"MultimodalRAG(directory={self.directory}, "
            f"indexed_files={len(self._indexed_files)}, "
            f"collection={self.collection.name})"
        )


__all__ = ["MultimodalRAG"]
