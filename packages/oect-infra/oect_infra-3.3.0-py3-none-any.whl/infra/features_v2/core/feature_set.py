"""
FeatureSet - ç‰¹å¾é›†åˆç®¡ç†å™¨ï¼ˆç”¨æˆ·ä¸»æ¥å£ï¼‰

æä¾›å£°æ˜å¼ API ç”¨äºå®šä¹‰å’Œè®¡ç®—ç‰¹å¾
"""

from typing import Any, Callable, Dict, List, Optional, Tuple, Union
import numpy as np
import pandas as pd
from pathlib import Path

from infra.features_v2.core.compute_graph import ComputeGraph, ComputeNode
from infra.features_v2.core.executor import Executor, ExecutionContext
from infra.features_v2.extractors.base import (
    BaseExtractor,
    LambdaExtractor,
    get_extractor,
    EXTRACTOR_REGISTRY,
)
from infra.logger_config import get_module_logger

logger = get_module_logger()


class FeatureSet:
    """ç‰¹å¾é›†åˆç®¡ç†å™¨

    ç”¨æ³•ï¼š
        # åˆ›å»ºç‰¹å¾é›†åˆ
        features = FeatureSet(experiment=exp)

        # æ·»åŠ ç‰¹å¾ï¼ˆå¤šç§æ–¹å¼ï¼‰
        features.add('gm_max', extractor='transfer.gm_max')
        features.add('custom', func=lambda x: x.mean(), input='transfer')

        # è®¡ç®—
        result = features.compute()

        # å¯¼å‡º
        features.to_dataframe()
        features.to_parquet('output.parquet')
    """

    def __init__(
        self,
        experiment=None,
        unified_experiment=None,
        config_name=None,
        config_version='1.0'
    ):
        """
        Args:
            experiment: Experiment å®ä¾‹ï¼ˆå¯é€‰ï¼Œç¨åå¯é€šè¿‡ set_experiment è®¾ç½®ï¼‰
            unified_experiment: UnifiedExperiment å¯¹è±¡ï¼ˆä¼˜å…ˆä½¿ç”¨ï¼‰
            config_name: é…ç½®åç§°ï¼ˆç”¨äºç¼“å­˜æŸ¥æ‰¾ï¼‰
            config_version: é…ç½®å†…å®¹ç‰ˆæœ¬å·
        """
        self.unified_experiment = unified_experiment
        self.config_name = config_name
        self.config_version = config_version

        # è‡ªåŠ¨æå–åº•å±‚ experiment
        if unified_experiment and not experiment:
            experiment = unified_experiment._get_experiment()

        self.experiment = experiment
        self.graph = ComputeGraph()
        self.data_loaders = {}
        self._computed_results: Optional[ExecutionContext] = None

        # å¦‚æœæä¾›äº† experimentï¼Œè‡ªåŠ¨æ³¨å†Œæ•°æ®åŠ è½½å™¨
        if experiment:
            self._setup_data_loaders()

    def set_experiment(self, experiment):
        """è®¾ç½®å®éªŒå¯¹è±¡å¹¶æ³¨å†Œæ•°æ®åŠ è½½å™¨"""
        self.experiment = experiment
        self._setup_data_loaders()

    def _setup_data_loaders(self):
        """æ³¨å†Œæ•°æ®åŠ è½½å™¨"""
        if not self.experiment:
            return

        # Transfer æ•°æ®åŠ è½½å™¨
        def load_transfer():
            logger.debug("åŠ è½½ Transfer æ•°æ®...")
            transfer_data = self.experiment.get_transfer_all_measurement()
            # è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼ï¼ˆæ¯ä¸ª step ä¸€ä¸ªå­—å…¸ï¼‰
            n_steps = transfer_data['measurement_data'].shape[0]
            result = []
            for i in range(n_steps):
                step_data = {
                    'Vg': transfer_data['measurement_data'][i, 0, :],
                    'Id': transfer_data['measurement_data'][i, 1, :],
                }
                # è¿‡æ»¤ NaN
                valid_mask = ~np.isnan(step_data['Vg'])
                result.append({
                    'Vg': step_data['Vg'][valid_mask],
                    'Id': step_data['Id'][valid_mask],
                })
            return result

        # Transient æ•°æ®åŠ è½½å™¨
        def load_transient():
            logger.debug("åŠ è½½ Transient æ•°æ®...")

            # è·å–æ­¥éª¤ä¿¡æ¯è¡¨
            step_info = self.experiment.get_transient_step_info_table()
            if step_info is None:
                raise ValueError("æ— æ³•è·å– transient step_info_table")

            # è·å–æ‹¼æ¥åçš„æµ‹é‡æ•°æ®
            transient_data = self.experiment.get_transient_all_measurement()
            if transient_data is None:
                raise ValueError("æ— æ³•è·å– transient measurement data")

            # å°†æ•°æ®å­—å…¸è½¬æ¢ä¸º 2D æ•°ç»„æ ¼å¼ (3, total_points)
            measurement = np.array([
                transient_data['continuous_time'],
                transient_data['original_time'],
                transient_data['drain_current']
            ])

            # æ£€æŸ¥æ˜¯å¦æœ‰ç´¢å¼•ä¿¡æ¯ï¼ˆæ–°ç‰ˆ HDF5 æ–‡ä»¶ï¼‰
            if 'start_data_index' in step_info.columns and 'end_data_index' in step_info.columns:
                # ä½¿ç”¨ç´¢å¼•å¿«é€Ÿåˆ‡ç‰‡ï¼ˆæ¨èæ–¹å¼ï¼Œé«˜æ•ˆï¼‰
                logger.debug("ä½¿ç”¨ step_info_table ç´¢å¼•åˆ‡ç‰‡æ•°æ®")
                result = []
                for _, row in step_info.iterrows():
                    start_idx = int(row['start_data_index'])
                    end_idx = int(row['end_data_index'])

                    step_data = {
                        'continuous_time': measurement[0, start_idx:end_idx],
                        'original_time': measurement[1, start_idx:end_idx],
                        'drain_current': measurement[2, start_idx:end_idx],
                    }
                    result.append(step_data)
            else:
                # æ—§ç‰ˆ HDF5 æ–‡ä»¶æ²¡æœ‰ç´¢å¼•ï¼Œå›é€€åˆ°é€æ­¥åŠ è½½
                logger.warning(
                    "step_info_table ç¼ºå°‘ç´¢å¼•å­—æ®µï¼Œå›é€€åˆ°é€æ­¥åŠ è½½ï¼ˆè¾ƒæ…¢ï¼‰ã€‚"
                    "å»ºè®®é‡æ–°ç”Ÿæˆ HDF5 æ–‡ä»¶ä»¥è·å¾—æ›´å¥½æ€§èƒ½ã€‚"
                )
                n_steps = len(step_info)
                result = []
                for step_idx in range(n_steps):
                    step_data = self.experiment.get_transient_step_measurement(step_idx)
                    if step_data is None:
                        logger.warning(f"æ— æ³•åŠ è½½ transient step {step_idx}ï¼Œè·³è¿‡")
                        continue

                    # ç¡®ä¿æ•°æ®æ ¼å¼æ­£ç¡®
                    if not all(k in step_data for k in ['continuous_time', 'drain_current']):
                        logger.warning(f"Step {step_idx} æ•°æ®ä¸å®Œæ•´ï¼Œè·³è¿‡")
                        continue

                    result.append(step_data)

            if len(result) == 0:
                raise ValueError("æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½• transient æ­¥éª¤æ•°æ®")

            logger.debug(f"æˆåŠŸåŠ è½½ {len(result)} ä¸ª transient æ­¥éª¤")
            return result

        self.data_loaders['transfer'] = load_transfer
        self.data_loaders['transient'] = load_transient

    def add(
        self,
        name: str,
        extractor: Optional[str] = None,
        func: Optional[Callable] = None,
        input: Union[str, List[str]] = None,
        params: Optional[Dict[str, Any]] = None,
        output_shape: Optional[Tuple] = None,
    ):
        """æ·»åŠ ç‰¹å¾

        æ”¯æŒå¤šç§ä½¿ç”¨æ–¹å¼ï¼š
        1. ä½¿ç”¨æ³¨å†Œçš„æå–å™¨ï¼š
           features.add('gm_max', extractor='transfer.gm_max', input='transfer')

        2. ä½¿ç”¨è‡ªå®šä¹‰å‡½æ•°ï¼š
           features.add('mean_id', func=lambda x: np.mean([s['Id'] for s in x]),
                       input='transfer')

        3. ä½¿ç”¨ lambdaï¼ˆä¾èµ–å…¶ä»–ç‰¹å¾ï¼‰ï¼š
           features.add('gm_norm', func=lambda gm: (gm - gm.mean()) / gm.std(),
                       input='gm_max')

        Args:
            name: ç‰¹å¾åç§°
            extractor: æ³¨å†Œçš„æå–å™¨åç§°ï¼ˆå¦‚ 'transfer.gm_max'ï¼‰
            func: è‡ªå®šä¹‰å‡½æ•°ï¼ˆä¸ extractor äºŒé€‰ä¸€ï¼‰
            input: è¾“å…¥ä¾èµ–ï¼ˆæ•°æ®æºæˆ–å…¶ä»–ç‰¹å¾åï¼‰
            params: å‚æ•°å­—å…¸
            output_shape: è¾“å‡ºå½¢çŠ¶ï¼ˆä½¿ç”¨ func æ—¶å¿…é¡»æä¾›ï¼‰

        Raises:
            ValueError: å¦‚æœå‚æ•°ä¸åˆæ³•
        """
        if extractor is None and func is None:
            raise ValueError("å¿…é¡»æä¾› extractor æˆ– func ä¹‹ä¸€")

        if extractor and func:
            raise ValueError("extractor å’Œ func ä¸èƒ½åŒæ—¶æä¾›")

        # è§„èŒƒåŒ– input
        if input is None:
            inputs = []
        elif isinstance(input, str):
            inputs = [input]
        else:
            inputs = input

        params = params or {}

        # åˆ›å»ºè®¡ç®—èŠ‚ç‚¹
        if extractor:
            # ä½¿ç”¨æ³¨å†Œçš„æå–å™¨
            node = ComputeNode(
                name=name,
                func=extractor,
                inputs=inputs,
                params=params,
                is_extractor=True,
            )

        else:
            # ä½¿ç”¨è‡ªå®šä¹‰å‡½æ•°
            if output_shape is None:
                # å°è¯•æ¨æ–­ï¼ˆå‡è®¾ä¸ºæ ‡é‡ï¼‰
                output_shape = ('n_steps',)
                logger.warning(
                    f"ç‰¹å¾ '{name}' æœªæŒ‡å®š output_shapeï¼Œå‡è®¾ä¸º {output_shape}"
                )

            # ğŸ”‘ å°è¯•æå– lambda æºä»£ç ï¼ˆç”¨äºåºåˆ—åŒ–ï¼‰
            source_code = None
            if callable(func):
                source_code = self._extract_lambda_source(func)

            node = ComputeNode(
                name=name,
                func=func,
                inputs=inputs,
                params=params,
                output_shape=output_shape,
                is_extractor=False,
                source_code=source_code,  # ğŸ”‘ ä¿å­˜æºä»£ç 
            )

        # æ·»åŠ åˆ°è®¡ç®—å›¾
        self.graph.add_node(node)
        logger.debug(f"æ·»åŠ ç‰¹å¾: {name} (è¾“å…¥: {inputs})")

        return self

    def _extract_lambda_source(self, func: Callable) -> Optional[str]:
        """æå– lambda å‡½æ•°çš„æºä»£ç 

        Args:
            func: Lambda å‡½æ•°å¯¹è±¡

        Returns:
            Lambda æºä»£ç å­—ç¬¦ä¸²ï¼Œå¤±è´¥è¿”å› None
        """
        try:
            import inspect

            # åªå¤„ç† lambda å‡½æ•°
            if '<lambda>' not in func.__name__:
                return None

            # è·å–æºä»£ç 
            source = inspect.getsource(func).strip()

            # æŸ¥æ‰¾ lambda å…³é”®å­—
            if 'lambda' not in source:
                return None

            start_idx = source.find('lambda')
            remaining = source[start_idx:]

            # ä½¿ç”¨æ‹¬å·å¹³è¡¡ç®—æ³•æ‰¾åˆ° lambda è¡¨è¾¾å¼çš„ç»“å°¾
            paren_count = 0
            bracket_count = 0
            in_string = False
            quote_char = None
            colon_found = False
            end_idx = len(remaining)

            for i, char in enumerate(remaining):
                # å¤„ç†å­—ç¬¦ä¸²ï¼ˆè·³è¿‡å­—ç¬¦ä¸²å†…çš„æ‹¬å·ï¼‰
                if char in ('"', "'") and (i == 0 or remaining[i-1] != '\\'):
                    if not in_string:
                        in_string = True
                        quote_char = char
                    elif char == quote_char:
                        in_string = False

                if not in_string:
                    # ç»Ÿè®¡æ‹¬å·
                    if char == '(':
                        paren_count += 1
                    elif char == ')':
                        paren_count -= 1
                    elif char == '[':
                        bracket_count += 1
                    elif char == ']':
                        bracket_count -= 1

                    # æ ‡è®°å†’å·ä½ç½®ï¼ˆå‚æ•°åˆ—è¡¨ç»“æŸï¼Œè¡¨è¾¾å¼å¼€å§‹ï¼‰
                    if char == ':' and paren_count == 0 and bracket_count == 0:
                        colon_found = True

                    # æ£€æŸ¥æ˜¯å¦åˆ°è¾¾ lambda è¡¨è¾¾å¼ç»“å°¾
                    # åªæœ‰åœ¨å†’å·ä¹‹åï¼Œé‡åˆ°é€—å·æˆ–æ¢è¡Œæ—¶æ‰ç»“æŸ
                    if colon_found and char in (',', '\n') and paren_count == 0 and bracket_count == 0:
                        end_idx = i
                        break

            lambda_expr = remaining[:end_idx].strip()

            # éªŒè¯æå–çš„ lambda è¯­æ³•
            try:
                compile(lambda_expr, '<lambda>', 'eval')
                return lambda_expr
            except SyntaxError:
                return None

        except Exception:
            # æå–å¤±è´¥ä¸å½±å“åŠŸèƒ½
            return None

    def compute(self) -> Dict[str, np.ndarray]:
        """æ‰§è¡Œè®¡ç®—å›¾ï¼ˆæ”¯æŒå¢é‡è®¡ç®—ï¼‰

        ä¼˜å…ˆä» Parquet ç¼“å­˜åŠ è½½å·²æœ‰ç‰¹å¾ï¼Œåªè®¡ç®—ç¼ºå¤±çš„ç‰¹å¾ã€‚

        Returns:
            ç‰¹å¾å­—å…¸ {feature_name: ndarray}
        """
        if not self.experiment and self.graph.get_data_sources():
            raise RuntimeError("æœªè®¾ç½® experimentï¼Œæ— æ³•åŠ è½½æ•°æ®")

        # 1ï¸âƒ£ å°è¯•åŠ è½½ç¼“å­˜
        cached_features = {}
        if self.unified_experiment and self.config_name:
            try:
                cached_df = self.unified_experiment.get_v2_feature_dataframe(self.config_name)

                if cached_df is not None:
                    # éªŒè¯ç¼“å­˜æœ‰æ•ˆæ€§
                    if self._validate_cache(cached_df):
                        logger.info(f"âœ“ å‘ç°æœ‰æ•ˆç¼“å­˜ï¼ˆé…ç½®: {self.config_name}ï¼‰")

                        # æå–æ‰€æœ‰ç¼“å­˜ç‰¹å¾
                        for col in cached_df.columns:
                            if col != 'step_index' and col in self.graph.nodes:
                                cached_features[col] = cached_df[col].to_numpy()
                                logger.info(f"  âœ“ ä»ç¼“å­˜åŠ è½½: {col}")
                    else:
                        logger.warning("âš  ç¼“å­˜å¤±æ•ˆï¼ˆæºæ–‡ä»¶å·²æ”¹å˜ï¼‰ï¼Œé‡æ–°è®¡ç®—")
            except Exception as e:
                logger.warning(f"åŠ è½½ç¼“å­˜å¤±è´¥: {e}ï¼Œå°†é‡æ–°è®¡ç®—")

        # 2ï¸âƒ£ æ£€æŸ¥æ˜¯å¦å…¨éƒ¨å‘½ä¸­ç¼“å­˜
        all_features = set(self.graph.nodes.keys())
        cached_feature_names = set(cached_features.keys())
        missing_features = all_features - cached_feature_names

        if not missing_features:
            # å…¨éƒ¨å‘½ä¸­ç¼“å­˜
            logger.info(f"âœ“ å…¨éƒ¨ {len(cached_features)} ä¸ªç‰¹å¾ä»ç¼“å­˜åŠ è½½ï¼Œæ— éœ€è®¡ç®—")

            # å¡«å…… ExecutionContext
            self._computed_results = ExecutionContext()
            for name, value in cached_features.items():
                self._computed_results.set(name, value, 0)
            self._computed_results.cache_hits = len(cached_features)

            return cached_features

        # 3ï¸âƒ£ éƒ¨åˆ†å‘½ä¸­ï¼šå¢é‡è®¡ç®—
        logger.info(
            f"âš™ï¸ å¢é‡è®¡ç®—ï¼š{len(cached_features)} ä¸ªä»ç¼“å­˜ï¼Œ"
            f"{len(missing_features)} ä¸ªéœ€è®¡ç®—"
        )

        # åˆ›å»ºåˆå§‹ä¸Šä¸‹æ–‡ï¼ˆé¢„å¡«å……ç¼“å­˜ç‰¹å¾ï¼‰
        initial_context = ExecutionContext()
        for name, value in cached_features.items():
            initial_context.set(name, value, 0)
        initial_context.cache_hits = len(cached_features)

        # å®ä¾‹åŒ–æå–å™¨ï¼ˆåªéœ€è¦æœªç¼“å­˜çš„ï¼‰
        extractor_instances = {}
        for node_name in missing_features:
            if node_name in self.graph.nodes:
                node = self.graph.nodes[node_name]
                if node.is_extractor:
                    extractor_instances[node.func] = get_extractor(node.func, node.params)

        # æ‰§è¡Œè®¡ç®—ï¼ˆä¼ å…¥åˆå§‹ä¸Šä¸‹æ–‡ï¼‰
        executor = Executor(
            compute_graph=self.graph,
            data_loaders=self.data_loaders,
            extractor_registry=extractor_instances,
        )

        context = executor.execute(initial_context=initial_context)
        self._computed_results = context

        # 4ï¸âƒ£ è¿”å›æ‰€æœ‰ç‰¹å¾
        features = {}
        for name in self.graph.nodes:
            if name in context.results:
                features[name] = context.results[name]

        # ä¿®æ­£ç»Ÿè®¡æ•°æ®ï¼ˆåªç»Ÿè®¡ç‰¹å¾èŠ‚ç‚¹ï¼Œä¸å«æ•°æ®æºï¼‰
        # ExecutionContext çš„ cache_hits åŒ…å«äº†æ•°æ®æºèŠ‚ç‚¹ï¼Œéœ€è¦ä¿®æ­£
        actual_cache_hits = len(cached_features)
        actual_cache_misses = len(missing_features)
        context.cache_hits = actual_cache_hits
        context.cache_misses = actual_cache_misses

        # è¾“å‡ºç»Ÿè®¡ï¼ˆåŒ…å«ç¼“å­˜ä¿¡æ¯ï¼‰
        cache_hit_rate = len(cached_features) / len(all_features) if all_features else 0
        logger.info(
            f"âœ… è®¡ç®—å®Œæˆï¼š{len(features)} ä¸ªç‰¹å¾ï¼Œ"
            f"ç¼“å­˜å‘½ä¸­ç‡ {cache_hit_rate:.1%}ï¼Œ"
            f"è€—æ—¶ {context.get_total_time():.2f}ms"
        )

        return features

    def to_dataframe(self, expand_multidim: bool = True) -> pd.DataFrame:
        """è½¬æ¢ä¸º pandas DataFrame

        Args:
            expand_multidim: æ˜¯å¦å±•å¼€å¤šç»´ç‰¹å¾ï¼ˆå¦‚ (n_steps, 100) â†’ 100 åˆ—ï¼‰

        Returns:
            DataFrame
        """
        if not self._computed_results:
            raise RuntimeError("è¯·å…ˆè°ƒç”¨ compute()")

        data_dict = {}

        for name, array in self._computed_results.results.items():
            if name in self.graph.nodes:  # åªåŒ…å«ç‰¹å¾ï¼ˆä¸å«æ•°æ®æºï¼‰
                if array.ndim == 1:
                    # æ ‡é‡ç‰¹å¾
                    data_dict[name] = array
                elif array.ndim == 2 and expand_multidim:
                    # å¤šç»´ç‰¹å¾ï¼šå±•å¼€ä¸ºå¤šåˆ—
                    for i in range(array.shape[1]):
                        data_dict[f'{name}_{i}'] = array[:, i]
                elif not expand_multidim:
                    # ä¿æŒåµŒå¥—ï¼ˆè½¬ä¸ºåˆ—è¡¨ï¼‰
                    data_dict[name] = list(array)
                else:
                    logger.warning(
                        f"ç‰¹å¾ '{name}' çš„ç»´åº¦ä¸º {array.ndim}ï¼Œæš‚ä¸æ”¯æŒè½¬æ¢"
                    )

        df = pd.DataFrame(data_dict)
        df.insert(0, 'step_index', np.arange(len(df)))
        return df

    def to_parquet(
        self,
        output_path: str,
        merge_existing: bool = False,
        save_metadata: bool = True
    ):
        """å¯¼å‡ºä¸º Parquet æ–‡ä»¶ï¼ˆæ”¯æŒå¢é‡åˆå¹¶å’Œå…ƒæ•°æ®ï¼‰

        Args:
            output_path: è¾“å‡ºè·¯å¾„
            merge_existing: æ˜¯å¦åˆå¹¶å·²æœ‰æ–‡ä»¶ï¼ˆå¢é‡æ¨¡å¼ï¼‰
            save_metadata: æ˜¯å¦ä¿å­˜å…ƒæ•°æ®ï¼ˆç”¨äºç¼“å­˜éªŒè¯ï¼‰
        """
        output_path = Path(output_path)
        new_df = self.to_dataframe(expand_multidim=True)

        # å¢é‡åˆå¹¶
        if merge_existing and output_path.exists():
            logger.info(f"ğŸ”„ å¢é‡åˆå¹¶åˆ°å·²æœ‰æ–‡ä»¶: {output_path.name}")
            try:
                existing_df = pd.read_parquet(output_path)

                # ä¿ç•™æ—§å…ƒæ•°æ®ï¼ˆç¨åæ›´æ–°ï¼‰
                old_attrs = existing_df.attrs.copy() if hasattr(existing_df, 'attrs') else {}

                # éªŒè¯è¡Œæ•°ä¸€è‡´æ€§
                if len(existing_df) != len(new_df):
                    raise ValueError(
                        f"Parquet åˆå¹¶å¤±è´¥ï¼šè¡Œæ•°ä¸åŒ¹é… "
                        f"(existing: {len(existing_df)}, new: {len(new_df)})"
                    )

                # åˆå¹¶åˆ—ï¼ˆè¦†ç›–åŒåï¼Œè¿½åŠ æ–°åˆ—ï¼‰
                for col in new_df.columns:
                    if col != 'step_index':
                        if col in existing_df.columns:
                            logger.debug(f"  è¦†ç›–åˆ—: {col}")
                        else:
                            logger.debug(f"  æ–°å¢åˆ—: {col}")
                        existing_df[col] = new_df[col]

                final_df = existing_df

                # æ›´æ–°ç‰¹å¾è®¡æ•°
                if save_metadata:
                    old_attrs['feature_count'] = len(final_df.columns) - 1
                    old_attrs['updated_at'] = pd.Timestamp.now().isoformat()
                    final_df.attrs = old_attrs

            except Exception as e:
                logger.error(f"åˆå¹¶å¤±è´¥: {e}ï¼Œå°†è¦†ç›–å†™å…¥")
                final_df = new_df
        else:
            final_df = new_df

        # æ·»åŠ å…ƒæ•°æ®
        if save_metadata and self.unified_experiment:
            # æ¨æ–­ç‰¹å¾ååˆ—è¡¨ï¼ˆåŒºåˆ†æ ‡é‡å’Œå¤šç»´ç‰¹å¾ï¼‰
            scalar_features = []
            multidim_features = {}

            for name, array in self._computed_results.results.items():
                if name not in self.graph.nodes:
                    continue  # è·³è¿‡æ•°æ®æºèŠ‚ç‚¹
                if array.ndim == 1:
                    scalar_features.append(name)
                elif array.ndim == 2:
                    multidim_features[name] = array.shape[1]

            final_df.attrs = {
                'chip_id': self.unified_experiment.chip_id,
                'device_id': self.unified_experiment.device_id,
                'config_name': self.config_name or 'unknown',
                'config_version': self.config_version,
                'source_file': str(self.unified_experiment._get_experiment().hdf5_path),  # è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                'source_hash': self._compute_source_hash(),
                'created_at': pd.Timestamp.now().isoformat(),
                'feature_count': len(final_df.columns) - 1,
                'scalar_features': scalar_features,  # æ–°å¢ï¼šæ ‡é‡ç‰¹å¾ååˆ—è¡¨
                'multidim_features': multidim_features  # æ–°å¢ï¼šå¤šç»´ç‰¹å¾ååŠå…¶ç»´åº¦æ•°
            }
            logger.debug(f"å·²æ·»åŠ å…ƒæ•°æ®: source_hash={final_df.attrs['source_hash']}, "
                        f"scalar_features={len(scalar_features)}, multidim_features={len(multidim_features)}")

        # ä¿å­˜
        final_df.to_parquet(output_path, compression='zstd', index=False)
        logger.info(
            f"âœ… å·²ä¿å­˜åˆ° {output_path} "
            f"({len(final_df)} è¡Œ Ã— {len(final_df.columns)} åˆ—)"
        )

    def get_statistics(self) -> Dict[str, Any]:
        """è·å–æ‰§è¡Œç»Ÿè®¡ä¿¡æ¯"""
        if not self._computed_results:
            raise RuntimeError("è¯·å…ˆè°ƒç”¨ compute()")

        return self._computed_results.get_statistics()

    @classmethod
    def from_config(cls, config_path: str, experiment=None, unified_experiment=None):
        """ä»é…ç½®æ–‡ä»¶åŠ è½½

        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆ.yaml æˆ– .jsonï¼‰
            experiment: Experiment å®ä¾‹
            unified_experiment: UnifiedExperiment å®ä¾‹

        Returns:
            FeatureSet å¯¹è±¡

        ç¤ºä¾‹:
            features = FeatureSet.from_config('config/v2_transfer.yaml', experiment=exp)
            # æˆ–ä½¿ç”¨ UnifiedExperiment
            features = FeatureSet.from_config('~/.my_features/my_config.yaml', unified_experiment=exp)
        """
        from infra.features_v2.config.parser import ConfigParser
        return ConfigParser.from_file(config_path, experiment, unified_experiment)

    def visualize_graph(self) -> str:
        """å¯è§†åŒ–è®¡ç®—å›¾"""
        return self.graph.visualize()

    def save_as_config(
        self,
        config_name: str,
        save_parquet: bool = True,
        append: bool = False,
        config_dir: str = 'user',
        description: str = ""
    ) -> Dict[str, Any]:
        """å›ºåŒ–å½“å‰ç‰¹å¾é›†ä¸ºé…ç½® + Parquet

        å°†å½“å‰è®¡ç®—çš„ç‰¹å¾é›†ä¿å­˜ä¸º YAML é…ç½®æ–‡ä»¶ï¼Œå¯é€‰åŒæ—¶ä¿å­˜ Parquet æ•°æ®ã€‚

        Args:
            config_name: é…ç½®åç§°
            save_parquet: æ˜¯å¦ä¿å­˜ Parquet æ•°æ®
            append: æ˜¯å¦å¢é‡è¿½åŠ ï¼ˆåˆå¹¶å·²æœ‰é…ç½®ï¼‰
            config_dir: é…ç½®ä¿å­˜ä½ç½®
                - 'user': ~/.my_features/ ï¼ˆä¸ªäººé…ç½®ï¼‰
                - 'global': infra/catalog/feature_configs/ ï¼ˆå…¨å±€å…±äº«ï¼‰
                - å…¶ä»–: è‡ªå®šä¹‰è·¯å¾„
            description: é…ç½®æè¿°

        Returns:
            {'config_file': '...', 'parquet_file': '...', 'features_added': [...], 'config_version': '...'}

        Raises:
            RuntimeError: å¦‚æœæœªå…ˆè°ƒç”¨ compute()
        """
        import yaml

        if not self._computed_results:
            raise RuntimeError("è¯·å…ˆè°ƒç”¨ compute() è®¡ç®—ç‰¹å¾")

        # 1ï¸âƒ£ ç¡®å®šä¿å­˜è·¯å¾„
        if config_dir == 'user':
            base_dir = Path.home() / '.my_features'
        elif config_dir == 'global':
            base_dir = Path(__file__).parent.parent.parent.parent / 'catalog' / 'feature_configs'
        else:
            base_dir = Path(config_dir)

        base_dir.mkdir(parents=True, exist_ok=True)
        config_file = base_dir / f"{config_name}.yaml"

        # 2ï¸âƒ£ æ„å»ºé…ç½®å­—å…¸
        feature_specs = []
        unsupported_features = []

        for node_name, node in self.graph.nodes.items():
            spec = {
                'name': node_name,
                'input': node.inputs[0] if len(node.inputs) == 1 else node.inputs,
            }

            # æ·»åŠ å‚æ•°ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if node.params:
                spec['params'] = node.params

            if node.is_extractor:
                # æå–å™¨ç‰¹å¾ï¼šä¿å­˜æå–å™¨åç§°
                spec['extractor'] = node.func
            elif callable(node.func):
                # Lambda/å‡½æ•°ç‰¹å¾ï¼šå°è¯•åºåˆ—åŒ–
                try:
                    import inspect

                    # ğŸ”‘ ä¼˜å…ˆä½¿ç”¨ä¿å­˜çš„æºä»£ç 
                    if node.source_code:
                        spec['func'] = node.source_code
                        logger.debug(f"âœ“ ä½¿ç”¨ä¿å­˜çš„æºä»£ç : {node_name} <- {node.source_code[:60]}...")
                    # æ£€æŸ¥æ˜¯å¦ä¸º lambdaï¼ˆå›é€€æ–¹æ¡ˆï¼‰
                    elif '<lambda>' in node.func.__name__:
                        # æå–çº¯ lambda è¡¨è¾¾å¼ï¼ˆä½¿ç”¨æ‹¬å·å¹³è¡¡ç®—æ³•ï¼‰
                        source = inspect.getsource(node.func).strip()

                        # æŸ¥æ‰¾ lambda å…³é”®å­—ä½ç½®
                        if 'lambda' in source:
                            start_idx = source.find('lambda')
                            remaining = source[start_idx:]

                            # æ™ºèƒ½æå–ï¼šä½¿ç”¨æ‹¬å·å¹³è¡¡ç®—æ³•æ‰¾åˆ° lambda è¡¨è¾¾å¼çš„ç»“å°¾
                            paren_count = 0
                            bracket_count = 0
                            in_string = False
                            quote_char = None
                            colon_found = False  # å…³é”®ï¼šæ ‡è®°æ˜¯å¦å·²ç»é‡åˆ°å†’å·
                            end_idx = len(remaining)

                            for i, char in enumerate(remaining):
                                # å¤„ç†å­—ç¬¦ä¸²ï¼ˆè·³è¿‡å­—ç¬¦ä¸²å†…çš„æ‹¬å·ï¼‰
                                if char in ('"', "'") and (i == 0 or remaining[i-1] != '\\'):
                                    if not in_string:
                                        in_string = True
                                        quote_char = char
                                    elif char == quote_char:
                                        in_string = False

                                if not in_string:
                                    # ç»Ÿè®¡æ‹¬å·
                                    if char == '(':
                                        paren_count += 1
                                    elif char == ')':
                                        paren_count -= 1
                                    elif char == '[':
                                        bracket_count += 1
                                    elif char == ']':
                                        bracket_count -= 1

                                    # æ ‡è®°å†’å·ä½ç½®ï¼ˆå‚æ•°åˆ—è¡¨ç»“æŸï¼Œè¡¨è¾¾å¼å¼€å§‹ï¼‰
                                    if char == ':' and paren_count == 0 and bracket_count == 0:
                                        colon_found = True

                                    # æ£€æŸ¥æ˜¯å¦åˆ°è¾¾ lambda è¡¨è¾¾å¼ç»“å°¾
                                    # åªæœ‰åœ¨å†’å·ä¹‹åï¼Œé‡åˆ°é€—å·æˆ–æ¢è¡Œæ—¶æ‰ç»“æŸ
                                    if colon_found and char in (',', '\n') and paren_count == 0 and bracket_count == 0:
                                        end_idx = i
                                        break

                            lambda_expr = remaining[:end_idx].strip()
                            logger.debug(f"æå–çš„ lambda: '{lambda_expr}'")

                            # éªŒè¯æå–çš„ lambda è¯­æ³•æ˜¯å¦æœ‰æ•ˆ
                            test_namespace = {'np': np, 'numpy': np}
                            try:
                                # ä½¿ç”¨ compile éªŒè¯è¯­æ³•ï¼ˆä¸æ‰§è¡Œï¼‰
                                compile(lambda_expr, '<lambda>', 'eval')
                                # å†ç”¨ eval åˆ›å»ºå‡½æ•°å¯¹è±¡ï¼ˆéªŒè¯å¯è°ƒç”¨æ€§ï¼‰
                                func_obj = eval(lambda_expr, test_namespace)
                                if not callable(func_obj):
                                    raise ValueError(f"'{lambda_expr}' ä¸æ˜¯å¯è°ƒç”¨å¯¹è±¡")
                            except SyntaxError as e:
                                raise ValueError(f"Lambda è¯­æ³•é”™è¯¯: {e}")

                            spec['func'] = lambda_expr
                            logger.debug(f"âœ“ åºåˆ—åŒ– lambda: {lambda_expr[:60]}...")
                        else:
                            raise ValueError("æ— æ³•åœ¨æºä»£ç ä¸­æ‰¾åˆ° lambda å…³é”®å­—")
                    else:
                        # å‘½åå‡½æ•°ï¼šè­¦å‘Šæ— æ³•åºåˆ—åŒ–
                        logger.warning(
                            f"ç‰¹å¾ '{node_name}' ä½¿ç”¨å‘½åå‡½æ•°ï¼Œæ— æ³•å®Œæ•´åºåˆ—åŒ–åˆ°é…ç½®æ–‡ä»¶"
                        )
                        spec['func'] = f"# UNSUPPORTED: {node.func.__name__}"
                        unsupported_features.append(node_name)
                except Exception as e:
                    logger.warning(f"æ— æ³•åºåˆ—åŒ–ç‰¹å¾ '{node_name}' çš„å‡½æ•°: {e}")
                    spec['func'] = "# UNSUPPORTED"
                    unsupported_features.append(node_name)

                # æ·»åŠ è¾“å‡ºå½¢çŠ¶
                if node.output_shape:
                    spec['output_shape'] = list(node.output_shape)
            else:
                logger.warning(f"è·³è¿‡ç‰¹å¾ '{node_name}'ï¼šä¸æ”¯æŒçš„ç±»å‹")
                continue

            feature_specs.append(spec)

        if unsupported_features:
            logger.warning(
                f"ä»¥ä¸‹ç‰¹å¾æ— æ³•å®Œæ•´åºåˆ—åŒ–: {unsupported_features}ã€‚"
                "å»ºè®®ä½¿ç”¨æå–å™¨æˆ–ç®€å• lambda è¡¨è¾¾å¼ã€‚"
            )

        config_dict = {
            'version': 'v2',
            'name': config_name,
            'config_version': self.config_version,
            'description': description or f"Auto-generated config for {config_name}",
            'data_type': 'transfer',  # TODO: ä»æ•°æ®æºæ¨æ–­
            'features': feature_specs
        }

        # 3ï¸âƒ£ å¤„ç† append æ¨¡å¼
        features_added = []
        if append and config_file.exists():
            logger.info(f"ğŸ“ å¢é‡æ¨¡å¼ï¼šåˆå¹¶å·²æœ‰é…ç½® {config_file.name}")
            with open(config_file, 'r', encoding='utf-8') as f:
                existing_config = yaml.safe_load(f)

            # åˆå¹¶ç‰¹å¾ï¼ˆæ™ºèƒ½å»é‡ï¼‰
            existing_features = {f['name']: f for f in existing_config.get('features', [])}

            for spec in feature_specs:
                name = spec['name']
                if name in existing_features:
                    # æ£€æŸ¥å®šä¹‰æ˜¯å¦ç›¸åŒ
                    if existing_features[name] == spec:
                        logger.info(f"  ç‰¹å¾ '{name}' å·²å­˜åœ¨ä¸”å®šä¹‰ç›¸åŒï¼Œè·³è¿‡")
                    else:
                        # å®šä¹‰ä¸åŒï¼Œæ›´æ–°
                        logger.warning(f"  ç‰¹å¾ '{name}' å®šä¹‰å·²æ›´æ–°")
                        existing_features[name] = spec
                        features_added.append(name)
                else:
                    # æ–°ç‰¹å¾
                    existing_features[name] = spec
                    features_added.append(name)
                    logger.info(f"  æ·»åŠ æ–°ç‰¹å¾: {name}")

            config_dict['features'] = list(existing_features.values())

            # é€’å¢ç‰ˆæœ¬å·
            old_version = existing_config.get('config_version', '1.0')
            try:
                major, minor = map(int, old_version.split('.'))
                config_dict['config_version'] = f"{major}.{minor + 1}"
            except ValueError:
                logger.warning(f"æ— æ³•è§£æç‰ˆæœ¬å· '{old_version}'ï¼Œé‡ç½®ä¸º 1.1")
                config_dict['config_version'] = "1.1"

            logger.info(f"  âœ“ é…ç½®ç‰ˆæœ¬æ›´æ–°: {old_version} â†’ {config_dict['config_version']}")
        else:
            features_added = [spec['name'] for spec in feature_specs]

        # 4ï¸âƒ£ ä¿å­˜é…ç½®æ–‡ä»¶
        with open(config_file, 'w', encoding='utf-8') as f:
            yaml.dump(config_dict, f, default_flow_style=False, allow_unicode=True, sort_keys=False)

        logger.info(f"âœ“ é…ç½®å·²ä¿å­˜: {config_file}")

        # 5ï¸âƒ£ ä¿å­˜ Parquetï¼ˆå¯é€‰ï¼‰
        parquet_file = None
        if save_parquet:
            if self.unified_experiment:
                # è·å– catalog é…ç½®çš„ features_v2 ç›®å½•ï¼ˆä¸ç°æœ‰ V2 æå–ä¿æŒä¸€è‡´ï¼‰
                try:
                    parquet_dir = self.unified_experiment._manager.catalog.config.get_absolute_path('features_v2')
                except Exception:
                    # Fallback: ä½¿ç”¨é»˜è®¤è·¯å¾„
                    parquet_dir = Path('data') / 'features_v2'

                parquet_dir.mkdir(parents=True, exist_ok=True)

                chip_id = self.unified_experiment.chip_id
                device_id = self.unified_experiment.device_id

                # åˆ é™¤åŒé…ç½®çš„æ—§æ–‡ä»¶ï¼ˆä¿æŒå”¯ä¸€æ€§ï¼‰
                old_pattern = f"{chip_id}-{device_id}-{config_name}-feat_*.parquet"
                old_files = list(parquet_dir.glob(old_pattern))
                if old_files:
                    logger.info(f"ğŸ—‘ï¸  åˆ é™¤ {len(old_files)} ä¸ªæ—§ Parquet æ–‡ä»¶ï¼ˆåŒé…ç½®ï¼‰")
                    for old_file in old_files:
                        old_file.unlink()
                        logger.debug(f"   åˆ é™¤: {old_file.name}")

                # ç”Ÿæˆæ–°æ–‡ä»¶å
                timestamp = pd.Timestamp.now().strftime('%Y%m%d-%H%M%S')
                import hashlib
                hash_suffix = hashlib.md5(f"{chip_id}{device_id}{config_name}{timestamp}".encode()).hexdigest()[:8]

                parquet_file = parquet_dir / f"{chip_id}-{device_id}-{config_name}-feat_{timestamp}_{hash_suffix}.parquet"

                self.to_parquet(
                    str(parquet_file),
                    merge_existing=False,  # é…ç½®å›ºåŒ–æ—¶æ€»æ˜¯åˆ›å»ºæ–°æ–‡ä»¶
                    save_metadata=True
                )
                logger.info(f"âœ“ Parquet å·²ä¿å­˜: {parquet_file}")

                # æ›´æ–°æ•°æ®åº“å…ƒæ•°æ®ï¼ˆå…³é”®ï¼ï¼‰
                self._update_v2_metadata_in_database(str(parquet_file), config_name)
            else:
                logger.warning("æœªæä¾› unified_experimentï¼Œè·³è¿‡ Parquet ä¿å­˜")

        return {
            'config_file': str(config_file),
            'parquet_file': str(parquet_file) if parquet_file else None,
            'features_added': features_added,
            'config_version': config_dict['config_version']
        }

    def _compute_source_hash(self) -> str:
        """è®¡ç®—æºæ–‡ä»¶è½»é‡çº§å“ˆå¸Œ

        åŸºäºå…ƒæ•°æ®ï¼ˆchip_id, device_id, mtime, sizeï¼‰è®¡ç®—å“ˆå¸Œï¼Œ
        é¿å…è¯»å–å®Œæ•´æ–‡ä»¶å†…å®¹ã€‚

        Returns:
            å“ˆå¸Œå­—ç¬¦ä¸²ï¼ˆMD5 å‰16ä½ï¼‰
        """
        import hashlib

        if not self.unified_experiment:
            return ""

        try:
            exp = self.unified_experiment._get_experiment()
            file_path = Path(exp.hdf5_path)

            if not file_path.exists():
                logger.warning(f"æºæ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                return ""

            stat = file_path.stat()
            hash_input = (
                f"{self.unified_experiment.chip_id}|"
                f"{self.unified_experiment.device_id}|"
                f"{stat.st_mtime}|"
                f"{stat.st_size}"
            )

            hash_value = hashlib.md5(hash_input.encode()).hexdigest()[:16]
            logger.debug(f"è®¡ç®—æºæ–‡ä»¶å“ˆå¸Œ: {hash_value} (åŸºäº {file_path.name})")
            return hash_value

        except Exception as e:
            logger.error(f"è®¡ç®—æºæ–‡ä»¶å“ˆå¸Œå¤±è´¥: {e}")
            return ""

    def _update_v2_metadata_in_database(self, parquet_file: str, config_name: str):
        """æ›´æ–°æ•°æ®åº“ä¸­çš„ V2 ç‰¹å¾å…ƒæ•°æ®

        Args:
            parquet_file: Parquet æ–‡ä»¶è·¯å¾„
            config_name: é…ç½®åç§°
        """
        if not self.unified_experiment:
            return

        try:
            # æ„å»ºå…ƒæ•°æ®
            metadata = {
                'config_name': config_name,
                'config_version': self.config_version,
                'output_files': [parquet_file],
                'created_at': pd.Timestamp.now().isoformat(),
                'feature_count': len(self.graph.nodes)
            }

            # è·å–æ•°æ®åº“ä»“åº“
            repo = self.unified_experiment._manager.catalog.repository

            # è¯»å–ç°æœ‰å…ƒæ•°æ®
            exp_id = self.unified_experiment.id
            existing_metadata = repo.get_v2_feature_metadata(exp_id)

            if existing_metadata:
                # åˆå¹¶æ–‡ä»¶åˆ—è¡¨ï¼ˆé¿å…é‡å¤ï¼‰
                existing_files = existing_metadata.get('output_files', [])
                if parquet_file not in existing_files:
                    existing_files.append(parquet_file)
                    metadata['output_files'] = existing_files
                else:
                    metadata['output_files'] = existing_files

            # æ›´æ–°åˆ°æ•°æ®åº“
            repo.update_v2_feature_metadata(exp_id, metadata)
            logger.info(f"âœ“ å·²æ›´æ–°æ•°æ®åº“å…ƒæ•°æ®: exp_id={exp_id}, config={config_name}")

        except Exception as e:
            logger.warning(f"æ›´æ–°æ•°æ®åº“å…ƒæ•°æ®å¤±è´¥: {e}ï¼ˆä¸å½±å“ Parquet ä¿å­˜ï¼‰")

    def _validate_cache(self, cached_df: pd.DataFrame, strict: bool = False) -> bool:
        """éªŒè¯ Parquet ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ

        é€šè¿‡æ¯”è¾ƒæºæ–‡ä»¶å“ˆå¸Œåˆ¤æ–­ç¼“å­˜æ˜¯å¦ä»ç„¶æœ‰æ•ˆã€‚

        Args:
            cached_df: ç¼“å­˜çš„ DataFrame
            strict: ä¸¥æ ¼æ¨¡å¼ï¼ˆç¼ºå°‘ hash æ—¶è¿”å› Falseï¼‰

        Returns:
            ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ
        """
        if not hasattr(cached_df, 'attrs'):
            logger.warning("ç¼“å­˜ DataFrame ç¼ºå°‘ attrs å±æ€§")
            return not strict

        metadata = cached_df.attrs
        cached_hash = metadata.get('source_hash')

        if not cached_hash:
            if strict:
                logger.error("ä¸¥æ ¼æ¨¡å¼ï¼šç¼“å­˜ç¼ºå°‘ source_hashï¼Œè§†ä¸ºæ— æ•ˆ")
                return False
            else:
                # å‘åå…¼å®¹ï¼šæ—§ç¼“å­˜æ²¡æœ‰ hashï¼Œå‡è®¾æœ‰æ•ˆ
                logger.warning("ç¼“å­˜ç¼ºå°‘ source_hashï¼Œå‘åå…¼å®¹æ¨¡å¼ä¸‹å‡è®¾æœ‰æ•ˆ")
                return True

        # è®¡ç®—å½“å‰æºæ–‡ä»¶å“ˆå¸Œ
        current_hash = self._compute_source_hash()

        if not current_hash:
            logger.warning("æ— æ³•è®¡ç®—å½“å‰æºæ–‡ä»¶å“ˆå¸Œï¼Œå‡è®¾ç¼“å­˜æœ‰æ•ˆ")
            return True

        if current_hash != cached_hash:
            logger.debug(f"æºæ–‡ä»¶å“ˆå¸Œä¸åŒ¹é…: å½“å‰={current_hash}, ç¼“å­˜={cached_hash}")
            return False

        logger.debug("ç¼“å­˜éªŒè¯é€šè¿‡ï¼šæºæ–‡ä»¶å“ˆå¸ŒåŒ¹é…")
        return True

    def __repr__(self):
        return f"FeatureSet(features={len(self.graph.nodes)})"
