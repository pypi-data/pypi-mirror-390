# BSD 3-Clause License
#
# Copyright (c) 2025, Jean-Pierre Morard, THALES SIX GTS France SAS
# All rights reserved.
#
# Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:
#
# 1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.
# 2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.
# 3. Neither the name of Jean-Pierre Morard nor the names of its contributors, or THALES SIX GTS France SAS, may be used to endorse or promote products derived from this software without specific prior written permission.
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

import os
import sys
import pandas as pd
import streamlit as st
import plotly.express as px
from pathlib import Path
import argparse


def _ensure_repo_on_path() -> None:
    here = Path(__file__).resolve()
    for parent in here.parents:
        candidate = parent / "agilab"
        if candidate.is_dir():
            src_root = candidate.parent
            repo_root = src_root.parent
            for entry in (str(src_root), str(repo_root)):
                if entry not in sys.path:
                    sys.path.insert(0, entry)
            break


_ensure_repo_on_path()

def _default_app() -> Path | None:
    apps_dir = Path(__file__).resolve().parents[4] / "apps"
    if not apps_dir.exists():
        return None
    for candidate in sorted(apps_dir.iterdir()):
        if candidate.is_dir() and candidate.name.endswith("_project"):
            return candidate
    return None


from agi_env import AgiEnv
from agi_env.pagelib import find_files, load_df, update_datadir, initialize_csv_files

var = ["discrete", "continuous", "lat", "long"]
var_default = [0, None]

st.title(":world_map: Cartography Visualisation")


def continuous():
    """Set coltype to 'continuous'."""
    st.session_state["coltype"] = "continuous"


def discrete():
    """Set coltype to 'discrete'."""
    st.session_state["coltype"] = "discrete"

  # Default to 'discrete'


def downsample_df_deterministic(df: pd.DataFrame, ratio: int) -> pd.DataFrame:
    """
    Return a new DataFrame containing every `ratio`-th row from the original df.

    Parameters
    ----------
    df : pd.DataFrame
        The original DataFrame to down-sample.
    ratio : int
        Keep one row every `ratio` rows. E.g. ratio=20 → rows 0, 20, 40, …

    Returns
    -------
    pd.DataFrame
        The down-sampled DataFrame, re-indexed from 0.
    """
    if ratio <= 0:
        raise ValueError("`ratio` must be a positive integer.")
    # Ensure a clean integer index before slicing
    df_reset = df.reset_index(drop=True)
    # Take every ratio-th row
    sampled = df_reset.iloc[::ratio].copy()
    # Reset index for the result
    return sampled.reset_index(drop=True)

def page(env):
    """
    Page function for displaying and interacting with data in a Streamlit app.

    This function sets up the page layout and functionality for displaying and interacting with data in a Streamlit app.

    It handles the following key tasks:
    - Setting up default values for session state variables related to the project, help path, and available projects.
    - Checking and validating the data directory path, and displaying appropriate messages if it is invalid or not found.
    - Loading and displaying the selected data file in a DataFrame.
    - Allowing users to select columns for visualizations and customization options like color sequence and scale.
    - Generating and displaying interactive scatter maps based on selected columns for latitude, longitude, and coloring.

    No specific Args are passed to this function as it directly interacts with and manipulates the page layout and user inputs in a Streamlit app.

    Returns:
        None

    Raises:
        None
    """

    if "project" not in st.session_state:
        st.session_state["project"] = env.target

    if "projects" not in st.session_state:
        st.session_state["projects"] = env.projects

    # Resolve the data directory for the currently selected app
    default_datadir = Path(env.AGILAB_EXPORT_ABS) / env.target
    last_target_key = "_view_maps_last_target"
    last_target = st.session_state.get(last_target_key)

    current = st.session_state.get("datadir")
    if (
        last_target != env.target
        or current is None
        or str(current).strip() == ""
    ):
        current = str(default_datadir)
    st.session_state["datadir"] = str(current)

    st.session_state["datadir_str"] = st.session_state["datadir"]
    st.session_state[last_target_key] = env.target
    datadir = Path(st.session_state["datadir"])
    datadir_changed = st.session_state.get("_view_maps_last_datadir") != str(datadir)
    st.session_state["_view_maps_last_datadir"] = str(datadir)
    # Data directory input
    st.sidebar.text_input(
        "Data Directory",
        value=st.session_state["datadir"],
        key="input_datadir",
        on_change=update_datadir,
        args=("datadir", "input_datadir"),
    )

    if not datadir.exists() or not datadir.is_dir():
        st.sidebar.error("Directory not found.")
        st.warning("A valid data directory is required to proceed.")
        return  # Stop further processing

    # Find CSV files in the data directory
    dataset_key = "dataset_files"
    legacy_key = "csv_files"
    if dataset_key not in st.session_state and legacy_key in st.session_state:
        st.session_state[dataset_key] = st.session_state.pop(legacy_key)

    datadir_exts = (".csv", ".parquet", ".json")
    dataset_files: list[Path] = []
    for ext in datadir_exts:
        try:
            dataset_files.extend(find_files(st.session_state["datadir"], ext=ext))
        except NotADirectoryError as exc:
            st.warning(str(exc))
            dataset_files = []
            break

    st.session_state[dataset_key] = dataset_files
    if not st.session_state[dataset_key]:
        st.warning(
            f"No dataset found in {datadir}. "
            "Use the EXECUTE → EXPORT workflow to materialize CSV/Parquet/JSON outputs first."
        )
        st.stop()  # Stop further processing

    # Prepare list of CSV files relative to the data directory
    dataset_files_rel = sorted(
        {
            Path(file).relative_to(datadir).as_posix()
            for file in st.session_state[dataset_key]
        }
    )

    # Prefer the consolidated export file when present (matches flight app UX)
    priority_files = [
        candidate
        for candidate in dataset_files_rel
        if Path(candidate).name.lower() in {"export.csv", "export.parquet", "export.json"}
    ]
    default_selection = [priority_files[0]] if priority_files else (dataset_files_rel[:1] if dataset_files_rel else [])

    if (
        "df_files_selected" not in st.session_state
        or not st.session_state["df_files_selected"]
        or any(item not in dataset_files_rel for item in st.session_state["df_files_selected"])
    ):
        st.session_state["df_files_selected"] = default_selection

    current_selection = st.session_state.get("df_files_selected")
    if datadir_changed:
        st.session_state["df_files_selected"] = default_selection
        current_selection = default_selection
    if (
        current_selection is None
        or any(item not in dataset_files_rel for item in current_selection)
    ):
        st.session_state["df_files_selected"] = default_selection
    elif not current_selection and default_selection:
        st.session_state["df_files_selected"] = default_selection
    st.sidebar.multiselect(
        label="DataFrames",
        options=dataset_files_rel,
        key="df_files_selected",
    )

    selected_files = st.session_state.get("df_files_selected", [])
    if not selected_files:
        st.warning("Please select at least one dataset to proceed.")
        return

    # Load and concatenate selected DataFrames
    dataframes = []
    for rel_path in selected_files:
        df_file_abs = datadir / rel_path
        try:
            df_loaded = load_df(df_file_abs, with_index=True)
        except Exception as e:
            st.error(f"Error loading data from {rel_path}: {e}")
            continue
        df_loaded = df_loaded.copy()
        df_loaded["__dataset__"] = rel_path
        dataframes.append(df_loaded)

    if not dataframes:
        st.warning("The selected data files could not be loaded. Please select valid files.")
        return

    try:
        combined_df = pd.concat(dataframes, ignore_index=True)
    except Exception as e:
        st.error(f"Error concatenating datasets: {e}")
        return

    st.session_state["loaded_df"] = combined_df

    # Check if data is loaded and valid
    if (
            "loaded_df" not in st.session_state
            or not isinstance(st.session_state.loaded_df, pd.DataFrame)
            or not st.session_state.loaded_df.shape[1] > 0
    ):
        st.warning("The dataset is empty or could not be loaded. Please select a valid data file.")
        return  # Stop further processing

    # data filter to speed-up
    c = st.columns(5)
    sampling_ratio = c[4].number_input(
        "Sampling ratio",
        min_value=1,
        value=st.session_state.GUI_SAMPLING,
        step=1,
    )
    st.session_state.GUI_SAMPLING = sampling_ratio
    st.session_state.loaded_df = downsample_df_deterministic(st.session_state.loaded_df, sampling_ratio)
    nrows = st.session_state.loaded_df.shape[0]

    lines = st.slider(
        "Select the desired number of points:",
        min_value=5,
        max_value=nrows,
        value=st.session_state.TABLE_MAX_ROWS,
        step=10,
    )
    st.session_state.TABLE_MAX_ROWS = lines
    if lines >= 0:
        st.session_state.loaded_df = st.session_state.loaded_df.iloc[:lines, :]

    df = st.session_state.loaded_df

    # Select numeric columns
    numeric_cols = st.session_state.loaded_df.select_dtypes(include=["number"]).columns.tolist()

    # Define lists to store continuous and discrete numeric variables
    continuous_cols = []
    discrete_numeric_cols = []

    # Define a threshold: if a numeric column has fewer unique values than this threshold,
    # treat it as discrete. Adjust this value based on your needs.
    # Threshold to classify numeric columns as discrete vs continuous
    unique_threshold = st.sidebar.number_input(
        "Discrete threshold (unique values <)",
        min_value=2,
        max_value=100,
        value=10,
        step=1,
    )

    # Loop through numeric columns and classify them based on the unique value count.
    for col in numeric_cols:
        if df[col].nunique() < unique_threshold:
            discrete_numeric_cols.append(col)
        else:
            continuous_cols.append(col)

    # Get discrete variables from object type
    discrete_object_cols = df.select_dtypes(include=["object"]).columns.tolist()

    # Combine numeric discrete and object discrete variables
    discrete_cols = discrete_numeric_cols + discrete_object_cols
    discreteseq = None
    colorscale = None

    # Identify numerical columns
    for col in discrete_cols.copy():  # Use copy to avoid modifying the list during iteration
        try:
            pd.to_datetime(
                st.session_state.loaded_df[col],
                format="%Y-%m-%d %H:%M:%S",
                errors="raise",
            )
            discrete_cols.remove(col)
            continuous_cols.append(col)
        except (ValueError, TypeError):
            pass

    for i, cols in enumerate([discrete_cols, continuous_cols]):
        if cols:
            colsn = (
                pd.DataFrame(
                    [
                        {
                            "Columns": col,
                            "nbval": len(set(st.session_state.loaded_df[col])),
                        }
                        for col in cols
                    ]
                )
                .sort_values(by="nbval", ascending=False)
                .Columns.tolist()
            )
            on_change_function = None
            if var[i] == "discrete":
                on_change_function = discrete
            elif var[i] == "continuous":
                on_change_function = continuous
            with c[i]:
                st.selectbox(
                    label=f"{var[i]}",
                    options=colsn,
                    index=var_default[i] if var_default[i] is not None and var_default[i] < len(colsn) else 0,
                    key=var[i],
                    on_change=on_change_function,
                )
                if var[i] == "discrete":
                    discreteseqs = [
                        "Plotly",
                        "D3",
                        "G10",
                        "T10",
                        "Alphabet",
                        "Dark24",
                        "Light24",
                        "Set1",
                        "Pastel1",
                        "Dark2",
                        "Set2",
                        "Pastel2",
                        "Set3",
                    ]
                    discreteseq = st.selectbox("Color Sequence", discreteseqs, index=0)
                elif var[i] == "continuous":
                    colorscales = px.colors.named_colorscales()
                    colorscale = st.selectbox("Color Scale", colorscales, index=0)
        else:
            with c[i]:
                st.warning(f"No columns available for {var[i]}.")
                st.session_state[var[i]] = None

    for i in range(2, 4):
        colsn = st.session_state.loaded_df.filter(regex=var[i]).columns.tolist()
        with c[i]:
            if colsn:
                st.selectbox(f"{var[i]}", colsn, index=0, key=var[i])
            else:
                st.warning(f"No columns matching '{var[i]}' found.")
                st.session_state[var[i]] = None

    if st.session_state.get("lat") and st.session_state.get("long"):
        if st.session_state.get("coltype") and st.session_state.get(st.session_state["coltype"]):
            if discreteseq:
                # Get the color sequence
                color_sequence = getattr(px.colors.qualitative, discreteseq)
                fig = px.scatter_mapbox(
                    st.session_state.loaded_df,
                    lat=st.session_state.lat,
                    lon=st.session_state.long,
                    zoom=2.5,
                    color_discrete_sequence=color_sequence,
                    color=st.session_state[st.session_state.coltype],
                )
            elif colorscale:
                fig = px.scatter_mapbox(
                    st.session_state.loaded_df,
                    lat=st.session_state.lat,
                    lon=st.session_state.long,
                    zoom=2.5,
                    color_continuous_scale=colorscale,
                    color=st.session_state[st.session_state.coltype],
                )
            else:
                fig = px.scatter_mapbox(
                    st.session_state.loaded_df,
                    lat=st.session_state.lat,
                    lon=st.session_state.long,
                    zoom=2.5,
                )

            fig.update_layout(mapbox_style="open-street-map")
            fig.update_layout(margin={"r": 0, "t": 0, "l": 0, "b": 0})

            st.plotly_chart(fig, use_container_width=True, theme="streamlit")
        else:
            st.warning("Please select a valid column for coloring.")
    else:
        st.warning("Latitude and Longitude columns are required for the map.")


# -------------------- Main Application Entry -------------------- #
def main():
    """
    Main function to run the application.
    """

    try:
        parser = argparse.ArgumentParser(description="Run the AGI Streamlit View with optional parameters.")
        parser.add_argument(
            "--active-app",
            dest="active_app",
            type=str,
            help="Active app path (e.g. src/agilab/apps/flight_project)",
            required=True,
        )
        args, _ = parser.parse_known_args()

        active_app = Path(args.active_app).expanduser()
        if not active_app.exists():
            st.error(f"Error: provided --active-app path not found: {active_app}")
            sys.exit(1)

        if "coltype" not in st.session_state:
            st.session_state["coltype"] = var[0]

        # Derive the short app name (e.g., 'flight_project')
        app = active_app.name
        st.session_state["apps_dir"] = str(active_app.parent)
        st.session_state["app"] = app

        st.info(f"active_app: {active_app}")
        env = AgiEnv(
            apps_dir=active_app.parent,
            app=app,
            verbose=1,
        )
        env.init_done = True
        st.session_state['env'] = env
        st.session_state["IS_SOURCE_ENV"] = env.is_source_env
        st.session_state["IS_WORKER_ENV"] = env.is_worker_env

        if "TABLE_MAX_ROWS" not in st.session_state:
            st.session_state["TABLE_MAX_ROWS"] = env.TABLE_MAX_ROWS
        if "GUI_SAMPLING" not in st.session_state:
            st.session_state["GUI_SAMPLING"] = env.GUI_SAMPLING

        page(env)

    except Exception as e:
        st.error(f"An error occurred: {e}")
        import traceback

        st.code(traceback.format_exc())


# -------------------- Main Entry Point -------------------- #
if __name__ == "__main__":
    main()
