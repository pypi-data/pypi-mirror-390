<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ANE VoxCPM TTS Playground</title>
    <script src="https://cdn.tailwindcss.com"></script>
    <style>
        /* Simple spinner animation */
        @keyframes spin {
            to { transform: rotate(360deg); }
        }
        .spinner {
            animation: spin 1s linear infinite;
        }
    </style>
</head>
<body class="bg-gray-900 text-gray-100 font-sans p-4 md:p-8">

    <div class="max-w-2xl mx-auto">
        <header class="mb-8">
            <h1 class="text-4xl font-bold text-center text-transparent bg-clip-text bg-gradient-to-r from-purple-400 to-pink-500">
                ANE VoxCPM TTS Playground
            </h1>
        </header>

        <div id="status-message" class="hidden p-4 rounded-lg mb-6 text-center"></div>

        <form id="tts-form" class="space-y-6 bg-gray-800 p-6 rounded-xl shadow-2xl">
            
            <div>
                <label for="text-input" class="block text-sm font-medium text-gray-300 mb-2">
                    Text to Generate
                </label>
                <textarea id="text-input" rows="4" class="w-full p-3 bg-gray-700 border border-gray-600 rounded-lg text-gray-100 focus:ring-2 focus:ring-purple-500 focus:border-transparent outline-none transition" placeholder="Enter text to synthesize...">Jittery Jack's jam jars jiggled jauntily, jolting Jack's jumbled jelly-filled jars joyously.
Cindy's circular cymbals clanged cheerfully, clashing crazily near Carla's crashing crockery.
You think you can just waltz in here and cause chaos? Well, I've got news for you.</textarea>
            </div>

            <div>
                <label for="voice-select" class="flex items-center text-sm font-medium text-gray-300 mb-2">
                    <span>Voice Selection (Optional)</span>
                    <a href="https://gregr.org/tts-samples/" target="_blank" rel="noopener noreferrer" class="ml-2 text-sm text-purple-400 hover:text-purple-300 transition-colors">
                        (See Samples)
                    </a>
                </label>
                <select id="voice-select" class="w-full p-3 bg-gray-700 border border-gray-600 rounded-lg text-gray-100 focus:ring-2 focus:ring-purple-500 focus:border-transparent outline-none transition">
                    <option value="">Use Prompt Audio</option>
                </select>
                <div id="voice-status" class="mt-2 text-sm text-gray-400"></div>
            </div>

            <div id="prompt-section">
                <label for="prompt-wav-path" class="flex items-center text-sm font-medium text-gray-300 mb-2">
                    <span>Prompt WAV Path (Optional for Voice Cloning)</span>
                    <div class="relative group ml-2">
                        <span class="w-4 h-4 flex items-center justify-center bg-gray-600 text-gray-300 text-xs font-bold rounded-full cursor-help">?</span>
                        <div class="absolute hidden group-hover:block left-full ml-3 top-1/2 -translate-y-1/2 w-72 p-3 bg-black text-white text-sm rounded-lg shadow-lg z-10 transition-all duration-150 ease-in-out opacity-0 group-hover:opacity-100 pointer-events-none">
                            Random sounding voice if unspecified.
                        </div>
                    </div>
                </label>
                <input type="text" id="prompt-wav-path" value="" class="w-full p-3 bg-gray-700 border border-gray-600 rounded-lg text-gray-100 focus:ring-2 focus:ring-purple-500 focus:border-transparent outline-none transition">
            </div>
            
            <div id="prompt-text-section">
                <label for="prompt-text" class="flex items-center text-sm font-medium text-gray-300 mb-2">
                    <span>Prompt Text (Required when using prompt WAV)</span>
                    
                    <div class="relative group ml-2">
                        <span class="w-4 h-4 flex items-center justify-center bg-gray-600 text-gray-300 text-xs font-bold rounded-full cursor-help">?</span>
                        <div class="absolute hidden group-hover:block left-full ml-3 top-1/2 -translate-y-1/2 w-72 p-3 bg-black text-white text-sm rounded-lg shadow-lg z-10 transition-all duration-150 ease-in-out opacity-0 group-hover:opacity-100 pointer-events-none">
                            This text is transcribed from the 'Prompt WAV' and is used to condition the model on the voice's acoustic properties. It <strong>must</strong> be an accurate transcription of the prompt audio.
                        </div>
                    </div>
                    </label>
                <textarea id="prompt-text" rows="2" class="w-full p-3 bg-gray-700 border border-gray-600 rounded-lg text-gray-100 focus:ring-2 focus:ring-purple-500 focus:border-transparent outline-none transition" placeholder="Not required if using a provided voice"></textarea>
            </div>

            <div class="grid grid-cols-1 md:grid-cols-3 gap-4">
                <div>
                    <label for="max-length" class="block text-sm font-medium text-gray-300 mb-2">
                        Max Length (0.04s per unit)
                    </label>
                    <input type="number" id="max-length" value="2048" min="1" max="2048" class="w-full p-3 bg-gray-700 border border-gray-600 rounded-lg text-gray-100 focus:ring-2 focus:ring-purple-500 focus:border-transparent outline-none transition">
                    <div class="mt-1 text-xs text-gray-500">Maximum generated sequence length (1-2048)</div>
                </div>
                <div>
                    <label for="cfg-value" class="block text-sm font-medium text-gray-300 mb-2">
                        CFG Value
                    </label>
                    <input type="number" id="cfg-value" value="2.0" min="0.0" max="10.0" step="0.1" class="w-full p-3 bg-gray-700 border border-gray-600 rounded-lg text-gray-100 focus:ring-2 focus:ring-purple-500 focus:border-transparent outline-none transition">
                    <div class="mt-1 text-xs text-gray-500">Classifier-free guidance value (0.0-10.0)</div>
                </div>
                <div>
                    <label for="inference-timesteps" class="flex items-center text-sm font-medium text-gray-300 mb-2">
                        <span>Inference Timesteps</span>
                        
                        <div class="relative group ml-2">
                            <span class="w-4 h-4 flex items-center justify-center bg-gray-600 text-gray-300 text-xs font-bold rounded-full cursor-help">?</span>
                            <div class="absolute hidden group-hover:block left-full ml-3 top-1/2 -translate-y-1/2 w-72 p-3 bg-black text-white text-sm rounded-lg shadow-lg z-10 transition-all duration-150 ease-in-out opacity-0 group-hover:opacity-100 pointer-events-none">
                                Controls the number of diffusion steps. A higher number (e.g., 20) is slower but may increase quality. A lower number (e.g., 5-10) is faster. This model works well with 10.
                            </div>
                        </div>
                        </label>
                    <input type="number" id="inference-timesteps" value="10" min="1" max="100" class="w-full p-3 bg-gray-700 border border-gray-600 rounded-lg text-gray-100 focus:ring-2 focus:ring-purple-500 focus:border-transparent outline-none transition">
                    <div class="mt-1 text-xs text-gray-500">Number of inference steps (1-100)</div>
                </div>
            </div>

            <div class="flex flex-col sm:flex-row gap-4">
                <button type="submit" id="play-button" class="flex-1 flex items-center justify-center gap-2 bg-gradient-to-r from-purple-600 to-pink-600 text-white font-bold py-3 px-6 rounded-lg shadow-lg hover:from-purple-700 hover:to-pink-700 transition duration-300 disabled:opacity-50 disabled:cursor-not-allowed">
                    <svg class="w-6 h-6" fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" d="M10 18a8 8 0 100-16 8 8 0 000 16zM9.555 7.168A1 1 0 008 8v4a1 1 0 001.555.832l3-2a1 1 0 000-1.664l-3-2z" clip-rule="evenodd"></path></svg>
                    <span id="play-button-text">Generate & Play</span>
                    <div id="spinner" class="hidden spinner w-5 h-5 border-t-2 border-b-2 border-white rounded-full"></div>
                </button>

                <button type="button" id="generate-full-button" class="flex-1 flex items-center justify-center gap-2 bg-blue-600 text-white font-bold py-3 px-6 rounded-lg shadow-lg hover:bg-blue-700 transition duration-300 disabled:opacity-50 disabled:cursor-not-allowed">
                    <svg class="w-6 h-6" fill="currentColor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" d="M3 17a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1zM6.293 9.293a1 1 0 011.414 0L9 10.586V3a1 1 0 112 0v7.586l1.293-1.293a1 1 0 111.414 1.414l-3 3a1 1 0 01-1.414 0l-3-3a1 1 0 010-1.414z" clip-rule="evenodd"></path></svg>
                    <span id="generate-full-button-text">Generate Full Audio</span>
                    <div id="generate-full-spinner" class="hidden spinner w-5 h-5 border-t-2 border-b-2 border-white rounded-full"></div>
                </button>
                <button type="button" id="stop-button" class="flex-1 bg-gray-600 text-gray-200 font-bold py-3 px-6 rounded-lg shadow-lg hover:bg-gray-500 transition duration-300 disabled:opacity-50 disabled:cursor-not-allowed" disabled>
                    Stop Generation
                </button>
            </div>
        </form>
        
        <div id="audio-player-container" class="hidden mt-6 bg-gray-800 p-4 rounded-xl shadow-2xl">
            <label class="block text-sm font-medium text-gray-300 mb-3">
                Full Audio Playback
            </label>
            <audio id="audio-player" controls class="w-full"></audio>
        </div>

    </div>

    <script>
        // --- DOM Elements ---
        const ttsForm = document.getElementById('tts-form');
        const playButton = document.getElementById('play-button');
        const playButtonText = document.getElementById('play-button-text');
        const stopButton = document.getElementById('stop-button');
        const spinner = document.getElementById('spinner');
        const statusMessage = document.getElementById('status-message');

        const textInput = document.getElementById('text-input');
        const promptTextInput = document.getElementById('prompt-text');
        const promptWavPathInput = document.getElementById('prompt-wav-path');
        
        // --- Voice Selection Elements ---
        const voiceSelect = document.getElementById('voice-select');
        const voiceStatus = document.getElementById('voice-status');
        const promptSection = document.getElementById('prompt-section');
        // --- NEW: Get the prompt text section ---
        const promptTextSection = document.getElementById('prompt-text-section');
        
        // --- Parameter Elements ---
        const maxLengthInput = document.getElementById('max-length');
        const cfgValueInput = document.getElementById('cfg-value');
        const inferenceTimestepsInput = document.getElementById('inference-timesteps');
        
        // --- Audio Player Elements ---
        const audioPlayerContainer = document.getElementById('audio-player-container');
        const audioPlayer = document.getElementById('audio-player');
        
        // === NEW: Full Generation Button Elements ===
        const generateFullButton = document.getElementById('generate-full-button');
        const generateFullButtonText = document.getElementById('generate-full-button-text');
        const generateFullSpinner = document.getElementById('generate-full-spinner');
        // === END NEW ===

        // --- AudioContext Variables ---
        let audioContext;
        let streamReader;
        let nextStartTime = 0; // Time at which the next audio chunk should start playing
        let isPlaying = false;
        let sampleRate = 16000; // Default, will try to get from header
        
        // --- Array to store audio chunks for final playback ---
        let audioChunks = [];

        // --- Load available voices on page load ---
        async function loadAvailableVoices() {
            try {
                const response = await fetch('/voices');
                const data = await response.json();
                
                // Clear existing options except the first one
                voiceSelect.innerHTML = '<option value="">Use Prompt Audio</option>';
                
                // Add voice options
                data.voices.forEach(voice => {
                    const option = document.createElement('option');
                    option.value = voice;
                    option.textContent = voice;
                    voiceSelect.appendChild(option);
                });
                
                voiceStatus.textContent = `Found ${data.count} available voices`;
                voiceStatus.className = 'mt-2 text-sm text-green-400';
                
            } catch (error) {
                console.error('Error loading voices:', error);
                voiceStatus.textContent = 'Failed to load available voices';
                voiceStatus.className = 'mt-2 text-sm text-red-400';
            }
        }

        // --- Handle voice selection changes ---
        voiceSelect.addEventListener('change', (event) => {
            const selectedVoice = event.target.value;
            if (selectedVoice) {
                // --- MODIFIED: Hide both prompt sections ---
                promptSection.style.display = 'none';
                promptTextSection.style.display = 'none';
                voiceStatus.textContent = `Using cached voice: ${selectedVoice}`;
                voiceStatus.className = 'mt-2 text-sm text-blue-400';
            } else {
                // --- MODIFIED: Show both prompt sections ---
                promptSection.style.display = 'block';
                promptTextSection.style.display = 'block';
                voiceStatus.textContent = 'Using prompt audio';
                voiceStatus.className = 'mt-2 text-sm text-gray-400';
            }
        });

        // --- Load voices when page loads ---
        loadAvailableVoices();

        // --- Utility to show status messages ---
        function showStatus(message, type = 'info') {
            statusMessage.textContent = message;
            statusMessage.className = 'p-4 rounded-lg mb-6 text-center '; // Reset classes
            
            if (type === 'error') {
                statusMessage.classList.add('bg-red-800', 'text-red-100');
            } else if (type === 'success') {
                statusMessage.classList.add('bg-green-800', 'text-green-100');
            } else { // 'info'
                statusMessage.classList.add('bg-blue-800', 'text-blue-100');
            }
            statusMessage.classList.remove('hidden');
        }
        function hideStatus() {
            statusMessage.classList.add('hidden');
        }
        
        // --- UI State Management ---
        function setPlayingState(playing) {
            isPlaying = playing;
            if (playing) {
                // State for *streaming*
                playButton.disabled = true;
                generateFullButton.disabled = true; // --- MODIFIED ---
                stopButton.disabled = false;
                spinner.classList.remove('hidden');
                playButtonText.textContent = 'Generating...';
            } else {
                // State for "idle" / "finished" / "stopped"
                playButton.disabled = false;
                generateFullButton.disabled = false; // --- MODIFIED ---
                stopButton.disabled = true;
                spinner.classList.add('hidden');
                playButtonText.textContent = 'Generate & Play';
                
                // --- NEW: Also reset full gen button ---
                generateFullSpinner.classList.add('hidden');
                generateFullButtonText.textContent = 'Generate Full Audio';
                
                streamReader = null; // Clear reader
            }
        }
        
        // === NEW: State function for full generation ===
        function setFullGeneratingState(generating) {
            if (generating) {
                // State for *full generation*
                playButton.disabled = true;
                generateFullButton.disabled = true;
                stopButton.disabled = false; // Stop button is active
                generateFullSpinner.classList.remove('hidden');
                generateFullButtonText.textContent = 'Generating...';
            } else {
                // Use the master reset function
                setPlayingState(false);
            }
        }
        // === END NEW ===

        // --- Form Submit Handler (Streaming) ---
        ttsForm.addEventListener('submit', async (event) => {
            event.preventDefault();

            // Close any existing context from a previous run
            if (audioContext && audioContext.state !== 'closed') {
                audioContext.close().then(() => console.log('Previous AudioContext closed.'));
            }
            // Hide player and reset chunks
            audioPlayerContainer.classList.add('hidden');
            if (audioPlayer.src) {
                URL.revokeObjectURL(audioPlayer.src); // Revoke old URL
            }
            audioPlayer.src = '';
            audioChunks = [];
            
            const text = textInput.value;
            if (!text.trim()) {
                showStatus('Please enter some text to generate.', 'error');
                return;
            }

            hideStatus();
            setPlayingState(true);

            // Initialize AudioContext on user interaction
            try {
                audioContext = new (window.AudioContext || window.webkitAudioContext)();
                nextStartTime = audioContext.currentTime + 0.1; // Add slight delay
                console.log('AudioContext started, state:', audioContext.state);
            } catch (e) {
                console.error('Error creating AudioContext:', e);
                showStatus('Error: Could not initialize Web Audio. ' + e.message, 'error');
                setPlayingState(false);
                return;
            }

            // --- Fetch and Process Stream ---
            try {
                // Build request object with new parameters
                const requestBody = {
                    input: text,
                    response_format: 'pcm',
                    max_length: parseInt(maxLengthInput.value),
                    cfg_value: parseFloat(cfgValueInput.value),
                    inference_timesteps: parseInt(inferenceTimestepsInput.value)
                };

                // Add voice or prompt parameters based on selection
                const selectedVoice = voiceSelect.value;
                if (selectedVoice) {
                    requestBody.voice = selectedVoice;
                } else {
                    requestBody.prompt_wav_path = promptWavPathInput.value;
                    requestBody.prompt_text = promptTextInput.value;
                }

                const response = await fetch('/v1/audio/speech/stream', {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json',
                        'Accept': 'application/octet-stream',
                    },
                    body: JSON.stringify(requestBody)
                });

                if (!response.ok) {
                    const errorBody = await response.json();
                    throw new Error(errorBody.detail || `Server error: ${response.status}`);
                }

                // Try to get sample rate from header
                const headerSampleRate = response.headers.get('X-Sample-Rate');
                if (headerSampleRate) {
                    sampleRate = parseInt(headerSampleRate, 10);
                    console.log('Using sample rate from header:', sampleRate);
                } else {
                    console.warn(`No X-Sample-Rate header found, defaulting to ${sampleRate}Hz.`);
                }
                
                streamReader = response.body.getReader();

                // Start processing the stream
                showStatus('Streaming... Audio will play shortly.', 'info');
                playButtonText.textContent = 'Playing...';
                
                processStream();

            } catch (error) {
                console.error('Fetch error:', error);
                showStatus(`Error: ${error.message}`, 'error');
                setPlayingState(false);
                if (audioContext && audioContext.state !== 'closed') {
                    audioContext.close(); // Close context on fetch error
                }
            }
        });

        // === NEW: Full Generation Button Handler ===
        generateFullButton.addEventListener('click', async (event) => {
            event.preventDefault();
            
            // --- Clear old audio ---
            if (audioContext && audioContext.state !== 'closed') {
                audioContext.close().then(() => console.log('Previous AudioContext closed.'));
            }
            audioPlayerContainer.classList.add('hidden');
            if (audioPlayer.src) {
                URL.revokeObjectURL(audioPlayer.src); // Revoke old URL
            }
            audioPlayer.src = '';
            audioChunks = []; // Clear streaming chunks too
            
            const text = textInput.value;
            if (!text.trim()) {
                showStatus('Please enter some text to generate.', 'error');
                return;
            }

            hideStatus();
            setFullGeneratingState(true); // --- Use new state function ---

            try {
                // Build request object (same as streaming)
                const requestBody = {
                    input: text,
                    response_format: 'wav', // Request a common format
                    max_length: parseInt(maxLengthInput.value),
                    cfg_value: parseFloat(cfgValueInput.value),
                    inference_timesteps: parseInt(inferenceTimestepsInput.value)
                };

                const selectedVoice = voiceSelect.value;
                if (selectedVoice) {
                    requestBody.voice = selectedVoice;
                } else {
                    requestBody.prompt_wav_path = promptWavPathInput.value;
                    requestBody.prompt_text = promptTextInput.value;
                }

                const response = await fetch('/v1/audio/speech', { // --- Call full endpoint ---
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json',
                        'Accept': 'audio/wav', // We'll ask for WAV
                    },
                    body: JSON.stringify(requestBody)
                });

                if (!response.ok) {
                    // Try to parse JSON error, fall back to status text
                    let errorMsg;
                    try {
                        const errorBody = await response.json();
                        errorMsg = errorBody.detail;
                    } catch (e) {
                        errorMsg = response.statusText;
                    }
                    throw new Error(errorMsg || `Server error: ${response.status}`);
                }

                // Get the audio file as a Blob
                const audioBlob = await response.blob();
                
                // Check if blob is empty or has error type
                if (audioBlob.size === 0) {
                    throw new Error('Received empty audio file from server.');
                }
                
                if (audioBlob.type === 'application/json') {
                     throw new Error('Server returned an error. Check server logs.');
                }

                // Create an Object URL and set it to the player
                const audioUrl = URL.createObjectURL(audioBlob);
                audioPlayer.src = audioUrl;
                audioPlayerContainer.classList.remove('hidden');
                
                showStatus('Full audio generation successful.', 'success');
                setFullGeneratingState(false); // Reset UI

            } catch (error) {
                console.error('Full generation error:', error);
                // If the fetch fails (e.g., server cancelled), this catch block will run.
                // We show the error, unless the user manually clicked stop
                // (in which case the stop handler already showed a message).
                if (isPlaying) { // isPlaying is a proxy for "was a job running"
                     showStatus(`Error: ${error.message}`, 'error');
                }
                setFullGeneratingState(false); // Reset UI on error
            }
        });
        // === END NEW ===

        // --- Stop Button Handler ---
        stopButton.addEventListener('click', async () => {
            // Cancel the stream reader first
            if (streamReader) {
                streamReader.cancel('User stopped playback.').catch(() => {});
            }
            
            // Call the server cancellation endpoint
            try {
                const response = await fetch('/v1/audio/speech/cancel', {
                    method: 'POST',
                    headers: {
                        'Content-Type': 'application/json',
                    }
                });
                
                if (response.ok) {
                    const result = await response.json();
                    console.log('Server cancellation successful:', result.message);
                } else {
                    console.error('Server cancellation failed:', response.status);
                }
            } catch (error) {
                console.error('Error calling server cancellation:', error);
            }
            
            // Close the audio context
            if (audioContext && audioContext.state !== 'closed') {
                audioContext.close().then(() => {
                    console.log('AudioContext closed by user.');
                });
                audioContext = null;
            }
            
            setPlayingState(false);
            showStatus('Generation/Playback stopped by user.', 'info');
            
            // --- MODIFIED: Player is no longer hidden on stop ---
        });

        // --- Core Stream Processing Function ---
        async function processStream() {
            if (!streamReader) return;

            try {
                const { done, value } = await streamReader.read();

                // --- MODIFICATION: Handle stream completion ---
                if (done) {
                    console.log('Stream download finished. Waiting for playback to complete.');
                    
                    if (audioContext && audioContext.state !== 'closed' && isPlaying) {
                        // 1. Build the final audio player
                        buildFinalAudio(); 

                        // 2. Calculate remaining playback time
                        const remainingTimeInSeconds = nextStartTime - audioContext.currentTime;
                        const delayInMs = Math.max(0, remainingTimeInSeconds * 1000) + 250; // 250ms buffer
                        
                        console.log(`Playback will finish in ${remainingTimeInSeconds.toFixed(2)}s. Closing context in ${delayInMs.toFixed(0)}ms.`);

                        // 3. Schedule the context closure and UI update
                        setTimeout(() => {
                            if (audioContext && audioContext.state !== 'closed') {
                                audioContext.close().then(() => console.log('AudioContext closed after finishing.'));
                                audioContext = null;
                            }
                            
                            // Update UI only if the user hasn't stopped it in the meantime
                            if (isPlaying) { 
                                showStatus('Playback finished.', 'success');
                                setPlayingState(false);
                            }
                        }, delayInMs);

                    } else {
                        // Stream finished, but context is already closed or user stopped
                        if (isPlaying) {
                            setPlayingState(false);
                        }
                    }
                    return;
                }
                // --- END MODIFICATION ---


                // --- Web Audio Magic ---
                // 'value' is a Uint8Array. We need to convert it to 16-bit PCM (Int16Array)
                // then to 32-bit float (-1.0 to 1.0) for the Web Audio API.

                // --- BUG FIX ---
                // 1. Create an Int16Array view on the *correct* buffer region
                const chunkByteLength = value.byteLength;
                const alignedByteLength = chunkByteLength - (chunkByteLength % 2);
                
                if (alignedByteLength === 0) {
                    processStream(); // Read next chunk
                    return;
                }
                
                const pcm16Data = new Int16Array(
                    value.buffer,
                    value.byteOffset,
                    alignedByteLength / 2 // Length in Int16 elements
                );
                // --- END BUG FIX ---

                // 2. Create a Float32Array and normalize the data
                const numSamples = pcm16Data.length;
                const pcm32Data = new Float32Array(numSamples);
                for (let i = 0; i < numSamples; i++) {
                    pcm32Data[i] = pcm16Data[i] / 32768.0; // Normalize from -32768/32767 to -1.0/1.0
                }
                
                // --- NEW: Store this chunk for the final audio file ---
                audioChunks.push(pcm32Data);

                // 3. Create an AudioBuffer
                if (!audioContext || audioContext.state === 'closed') {
                    console.log('AudioContext is closed, stopping stream processing.');
                    return;
                }
                const audioBuffer = audioContext.createBuffer(
                    1,             // 1 channel (mono)
                    numSamples,    // Number of samples
                    sampleRate     // Sample rate
                );
                audioBuffer.getChannelData(0).set(pcm32Data);

                // 4. Create a source node, connect it, and schedule it to play
                const source = audioContext.createBufferSource();
                source.buffer = audioBuffer;
                source.connect(audioContext.destination);

                // 5. Schedule playback
                const playTime = Math.max(nextStartTime, audioContext.currentTime);
                source.start(playTime);

                // 6. Update the start time for the *next* chunk
                nextStartTime = playTime + audioBuffer.duration;
                // -------------------------

                // Continue reading the stream
                processStream();

            } catch (error) {
                if (error.name === 'AbortError' || (error.message && error.message.includes('User stopped'))) {
                    console.log('Stream reading cancelled by user.');
                    // --- MODIFIED: Build audio from partial chunks ---
                    buildFinalAudio();
                } else {
                    console.error('Error reading stream:', error);
                    showStatus(`Stream error: ${error.message}`, 'error');
                }
                setPlayingState(false);
                if (audioContext && audioContext.state !== 'closed') {
                    audioContext.close();
                }
            }
        }
        
        // --- Function to build the final audio player source ---
        function buildFinalAudio() {
            // This function can be called even if the main context is closed,
            // as it uses an OfflineAudioContext.
            if (!audioChunks.length) { 
                console.log('No audio chunks to build final audio.');
                return;
            }

            try {
                const totalSamples = audioChunks.reduce((acc, chunk) => acc + chunk.length, 0);
                if (totalSamples === 0) {
                    console.log('No samples to build audio.');
                    return;
                }

                // Use the *offline* context to stitch buffer
                const offlineCtx = new OfflineAudioContext(1, totalSamples, sampleRate);
                const bufferSource = offlineCtx.createBufferSource();

                const finalBuffer = offlineCtx.createBuffer(1, totalSamples, sampleRate);
                let offset = 0;
                for (const chunk of audioChunks) {
                    finalBuffer.getChannelData(0).set(chunk, offset);
                    offset += chunk.length;
                }

                bufferSource.buffer = finalBuffer;
                bufferSource.connect(offlineCtx.destination);
                bufferSource.start();

                offlineCtx.startRendering().then(renderedBuffer => {
                    const wavBlob = bufferToWave(renderedBuffer);
                    const audioUrl = URL.createObjectURL(wavBlob);
                    
                    audioPlayer.src = audioUrl;
                    audioPlayerContainer.classList.remove('hidden');
                }).catch(e => {
                    console.error('Error rendering final audio:', e);
                    showStatus('Could not render final audio file.', 'error');
                });

            } catch (e) {
                console.error('Error building final audio:', e);
                showStatus('Could not build final audio file.', 'error');
            }
        }
        
        // --- Function to convert an AudioBuffer to a WAV file (Blob) ---
        function bufferToWave(audioBuffer) {
            const numOfChan = audioBuffer.numberOfChannels;
            const sampleRate = audioBuffer.sampleRate;
            const length = audioBuffer.length * numOfChan * 2 + 44;
            const buffer = new ArrayBuffer(length);
            const view = new DataView(buffer);
            const channels = [];
            let i, sample;
            let offset = 0;

            for (i = 0; i < numOfChan; i++) {
                channels.push(audioBuffer.getChannelData(i));
            }

            // Write WAV header
            writeString(view, offset, 'RIFF'); offset += 4; // "RIFF"
            view.setUint32(offset, length - 8, true); offset += 4; // file length - 8
            writeString(view, offset, 'WAVE'); offset += 4; // "WAVE"
            writeString(view, offset, 'fmt '); offset += 4; // "fmt "
            view.setUint32(offset, 16, true); offset += 4; // chunk size
            view.setUint16(offset, 1, true); offset += 2; // format (1 = PCM)
            view.setUint16(offset, numOfChan, true); offset += 2; // num channels
            view.setUint32(offset, sampleRate, true); offset += 4; // sample rate
            view.setUint32(offset, sampleRate * 2 * numOfChan, true); offset += 4; // byte rate
            view.setUint16(offset, numOfChan * 2, true); offset += 2; // block align
            view.setUint16(offset, 16, true); offset += 2; // bits per sample
            writeString(view, offset, 'data'); offset += 4; // "data"
            view.setUint32(offset, length - 44, true); offset += 4; // data chunk size

            // Write PCM data
            for (i = 0; i < audioBuffer.length; i++) {
                for (let chan = 0; chan < numOfChan; chan++) {
                    sample = Math.max(-1, Math.min(1, channels[chan][i]));
                    sample = (sample < 0 ? sample * 0x8000 : sample * 0x7FFF) | 0;
                    view.setInt16(offset, sample, true);
                    offset += 2;
                }
            }

            return new Blob([view], { type: 'audio/wav' });
        }

        function writeString(view, offset, string) {
            for (let i = 0; i < string.length; i++) {
                view.setUint8(offset + i, string.charCodeAt(i));
            }
        }

    </script>
</body>
</html>