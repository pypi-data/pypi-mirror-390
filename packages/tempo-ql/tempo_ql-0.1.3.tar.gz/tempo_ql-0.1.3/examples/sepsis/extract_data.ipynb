{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3072ff7-eae6-4ca3-a23f-53020abe71c7",
   "metadata": {},
   "source": [
    "# Sepsis EHR Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c687ca37-aaa1-4d71-b138-791e1360f82e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from tempo_ql import QueryEngine, FileVariableStore, MEDSDataset, GenericDataset, formats, TimeSeriesSet\n",
    "from google.cloud import bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eec2578-ff77-456b-93e1-b22831fa21b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_name = 'eicu' # or 'mimiciv' or 'ehrshot'\n",
    "data_path = f'data/{dataset_name}' \n",
    "if not os.path.exists(data_path): os.makedirs(data_path)\n",
    "gemini_key = open('../../gemini_key.txt').read().strip() # replace with your Gemini key\n",
    "\n",
    "reset_cohort = True # set to True to remove cohort definition if you've already created one using this script\n",
    "\n",
    "# GCP project in which to run queries - make sure it has access to MIMIC-IV through physionet.org\n",
    "project_id = \"ai-clinician\"\n",
    "bq_client = bigquery.Client(project=project_id)\n",
    "\n",
    "# name of a dataset within your project to store temporary results.\n",
    "scratch_dataset = f\"{project_id}.tempo_ql_scratch_\" + dataset_name\n",
    "try:\n",
    "    bq_client.get_dataset(scratch_dataset)\n",
    "except:\n",
    "    print(\"Creating scratch dataset\")\n",
    "    bq_client.create_dataset(scratch_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad05e676-d8c5-48ee-b433-efbf233cd9bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(data_path): os.mkdir(data_path)\n",
    "\n",
    "var_store = FileVariableStore(os.path.join(data_path, '_cache'))\n",
    "\n",
    "if dataset_name == 'eicu':\n",
    "    dataset = GenericDataset(f'bigquery://{project_id}', formats.eicu(), \n",
    "                         scratch_schema_name=scratch_dataset, \n",
    "                         time_field_transform=lambda x: x * 60)\n",
    "    if reset_cohort: dataset.reset_trajectory_ids()\n",
    "\n",
    "    if not os.path.exists(data_path): os.mkdir(data_path)\n",
    "    query_engine = QueryEngine(dataset, variable_stores=[var_store])\n",
    "elif dataset_name == 'mimiciv':\n",
    "    dataset = GenericDataset(f'bigquery://{project_id}', formats.mimiciv(), \n",
    "                         scratch_schema_name=scratch_dataset)\n",
    "    if reset_cohort: dataset.reset_trajectory_ids()\n",
    "\n",
    "    if not os.path.exists(data_path): os.mkdir(data_path)\n",
    "    query_engine = QueryEngine(dataset, variable_stores=[var_store])\n",
    "elif dataset_name == 'ehrshot':\n",
    "    dataset = MEDSDataset(os.path.join(data_path, \"data/*.parquet\"), os.path.join(data_path, \"metadata/*.parquet\"),\n",
    "                          connection_string='duckdb:///' + os.path.join(data_path, 'variables.db'))\n",
    "    if reset_cohort: dataset.reset_trajectory_ids()\n",
    "\n",
    "    query_engine = QueryEngine(dataset, variable_stores=[var_store])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7d65d3-7e7a-423a-a62a-b5ebbeee713a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Cohort Definition\n",
    "\n",
    "We want to select patients who have an antibiotic and a culture taken within 24 hours, or a diagnosis code with sepsis. We also want to exclude patients under 18 years old and patients who are in the ICU for at least 4 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d9d881-5146-4631-bffe-2a2e60cb7c7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_engine.interactive(file_path='test.json', api_key=gemini_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec04ca11-6c45-4928-8c10-af84734aafb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Uncomment to start interactive widget to edit cohorts\n",
    "# query_engine.interactive(file_path=f'queries/cohort_{dataset_name}.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681a75b8-0c01-4001-adf7-22d787c8912e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cohort_information = query_engine.query_from(f'queries/cohort_{dataset_name}.json', \n",
    "                                             variable_store=var_store,\n",
    "                                             show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11d6002-badd-4d1a-b989-fbceca8f5721",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset.set_trajectory_ids_where(cohort_information['Cohort'])\n",
    "print(\"Filtered to\", len(dataset.get_ids()), \"IDs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3004f5da-c551-436d-b590-2b364bd28fdc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Signal Extraction\n",
    "\n",
    "In this stage we extract consolidated concepts for each of the variables we are ultimately interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d36efd-4563-4d90-87e1-eeab5be3b126",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Uncomment to start interactive widget to edit extracted data\n",
    "# query_engine.interactive(file_path=f'queries/extraction_{dataset_name}.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bb6ccc-6407-4015-b577-bd55cc31ee5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_engine.query_from(f'queries/extraction_{dataset_name}.json', \n",
    "                        variable_store=var_store, \n",
    "                        show_progress=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d90cb66-2f4e-4db8-b560-9e106aadabca",
   "metadata": {},
   "source": [
    "# Modeling Features\n",
    "\n",
    "Finally, we aggregate the features using a timestep definition: every 4 hours from either admission or sepsis onset to discharge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6218c58-9f2b-4f60-8e1a-0454bf4207d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_features = query_engine.query_from(f'queries/model_features.json', \n",
    "                                         show_progress=True,\n",
    "                                         query_transform=lambda _, query: f\"({query}) every 4 h from ((SepsisOnset where #value < Discharge) impute Admission) to min(Discharge, Admission + 14 days)\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae2eb65-5a6f-4083-9021-9bc52cb5e1c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tempo_ql.data_types import TimeSeriesSet\n",
    "\n",
    "# Write to file in a consistent order\n",
    "feature_names = sorted(model_features.keys())\n",
    "df = TimeSeriesSet.from_series([model_features[k].rename(k) for k in feature_names]).serialize()[1]\n",
    "df.assign(**{df.columns[0]: df[df.columns[0]].astype(int)}).rename(columns={df.columns[0]: 'id', df.columns[1]: 'timestep'}).to_csv(os.path.join(data_path, \"extracted_model_features.csv\"), index=False, float_format='%.4g')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fe913a-3376-44ec-9e6b-7f0805c68681",
   "metadata": {},
   "source": [
    "# Downstream Targets\n",
    "\n",
    "Here we define the variables used as predictive targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891ccc93-80cb-43df-bb20-6def1a9ea641",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query_engine.interactive(file_path='queries/predictive_targets.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45bc535c-7bc4-4653-b18e-5ccc5973f229",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "targets = query_engine.query_from(\"queries/predictive_targets.json\", show_progress=True)\n",
    "df = TimeSeriesSet.from_series([targets[k].rename(k) for k in targets]).serialize()[1]\n",
    "df.assign(**{df.columns[0]: df[df.columns[0]].astype(int)}).rename(columns={df.columns[0]: 'id', df.columns[1]: 'timestep'}).to_csv(os.path.join(data_path, \"predictive_targets.csv\"), index=False, float_format='%.4g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555c30f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sepsis-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
