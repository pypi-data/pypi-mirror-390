# src/civic_geo_generator/index.py
"""Generate index files and summary metadata in data-out/.

Outputs:
- data-out/index.json
    Flat listing of all GeoJSONs with bbox and feature counts (relative paths).

- data-out/manifest.json
    Dataset-level manifest (generic).

- data-out/us/<state>/index.json
    For each state under data-out/us/, a list of views and a pointer
    to the latest version's metadata.json.

CLI:
  uv run python -m civic_geo_generator.index
"""

import json
from pathlib import Path
from typing import Any

from civic_lib_core import date_utils, log_utils
import geopandas as gpd

# Single source of truth for filesystem layout
from civic_geo_generator.utils import paths

logger = log_utils.logger


class IndexBuildError(Exception):
    """Raised for errors encountered during index building."""


def _inspect_geojson(geojson_path: Path) -> tuple[list[float] | None, int | None]:
    """Return (bbox, feature_count) for a GeoJSON file, or (None, None) if unreadable.

    bbox format: [minx, miny, maxx, maxy] rounded to 6 decimals.
    """
    try:
        gdf: gpd.GeoDataFrame = gpd.read_file(geojson_path)
        minx, miny, maxx, maxy = [float(x) for x in gdf.total_bounds]
        bbox = [round(minx, 6), round(miny, 6), round(maxx, 6), round(maxy, 6)]
        return bbox, int(len(gdf))
    except Exception as exc:
        logger.warning(f"Could not read {geojson_path.name} for bbox/count: {exc}")
        return None, None


def _write_manifest(out_dir: Path, index_data: list[dict[str, Any]]) -> Path:
    """Write dataset-level manifest.json at data-out/."""
    total_files = len(index_data)
    total_features = sum(int(d.get("features") or 0) for d in index_data)

    manifest: dict[str, Any] = {
        "dataset": "civic-boundaries",
        "description": "Boundary layers generated by civic-geo-generator.",
        "license": "Public domain (verify per-source in individual metadata).",
        "generated_at": date_utils.now_utc_str_for_schemas(),
        "total_files": total_files,
        "total_features": total_features,
        "files_indexed": [d["path"] for d in index_data],
    }
    p = out_dir / "manifest.json"
    p.write_text(json.dumps(manifest, indent=2), encoding="utf-8")
    logger.info(f"Manifest written: {p}")
    return p


def _latest_version_folder(view_dir: Path) -> Path | None:
    """Return latest version folder under a view folder by descending name sort.

    view_dir: data-out/us/<state>/<view>
    """
    if not view_dir.exists():
        return None
    versions = sorted(
        (p for p in view_dir.iterdir() if p.is_dir()), key=lambda p: p.name, reverse=True
    )
    return versions[0] if versions else None


def _derive_layer_id(meta: dict[str, Any] | None, state_slug: str, view_key: str) -> str:
    """Produce a stable layer id.

    Preference order:
      1) metadata["layer_id"] if present
      2) "{state_abbr_lc}-{view}" if metadata has "state" like "mn"
      3) "{state_slug}-{view}" as last resort (state_slug is directory name)
    """
    if isinstance(meta, dict):
        lid = meta.get("layer_id")
        if isinstance(lid, str) and lid.strip():
            return lid.strip()

        st = meta.get("state")
        if isinstance(st, str) and len(st) in (2, 3):  # tolerate "mn" or similar short codes
            return f"{st.lower()}-{view_key}"

    return f"{state_slug}-{view_key}"


def _read_metadata(meta_path: Path) -> dict[str, Any] | None:
    """Load metadata.json if present and valid."""
    try:
        if meta_path.exists():
            return json.loads(meta_path.read_text(encoding="utf-8"))
    except Exception as exc:
        logger.warning(f"Failed to read {meta_path}: {exc}")
    return None


def _write_state_index(out_root: Path, state_slug: str) -> Path | None:
    """Generate data-out/us/<state>/index.json listing latest metadata per view.

    Scans data-out/us/<state>/*, finds latest version folders for each view,
    and writes pointers to their metadata.json.
    """
    state_root = out_root / "us" / state_slug
    if not state_root.exists():
        logger.warning(f"State directory missing, skipping: {state_root}")
        return None

    layers: list[dict[str, Any]] = []
    for view_dir in sorted(p for p in state_root.iterdir() if p.is_dir()):
        latest = _latest_version_folder(view_dir)
        if not latest:
            continue

        meta_path = latest / "metadata.json"
        if not meta_path.exists():
            logger.warning(f"Missing metadata.json for {view_dir.name} at {latest}")
            continue

        meta = _read_metadata(meta_path)
        view_key = view_dir.name
        layer_id = _derive_layer_id(meta, state_slug, view_key)
        rel_meta = meta_path.relative_to(out_root).as_posix()
        layers.append({"id": layer_id, "latest": rel_meta})

    if not layers:
        logger.warning(f"No latest layer versions found under {state_root}")
        return None

    state_index = {"layers": layers}
    out_path = state_root / "index.json"
    out_path.write_text(json.dumps(state_index, indent=2), encoding="utf-8")
    logger.info(f"State index written: {out_path}")
    return out_path


def index_main() -> int:
    """Scan data-out for GeoJSON, compute metadata, and write indexes.

    Workflow:
      1) Scan the output directory for all GeoJSON files.
      2) For each file, compute bbox and feature count.
      3) Write a flat index of all files to data-out/index.json.
      4) Write a dataset-level manifest to data-out/manifest.json.
      5) For each state under data-out/us/, write data-out/us/<state>/index.json
         with latest version pointers per view.
    """
    try:
        out_root = paths.get_data_out_dir()
        flat_index: list[dict[str, Any]] = []

        logger.info(f"Scanning {out_root} for GeoJSONs...")
        for geojson in out_root.rglob("*.geojson"):
            bbox, nfeat = _inspect_geojson(geojson)
            flat_index.append(
                {
                    "path": geojson.relative_to(out_root).as_posix(),
                    "bbox": bbox,
                    "features": nfeat,
                }
            )

        # data-out/index.json
        index_file = out_root / "index.json"
        index_file.write_text(json.dumps(flat_index, indent=2), encoding="utf-8")
        logger.info(f"Flat index written: {index_file} ({len(flat_index)} files)")

        # data-out/manifest.json (dataset-level)
        _write_manifest(out_root, flat_index)

        # Per-state indexes (latest per view)
        states_root = out_root / "us"
        if states_root.exists():
            for state_dir in sorted(p for p in states_root.iterdir() if p.is_dir()):
                _write_state_index(out_root, state_dir.name)

        return 0

    except Exception as exc:
        logger.error(f"Index build failed: {exc}")
        return 1


def main() -> int:
    """Entry point for building indexes."""
    try:
        return index_main()
    except Exception as exc:
        logger.error(f"Index command failed unexpectedly: {exc}")
        return 1


if __name__ == "__main__":
    import sys

    sys.exit(main())
