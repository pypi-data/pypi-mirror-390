import asyncio
import logging
from types import SimpleNamespace
from typing import Any, List, Tuple

logger = logging.getLogger("ketacli.textual")


def safe_notify(app, message: str, severity: str = None, timeout: int = None, **kwargs) -> None:
    try:
        notify = getattr(app, "notify", None)
        if callable(notify):
            if severity is not None:
                kwargs["severity"] = severity
            if timeout is not None:
                kwargs["timeout"] = timeout
            notify(message, **kwargs)
    except Exception:
        pass


async def stream_chat_with_tools_async(app: Any, messages: List[dict], tools: List[dict]):
    """å¼‚æ­¥æµå¼èŠå¤©ï¼ˆæ”¯æŒå·¥å…·è°ƒç”¨ï¼‰ï¼šå®æ—¶æ¨é€chunkï¼Œé¿å…ç­‰å¾…å…¨éƒ¨å®Œæˆã€‚

    ä½¿ç”¨å¼‚æ­¥é˜Ÿåˆ—å®ç°æµå¼èŠå¤©åŠŸèƒ½ï¼Œå°†AIå“åº”å®æ—¶æ¨é€ç»™ç”¨æˆ·ç•Œé¢ï¼Œ
    æ— è®ºæ˜¯æ™®é€šæ–‡æœ¬è¿˜æ˜¯å·¥å…·è°ƒç”¨ç»“æœï¼Œéƒ½é€šè¿‡æµå¼æ–¹å¼è¿”å›ã€‚
    """
    loop = asyncio.get_running_loop()
    queue: asyncio.Queue = asyncio.Queue()
    done = asyncio.Event()

    # å¯åŠ¨å‰è®°å½•ä¸€äº›ä¸Šä¸‹æ–‡ä¿¡æ¯
    try:
        provider = getattr(app.ai_client.model_config, "provider", "")
        current_model = app.ai_client.get_current_model() if hasattr(app.ai_client, "get_current_model") else ""
        logger.debug(f"[stream_tools] å¯åŠ¨: provider={provider}, model={current_model}, messages_len={len(messages) if isinstance(messages, list) else 1}")
    except Exception:
        pass

    # æ–°å¢ï¼šå¯åŠ¨è°ƒè¯•é€šçŸ¥
    safe_notify(app, "ğŸ”„ å¯åŠ¨æµå¼å·¥å…·è¯·æ±‚", timeout=2)

    def producer():
        try:
            for chunk in app.ai_client.stream_chat(messages, tools=tools, tool_choice="auto"):
                # è§£æchunkï¼Œè½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼
                if isinstance(chunk, str):
                    processed_chunk = {"content": chunk}
                elif isinstance(chunk, dict):
                    processed_chunk = chunk
                else:
                    try:
                        processed_chunk = {"content": str(chunk)}
                    except Exception:
                        processed_chunk = {"content": "æ— æ³•è§£æçš„å†…å®¹"}
                asyncio.run_coroutine_threadsafe(queue.put(processed_chunk), loop)
        except Exception as e:
            logger.error(f"[stream_tools] ç”Ÿäº§è€…å¼‚å¸¸: {type(e).__name__}: {e}")
            asyncio.run_coroutine_threadsafe(queue.put({"content": f"\n[æµå¼é”™è¯¯] {e}"}), loop)
        finally:
            asyncio.run_coroutine_threadsafe(done.set(), loop)

    # å¯åŠ¨ç”Ÿäº§è€…çº¿ç¨‹
    import threading
    threading.Thread(target=producer, daemon=True).start()

    # æ–°å¢ï¼šç»Ÿè®¡æ”¶åˆ°çš„ç‰‡æ®µæ•°é‡
    received_chunks = 0

    # æ¶ˆè´¹è€…ï¼šå¼‚æ­¥è·å–é˜Ÿåˆ—ä¸­çš„chunkå¹¶å¤„ç†
    while not done.is_set() or not queue.empty():
        try:
            chunk = await asyncio.wait_for(queue.get(), timeout=0.1)
            received_chunks += 1
            yield chunk
        except asyncio.TimeoutError:
            continue
        except Exception as e:
            logger.error(f"[stream_tools] æ¶ˆè´¹è€…å¼‚å¸¸: {type(e).__name__}: {e}")
            yield {"content": f"\n[æµå¼å¤„ç†é”™è¯¯] {e}"}
            break

    # æ–°å¢ï¼šç»“æŸè°ƒè¯•é€šçŸ¥
    safe_notify(app, f"ğŸ“¤ æµå¼ç»“æŸï¼šæ”¶åˆ° {received_chunks} ä¸ªç‰‡æ®µ", timeout=3)
    if received_chunks == 0:
        safe_notify(app, "âš ï¸ æµå¼æ¥å£æœªè¿”å›ä»»ä½•æ•°æ®ï¼ˆå†…å®¹/å·¥å…·è°ƒç”¨å‡ä¸ºç©ºï¼‰", severity="warning", timeout=5)


async def stream_chat_async(app: Any, messages: List[dict]):
    """å¼‚æ­¥æµå¼èŠå¤©ï¼šå®æ—¶æ¨é€chunkï¼Œé¿å…ç­‰å¾…å…¨éƒ¨å®Œæˆã€‚"""
    loop = asyncio.get_running_loop()
    queue: asyncio.Queue = asyncio.Queue()
    done = asyncio.Event()
    yield_count = 0

    # å¯åŠ¨å‰è®°å½•ä¸€äº›ä¸Šä¸‹æ–‡ä¿¡æ¯
    try:
        provider = getattr(app.ai_client.model_config, "provider", "")
        current_model = app.ai_client.get_current_model() if hasattr(app.ai_client, "get_current_model") else ""
        logger.debug(f"[stream] å¯åŠ¨: provider={provider}, model={current_model}, messages_len={len(messages) if isinstance(messages, list) else 1}")
    except Exception:
        pass

    def producer():
        try:
            for chunk in app.ai_client.stream_chat(messages):
                asyncio.run_coroutine_threadsafe(queue.put(chunk), loop)
        except Exception as e:
            logger.error(f"[stream] ç”Ÿäº§è€…å¼‚å¸¸: {type(e).__name__}: {e}")
            asyncio.run_coroutine_threadsafe(queue.put(f"\n[æµå¼é”™è¯¯] {e}"), loop)
        finally:
            loop.call_soon_threadsafe(done.set)

    # åœ¨çº¿ç¨‹ä¸­è¿è¡ŒåŒæ­¥çš„æµå¼ç”Ÿæˆå™¨
    producer_task = asyncio.create_task(asyncio.to_thread(producer))

    try:
        while True:
            if done.is_set() and queue.empty():
                break
            try:
                chunk = await asyncio.wait_for(queue.get(), timeout=0.5)
                yield_count += 1
                yield chunk
            except asyncio.TimeoutError:
                if done.is_set():
                    break
    finally:
        logger.debug(f"[stream] ç»“æŸ: yielded={yield_count}, done={done.is_set()}, queue_empty={queue.empty()}")
        await producer_task


async def stream_and_process_response(app: Any, messages: List[dict], tools: List[dict], streaming_widget) -> Tuple[SimpleNamespace, List[dict]]:
    """æµå¼è·å–å¹¶å¤„ç†AIå“åº”ï¼Œè¿”å›å“åº”å¯¹è±¡å’Œè¿‡æ»¤åçš„å·¥å…·è°ƒç”¨åˆ—è¡¨ã€‚"""
    accumulated_content = ""
    tool_calls_buffer: List[dict] = []
    is_tool_call = False

    # æ–°å¢ï¼šå¼€å§‹è°ƒè¯•é€šçŸ¥
    safe_notify(app, "â³ æ­£åœ¨æµå¼è·å–æ¨¡å‹å“åº”...", timeout=2)

    # å¯åŠ¨æµå¼å¤„ç†
    async for chunk in stream_chat_with_tools_async(app, messages, tools):
        # æ–‡æœ¬å†…å®¹
        if "content" in chunk and chunk.get("content"):
            content = chunk.get("content", "")
            accumulated_content += content
            try:
                streaming_widget.append_content(content)
            except Exception:
                pass

        # å·¥å…·è°ƒç”¨
        if "tool_calls" in chunk and chunk.get("tool_calls"):
            is_tool_call = True
            for tool_call in chunk.get("tool_calls", []):
                if tool_call not in tool_calls_buffer:
                    tool_calls_buffer.append(tool_call)
            try:
                streaming_widget.append_content("æ­£åœ¨å¤„ç†å·¥å…·è°ƒç”¨...\n")
            except Exception:
                pass

    # æ„é€ å“åº”å¯¹è±¡
    response = SimpleNamespace()
    response.content = accumulated_content
    response.tool_calls = tool_calls_buffer if is_tool_call else []

    # æ–°å¢ï¼šè‹¥æµå¼æœªè¿”å›ç»“æ„åŒ–tool_callsï¼Œä½†æ–‡æœ¬ä¸­åŒ…å«æ ‡è®°ï¼Œåˆ™å›é€€è§£æ
    try:
        if (not response.tool_calls) and (response.content or "").strip():
            parsed = app.ai_client._parse_tool_calls_from_text(response.content)
            if parsed:
                response.tool_calls = parsed
                is_tool_call = True
    except Exception:
        pass

    try:
        content_len = len(response.content or "")
        enabled_attr = getattr(app, "enabled_tools", None)
        if isinstance(enabled_attr, dict):
            enabled_names = set(enabled_attr.keys())
        elif isinstance(enabled_attr, (set, list, tuple)):
            enabled_names = set(enabled_attr)
        else:
            enabled_names = set()
        filtered_tool_calls = [
            tc for tc in (response.tool_calls or [])
            if ((tc or {}).get("function", {}).get("name") in enabled_names)
        ]
        tc_len = len(filtered_tool_calls)
        # æ–°å¢ï¼šç¦ç”¨è¿‡æ»¤ç»Ÿè®¡ä¸ç©ºå“åº”æé†’
        unfiltered_len = len(response.tool_calls or [])
        disabled_count = max(unfiltered_len - tc_len, 0)
        safe_notify(app, f"ğŸ“© æ”¶åˆ°å“åº”ï¼šå†…å®¹é•¿åº¦ {content_len}ï¼Œè§£æåˆ°å·¥å…·è°ƒç”¨ {tc_len} ä¸ªï¼ˆå·²æŒ‰å¯ç”¨å·¥å…·è¿‡æ»¤ï¼‰", timeout=3)
        if disabled_count > 0:
            safe_notify(app, f"ğŸ§© æ¨¡å‹æä¾› {unfiltered_len} æ¬¡å·¥å…·è°ƒç”¨ï¼Œå…¶ä¸­ {disabled_count} æ¬¡è¢«ç¦ç”¨è¿‡æ»¤", timeout=4)
        if content_len == 0 and tc_len == 0:
            safe_notify(app, "âš ï¸ æ¨¡å‹æœªè¿”å›æ–‡æœ¬æˆ–å·¥å…·è°ƒç”¨ï¼ŒæŒ‰ç©ºå“åº”å¤„ç†", severity="warning", timeout=5)
    except Exception:
        filtered_tool_calls = response.tool_calls or []

    logger.debug(f"[stream] å®Œæˆæµå¼å“åº”ï¼šcontent_len={len(response.content or '')}ï¼Œtool_calls={len(filtered_tool_calls or [])}")
    return response, filtered_tool_calls


def augment_system_prompt(base: str) -> str:
    """å°†ç³»ç»Ÿçº§å·¥å…·è°ƒç”¨è§„åˆ™æ³¨å…¥åˆ°åŸºç¡€æç¤ºè¯ä¸­ï¼Œé¿å…é‡å¤è¿½åŠ ã€‚

    å¹‚ç­‰ç­–ç•¥ï¼šè‹¥åŸºç¡€æç¤ºè¯å·²åŒ…å«è§„åˆ™é”šç‚¹ï¼ˆé¦–è¡Œæˆ–å…³é”®è¡Œï¼‰ï¼Œåˆ™ä¸å†äºŒæ¬¡è¿½åŠ ã€‚
    """
    rules = (
        "ä½ æ˜¯ä¸€ä¸ªå¯é çš„åŠ©æ‰‹ï¼Œå·¥å…·è°ƒç”¨ç­–ç•¥ä¸ºè‡ªåŠ¨æ¨¡å¼ã€‚\n"
        "å½“ä½ å†³å®šè°ƒç”¨å·¥å…·æ—¶ï¼Œè¯·éµå¾ªä»¥ä¸‹è§„åˆ™ï¼š\n"
        "1) æ¯è½®è‡³å¤šè°ƒç”¨ä¸€ä¸ªå·¥å…·ï¼›è°ƒç”¨åå¿…é¡»åŸºäºå·¥å…·ç»“æœè¿›è¡Œåˆ†æå¹¶è¾“å‡ºæ˜ç¡®æ–‡æœ¬ç»“è®ºã€‚\n"
        "2) é¿å…è¿ç»­å¤šæ¬¡é‡å¤è°ƒç”¨åŒä¸€å·¥å…·æˆ–åœ¨å·²å…·å¤‡è¶³å¤Ÿä¿¡æ¯æ—¶å†æ¬¡è°ƒç”¨ã€‚\n"
        "3) å¦‚å·¥å…·è°ƒç”¨å¤±è´¥ï¼Œè¯·è§£é‡ŠåŸå› å¹¶ç»™å‡ºæ›¿ä»£æ–¹æ¡ˆæˆ–æ¨æ–­ç»“è®ºã€‚\n"
        "4) ä»…åœ¨æ˜ç¡®ç¼ºå°‘ä¿¡æ¯ä¸”æ— æ³•äº§å‡ºç»“è®ºæ—¶ï¼Œæ‰å†æ¬¡è€ƒè™‘è°ƒç”¨å·¥å…·ã€‚\n"
        "5) ä¸è¦è¿”å›åªæœ‰å·¥å…·è°ƒç”¨è€Œæ²¡æœ‰ä»»ä½•æ–‡æœ¬å†…å®¹çš„å“åº”ã€‚\n"
        "6) è‹¥ä»»åŠ¡æ¶‰åŠæœªæ˜ç¡®æšä¸¾çš„å¤æ•°å¯¹è±¡ï¼ˆå¦‚â€˜å„ä¸ªæœåŠ¡/æ¯å°ä¸»æœºâ€™ï¼‰ï¼Œå…ˆè¿›è¡Œä¿¡æ¯å‘ç°å¹¶æšä¸¾å¯¹è±¡ã€‚\n"
        "7) å¯¹äºæŸ¥è¯¢/æ£€ç´¢/æ—¥å¿—ç±»ä»»åŠ¡ï¼Œå¿…é¡»ä½¿ç”¨æä¾›çš„å·¥å…·å®Œæˆæ£€ç´¢ï¼Œç¦æ­¢ç¼–é€ æˆ–æ¨¡æ‹Ÿæ•°æ®ç»“æœã€‚\n"
        "8) è‹¥å·¥å…·è¿”å›ç©ºç»“æœã€å‡ºç°â€˜æœªæŸ¥è¯¢åˆ°æ•°æ®/æœªæ‰¾åˆ°â€™æˆ–é»„è‰²æç¤ºï¼ˆä¾‹å¦‚æç¤ºæ”¾å®½æ¡ä»¶ï¼‰ï¼Œå¿…é¡»ä¿®æ”¹å‚æ•°åé‡è¯•ï¼Œ\n"
        "   ä¸å¾—å¤ç”¨ä¸ä¸Šä¸€è½®å®Œå…¨ç›¸åŒçš„å‚æ•°ã€‚ä¼˜å…ˆç­–ç•¥ï¼šæ‰©å¤§æ—¶é—´èŒƒå›´ï¼ˆå¦‚ -2h/-6hï¼‰ã€å°†ç²¾ç¡®åŒ¹é…æ”¹ä¸º like/containsã€\n"
        "   å‡å°‘ where è¿‡æ»¤ã€å¿…è¦æ—¶ç§»é™¤è¿‡ä¸¥çš„ origin è¿‡æ»¤ã€‚æ¯æ¬¡é‡è¯•å¿…é¡»åœ¨æ–‡æœ¬æœ«æ ‡æ³¨ [STEP_RETRY]ï¼Œå¹¶ç®€è¦è¯´æ˜å‚æ•°ä¸ä¸Šä¸€è½®çš„å·®å¼‚ã€‚\n"
        "9) è¿ç»­ä¸¤æ¬¡é‡è¯•ä»ä¸ºç©ºæ—¶ï¼Œåº”åœæ­¢é‡è¯•ï¼Œç»™å‡ºåŸå› åˆ†æä¸åç»­å»ºè®®ï¼ˆä¾‹å¦‚å­—æ®µä¸å­˜åœ¨ã€æ•°æ®è½åº“å»¶è¿Ÿã€ä»“åº“é€‰æ‹©ä¸å½“ç­‰ï¼‰ã€‚\n"
    )
    try:
        base_str = base or ""
    except Exception:
        base_str = ""
    anchor1 = "ä½ æ˜¯ä¸€ä¸ªå¯é çš„åŠ©æ‰‹ï¼Œå·¥å…·è°ƒç”¨ç­–ç•¥ä¸ºè‡ªåŠ¨æ¨¡å¼ã€‚"
    anchor2 = "å½“ä½ å†³å®šè°ƒç”¨å·¥å…·æ—¶ï¼Œè¯·éµå¾ªä»¥ä¸‹è§„åˆ™ï¼š"
    if (anchor1 in base_str) or (anchor2 in base_str):
        # å·²åŒ…å«è§„åˆ™é”šç‚¹ï¼Œé¿å…é‡å¤è¿½åŠ 
        return base_str
    return f"{base_str}\n\n{rules}".strip()


def sanitize_tool_messages(messages: list, provider: str) -> list:
    try:
        prov = (provider or "").lower()
    except Exception:
        prov = ""
    if "openai" not in prov:
        return messages
    sanitized = []
    allowed_ids = set()
    for msg in messages:
        role = msg.get("role")
        if role == "assistant":
            tool_calls = msg.get("tool_calls") or []
            allowed_ids = set([
                tc.get("id") for tc in tool_calls if isinstance(tc, dict)
            ])
            sanitized.append(msg)
        elif role == "tool":
            tool_id = msg.get("tool_call_id")
            if tool_id and tool_id in allowed_ids:
                sanitized.append(msg)
        else:
            allowed_ids = set()
            sanitized.append(msg)
    return sanitized


def enforce_openai_tool_sequence(msgs: list) -> tuple[list, int, int]:
    sanitized = []
    removed_assistant = 0
    removed_tool = 0
    i = 0
    n = len(msgs)
    while i < n:
        msg = msgs[i]
        role = msg.get("role")
        if role != "assistant":
            if role == "tool":
                removed_tool += 1
            else:
                sanitized.append(msg)
            i += 1
            continue
        tool_calls = msg.get("tool_calls") or []
        if not tool_calls:
            sanitized.append(msg)
            i += 1
            continue
        ids = [tc.get("id") for tc in tool_calls if isinstance(tc, dict) and tc.get("id")]
        j = i + 1
        matched_ids = []
        collected_tools = []
        while j < n and msgs[j].get("role") == "tool":
            tcid = msgs[j].get("tool_call_id")
            if tcid in ids:
                matched_ids.append(tcid)
                collected_tools.append(msgs[j])
            j += 1
        if set(matched_ids) == set(ids):
            sanitized.append(msg)
            sanitized.extend(collected_tools)
            i = j
        else:
            removed_assistant += 1
            i = j
    return sanitized, removed_assistant, removed_tool


async def process_tool_sequence(app, messages: list) -> tuple:
    try:
        msgs, ra, rt = enforce_openai_tool_sequence(messages)
        if ra or rt:
            safe_notify(
                app,
                f"ğŸ§© è¯·æ±‚å‰å·²è§„èŒƒåŒ–ï¼šç§»é™¤ä¸å®Œæ•´assistant {ra} æ¡/å­¤ç«‹tool {rt} æ¡",
                severity="warning",
                timeout=3
            )
        last_assistant_idx = None
        for idx in range(len(msgs)-1, -1, -1):
            if msgs[idx].get("role") == "assistant":
                last_assistant_idx = idx
                break
        if last_assistant_idx is not None:
            ids = [tc.get("id") for tc in msgs[last_assistant_idx].get("tool_calls") or [] if isinstance(tc, dict) and tc.get("id")]
            j = last_assistant_idx + 1
            following = 0
            while j < len(msgs) and msgs[j].get("role") == "tool":
                following += 1
                j += 1
            if ids:
                safe_notify(app, f"ğŸ”— æœ€è¿‘assistantå·¥å…·è°ƒç”¨: {len(ids)}ï¼Œåç»­toolæ¶ˆæ¯: {following}", timeout=3)
        return msgs, ra, rt
    except Exception:
        safe_notify(app, "ğŸ”§ è§£ææœ€è¿‘assistantå·¥å…·è°ƒç”¨å¤±è´¥", severity="warning", timeout=3)
        return messages, 0, 0