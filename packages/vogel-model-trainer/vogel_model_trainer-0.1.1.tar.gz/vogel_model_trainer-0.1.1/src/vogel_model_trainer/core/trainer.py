#!/usr/bin/env python3
"""
Training script for custom bird species classifier.
Fine-tunes an EfficientNet model on the extracted bird images.
"""

import os
import warnings
import torch
import signal
import sys
from pathlib import Path
from datetime import datetime
import logging

# Suppress specific warnings
warnings.filterwarnings('ignore', category=UserWarning, message='.*pin_memory.*')
warnings.filterwarnings('ignore', category=FutureWarning)

# Reduce transformers logging verbosity
logging.getLogger("transformers").setLevel(logging.ERROR)
os.environ["TRANSFORMERS_VERBOSITY"] = "error"
from transformers import (
    AutoImageProcessor,
    AutoModelForImageClassification,
    TrainingArguments,
    Trainer,
    EarlyStoppingCallback
)
from datasets import load_dataset
from torchvision.transforms import (
    Compose,
    RandomRotation,
    RandomAffine,
    GaussianBlur,
    RandomResizedCrop,
    RandomHorizontalFlip,
    ColorJitter,
)
import numpy as np
from PIL import Image
import json

# Configuration
DATA_DIR = Path("/home/imme/vogel-training-data/organized")
OUTPUT_DIR = Path("/home/imme/vogel-models")
MODEL_NAME = "google/efficientnet-b0"  # Base model for fine-tuning
BATCH_SIZE = 16
NUM_EPOCHS = 50
LEARNING_RATE = 2e-4
IMAGE_SIZE = 224

def get_species_from_directory(data_dir):
    """Automatically detect species from train directory structure."""
    train_dir = data_dir / "train"
    if not train_dir.exists():
        raise ValueError(f"Train directory not found: {train_dir}")
    
    # Get all subdirectories (each is a species)
    species = sorted([d.name for d in train_dir.iterdir() if d.is_dir()])
    
    if not species:
        raise ValueError(f"No species directories found in {train_dir}")
    
    return species

def prepare_model_and_processor(species):
    """Load base model and processor."""
    print(f"Lade Basis-Modell: {MODEL_NAME}")
    
    # Create label mappings
    id2label = {i: sp for i, sp in enumerate(species)}
    label2id = {sp: i for i, sp in enumerate(species)}
    
    processor = AutoImageProcessor.from_pretrained(MODEL_NAME)
    model = AutoModelForImageClassification.from_pretrained(
        MODEL_NAME,
        num_labels=len(species),
        id2label=id2label,
        label2id=label2id,
        ignore_mismatched_sizes=True
    )
    
    return model, processor

def get_augmentation_transforms():
    """Create data augmentation transforms (applied BEFORE processor)."""
    return Compose([
        RandomResizedCrop(IMAGE_SIZE, scale=(0.7, 1.0)),  # More scale variation
        RandomHorizontalFlip(p=0.5),
        RandomRotation(degrees=15),  # Birds can appear at different angles
        RandomAffine(degrees=0, translate=(0.1, 0.1)),  # Slight position shifts
        ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),  # Stronger color augmentation
        GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),  # Simulate focus variations
    ])

def transform_function(examples, processor, is_training=True):
    """Transform function for dataset mapping.
    
    Uses processor for normalization to match inference behavior.
    Only applies data augmentation for training.
    """
    images = []
    
    for img in examples["image"]:
        # Convert to RGB
        img = img.convert("RGB")
        
        # Apply data augmentation for training only
        if is_training:
            augmentation = get_augmentation_transforms()
            img = augmentation(img)
        
        # Use processor for final preprocessing (resize, normalize)
        # This ensures train/val/test preprocessing is consistent!
        processed = processor(img, return_tensors="pt")
        images.append(processed["pixel_values"][0])
    
    examples["pixel_values"] = images
    return examples

def create_compute_metrics(species):
    """Create compute_metrics function with species list."""
    id2label = {i: sp for i, sp in enumerate(species)}
    
    def compute_metrics(eval_pred):
        """Compute accuracy metrics."""
        predictions, labels = eval_pred
        predictions = np.argmax(predictions, axis=1)
        
        accuracy = (predictions == labels).mean()
        
        # Per-class accuracy
        per_class_acc = {}
        for i, sp in id2label.items():
            mask = labels == i
            if mask.sum() > 0:
                per_class_acc[sp] = (predictions[mask] == labels[mask]).mean()
        
        return {
            "accuracy": accuracy,
            **{f"acc_{sp}": acc for sp, acc in per_class_acc.items()}
        }
    
    return compute_metrics

def collate_fn(examples):
    """Custom collate function for DataLoader."""
    pixel_values = torch.stack([example["pixel_values"] for example in examples])
    labels = torch.tensor([example["label"] for example in examples])
    return {"pixel_values": pixel_values, "labels": labels}

# Global flag for graceful shutdown
interrupted = False

def signal_handler(sig, frame):
    """Handle Ctrl+C gracefully."""
    global interrupted
    if not interrupted:
        interrupted = True
        print("\n\n" + "="*60)
        print("⚠️  Training wird unterbrochen (Strg+C erkannt)")
        print("Warte auf sauberes Beenden des aktuellen Schritts...")
        print("="*60 + "\n")
        print("(Drücke Strg+C erneut für sofortiges Beenden)")
    else:
        print("\n⚠️  Sofortiges Beenden erzwungen!")
        sys.exit(1)

def main():
    """Main training function."""
    # Register signal handler
    signal.signal(signal.SIGINT, signal_handler)
    
    print("="*60)
    print("Vogel-Artenerkennung Training")
    print("="*60)
    print("(Drücke Strg+C zum sauberen Beenden)")
    print("="*60)
    
    # Detect species from directory structure
    print("\nErkenne Vogelarten aus Verzeichnisstruktur...")
    species = get_species_from_directory(DATA_DIR)
    
    # Create output directory
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = OUTPUT_DIR / f"bird-classifier-{timestamp}"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    print(f"\nOutput Ordner: {output_dir}")
    print(f"Vogelarten: {', '.join(species)}")
    print(f"Anzahl Klassen: {len(species)}")
    
    # Load dataset
    print("\nLade Dataset...")
    dataset = load_dataset("imagefolder", data_dir=str(DATA_DIR))
    
    print(f"  Training:   {len(dataset['train'])} Bilder")
    print(f"  Validation: {len(dataset['validation'])} Bilder")
    
    # Verify species match dataset labels
    dataset_labels = dataset["train"].features["label"].names
    print(f"\nDataset Labels: {dataset_labels}")
    print(f"Detected Species: {species}")
    
    if sorted(dataset_labels) != sorted(species):
        print("\n⚠️  WARNUNG: Species-Liste stimmt nicht mit Dataset überein!")
        print(f"   Dataset hat: {sorted(dataset_labels)}")
        print(f"   Erkannt wurden: {sorted(species)}")
        raise ValueError("Species mismatch - bitte Verzeichnisstruktur prüfen!")
    
    # Use dataset labels (they are already correctly mapped)
    species = dataset_labels
    print(f"\nVerwende Dataset Label-Mapping: {species}")
    
    # Prepare model and processor with correct species order
    model, processor = prepare_model_and_processor(species)
    
    # Apply transforms
    print("\nAppliziere Transformationen...")
    dataset["train"] = dataset["train"].map(
        lambda x: transform_function(x, processor, is_training=True),
        batched=True,
        remove_columns=["image"]
    )
    dataset["validation"] = dataset["validation"].map(
        lambda x: transform_function(x, processor, is_training=False),
        batched=True,
        remove_columns=["image"]
    )
    
    # Set format
    dataset["train"].set_format(type="torch", columns=["pixel_values", "label"])
    dataset["validation"].set_format(type="torch", columns=["pixel_values", "label"])
    
    # Training arguments with optimized hyperparameters
    training_args = TrainingArguments(
        output_dir=str(output_dir),
        num_train_epochs=NUM_EPOCHS,
        per_device_train_batch_size=BATCH_SIZE,
        per_device_eval_batch_size=BATCH_SIZE,
        learning_rate=LEARNING_RATE,
        warmup_ratio=0.1,
        lr_scheduler_type="cosine",  # Cosine annealing for better convergence
        weight_decay=0.01,  # L2 regularization to prevent overfitting
        logging_dir=str(output_dir / "logs"),
        logging_steps=10,
        logging_strategy="epoch",  # Nur Epochen-Logs anzeigen
        eval_strategy="epoch",
        save_strategy="epoch",
        save_total_limit=3,
        load_best_model_at_end=True,
        metric_for_best_model="accuracy",
        greater_is_better=True,
        push_to_hub=False,
        remove_unused_columns=False,
        dataloader_num_workers=4,
        label_smoothing_factor=0.1,  # Label smoothing for better generalization
        report_to="none",  # Keine Berichte an externe Tools
        disable_tqdm=False,  # Progress bar behalten
    )
    
    # Create compute_metrics function with species
    compute_metrics = create_compute_metrics(species)
    
    # Create trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=dataset["train"],
        eval_dataset=dataset["validation"],
        compute_metrics=compute_metrics,
        data_collator=collate_fn,
        callbacks=[EarlyStoppingCallback(early_stopping_patience=7)]  # Etwas mehr Geduld für bessere Konvergenz
    )
    
    # Train
    print("\n" + "="*60)
    print("Starte Training...")
    print("="*60)
    
    try:
        train_result = trainer.train()
    except KeyboardInterrupt:
        if interrupted:
            print("\n" + "="*60)
            print("Training wurde durch Benutzer unterbrochen")
            print("Speichere aktuellen Modellstand...")
            print("="*60)
            
            # Save interrupted model
            interrupted_dir = output_dir / "interrupted"
            trainer.save_model(str(interrupted_dir))
            processor.save_pretrained(str(interrupted_dir))
            print(f"\n✓ Modell gespeichert in: {interrupted_dir}")
            print("\nTraining kann später fortgesetzt werden mit:")
            print(f"  --resume_from_checkpoint {interrupted_dir}")
            return
        raise
    
    # Save final model
    print("\nSpeichere finales Modell...")
    trainer.save_model(str(output_dir / "final"))
    processor.save_pretrained(str(output_dir / "final"))
    
    # Save training stats
    with open(output_dir / "training_stats.json", "w") as f:
        json.dump({
            "train_runtime": train_result.metrics["train_runtime"],
            "train_samples_per_second": train_result.metrics["train_samples_per_second"],
            "train_loss": train_result.metrics["train_loss"],
            "species": species,
            "num_train_samples": len(dataset["train"]),
            "num_val_samples": len(dataset["validation"]),
        }, f, indent=2)
    
    # Final evaluation
    print("\n" + "="*60)
    print("Finale Evaluation")
    print("="*60)
    
    eval_results = trainer.evaluate()
    
    print(f"\nValidation Accuracy: {eval_results['eval_accuracy']:.4f}")
    print("\nPro-Vogelart Accuracy:")
    for sp in species:
        key = f"eval_acc_{sp}"
        if key in eval_results:
            print(f"  {sp:12s}: {eval_results[key]:.4f}")
    
    # Save evaluation results
    with open(output_dir / "eval_results.json", "w") as f:
        json.dump(eval_results, f, indent=2)
    
    print("\n" + "="*60)
    print("Training abgeschlossen!")
    print("="*60)
    print(f"\nModell gespeichert in: {output_dir / 'final'}")
    print(f"\nUm das Modell zu nutzen:")
    print(f"  vogel-video-analyzer --species-model {output_dir / 'final'} <video>")

if __name__ == "__main__":
    main()
