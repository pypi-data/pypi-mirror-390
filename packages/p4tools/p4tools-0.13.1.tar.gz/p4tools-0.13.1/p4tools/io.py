"""This module takes care of receiving the data catalogs."""

# AUTOGENERATED! DO NOT EDIT! File to edit: ../notebooks/00_io.ipynb.

# %% auto 0
__all__ = ['logger', 'v1', 'kwargs', 'v3', 'fetchers', 'mars_years', 'postprocessor', 'get_metafull', 'get_blotch_catalog',
           'get_fan_catalog', 'get_tile_coords', 'get_meta_data', 'get_region_names', 'get_tile_urls',
           'define_martian_year', 'normalize_tile_id', 'get_subframe', 'get_url_for_tile_id', 'get_url_for_tile',
           'get_subframe_by_tile_id', 'get_subframe_for_tile', 'get_fans_for_tile', 'get_blotches_for_tile',
           'get_hirise_id_for_tile']

# %% ../notebooks/00_io.ipynb 2
from pathlib import Path

import matplotlib.image as mplimg
import pandas as pd
import pooch
from matplotlib import pyplot as plt

# %% ../notebooks/00_io.ipynb 3
logger = pooch.get_logger()
logger.setLevel("DEBUG")

# %% ../notebooks/00_io.ipynb 4
v1 = pooch.create(
    path=pooch.os_cache("p4tools"),
    base_url="",
    version="v1",
    registry={
        "fans": "md5:71ff51ff79d6e975f704f19b1996d8ea",
        "blotches": "md5:f4d0c101f65abbaf34e092620133d56e",
        "metadata": "md5:c0dc46e0fc3d259c30afaec412074eae",
        "tile_coords": "md5:6b9a917a6997f1aa01cfef4322cabd81",
        "raw_data": "md5:39a8909590fe9f816454db93f0027d2c",
        "intermediate": "md5:6544bf0c7851eedd4783859c0adc42d7",
        "region_names": "md5:9101c7a0f8e248c9ffe9c07869da5635",
        "tile_urls": "md5:5717c8379d453cf4b11a5f5775f5fb6e",
    },
    urls={
        "fans": "doi:10.5281/zenodo.8102805/P4_catalog_v1.1_L1C_cut_0.5_fan.csv.zip",
        "blotches": "doi:10.5281/zenodo.8102805/P4_catalog_v1.1_L1C_cut_0.5_blotch.csv.zip",
        "metadata": "doi:10.5281/zenodo.8102805/P4_catalog_v1.1_metadata.csv.zip",
        "tile_coords": "doi:10.5281/zenodo.8102805/P4_catalog_v1.1_tile_coords_final.csv.zip",
        "raw_data": "doi:10.5281/zenodo.8102805/P4_catalog_v1.0_raw_classifications.hdf.zip",
        "intermediate": "doi:10.5281/zenodo.8102805/P4_catalog_v1.0_pipeline_products.zip",
        "region_names": "doi:10.5281/zenodo.8102805/region_names.zip",
        "tile_urls": "doi:10.5281/zenodo.8102805/tile_urls.csv.zip",
    },
)

# %% ../notebooks/00_io.ipynb 5
def postprocessor(fname, action, pup):
    """
    Post-processing hook to process the downloaded v1 catalog file.

    Postprocessing steps are:
    1. unzip to CSV
    2. load CSV into pandas
    3. save as parquet file.

    This time, at the next read, the performance is much increased due to reading the parquet file instead of loading
    the CSV file every time.

    Parameters
    ----------
    fname : str
       Full path of the zipped file in local storage
    action : str
       One of "download" (file doesn't exist and will download),
       "update" (file is outdated and will download), and
       "fetch" (file exists and is updated so no download).
    pup : Pooch
       The instance of Pooch that called the processor function.

    Returns
    -------
    fname : str
       The full path to the unzipped file. (Return the same fname is your
       processor doesn't modify the file).

    """
    fpath = Path(fname)
    if pup.urls[fpath.name].endswith(".zip"):
        unzipper = pooch.Unzip()
        csvfname = unzipper(fname, action, pup)[0]
    else:
        csvfname = fname
    parquetfname = Path(csvfname).with_suffix(".parq")
    if action in ("update", "download") or not parquetfname.is_file():
        logger.info(f"Converting CSV to parquet and storing as {parquetfname}.")
        pd.read_csv(csvfname).to_parquet(parquetfname)
    return parquetfname

# %% ../notebooks/00_io.ipynb 6
kwargs = dict(processor=postprocessor, progressbar=True)

# %% ../notebooks/00_io.ipynb 10
v3 = pooch.create(
    path=pooch.os_cache("p4tools"),
    base_url="",
    version="v3",
    registry={
        "fans": "md5:186c98dee8712dcab1ea08317e60ed9b",
        "blotches": "md5:651d767533f70196ab36512eff01a941",
        "metafull": "md5:5c2e5f34e2b9c98b18b35ed5e16f68b4",
        "tile_coords": "md5:adb3958c7ff2f80385ddcf4c6a7b8da1",
        "metadata": "md5:fa1ed90fd731fabc0f760b2d51759b4f",
    },
    urls={
        "fans": "https://refubium.fu-berlin.de/bitstream/handle/fub188/46287/P4_catalog_Full_Release_v3.0_L1C_cut_0.5_fan_meta_merged.csv",
        "blotches": "https://refubium.fu-berlin.de/bitstream/handle/fub188/46287/P4_catalog_Full_Release_v3.0_L1C_cut_0.5_blotch_meta_merged.csv",
        "metafull": "https://refubium.fu-berlin.de/bitstream/handle/fub188/46287/P4_catalog_Full_Release_v3.0_EDRINDEX_metadata.csv",
        "tile_coords": "https://refubium.fu-berlin.de/bitstream/handle/fub188/46287/P4_catalog_Full_Release_v3.0_tile_coords_final.csv",
        "metadata": "https://refubium.fu-berlin.de/bitstream/handle/fub188/46287/P4_catalog_Full_Release_v3.0_metadata.csv",
    },
)

# %% ../notebooks/00_io.ipynb 14
fetchers = dict(v1=v1.fetch, v3=v3.fetch)

# %% ../notebooks/00_io.ipynb 15
def get_metafull() -> pd.DataFrame:
    "only v3 exists"
    df = pd.read_parquet(v3.fetch("metafull", **kwargs)).drop("Unnamed: 0", axis=1)
    return define_martian_year(df, "OBSERVATION_START_TIME")


def get_blotch_catalog(version="v3") -> pd.DataFrame:
    bl = pd.read_parquet(fetchers[version]("blotches", **kwargs))
    meta = get_metafull()
    return bl.set_index("obsid").join(meta.set_index("OBSERVATION_ID")["MY"])


def get_fan_catalog(version="v3") -> pd.DataFrame:
    fans = pd.read_parquet(fetchers[version]("fans", **kwargs))
    meta = get_metafull()
    return fans.set_index("obsid").join(meta.set_index("OBSERVATION_ID")["MY"])


def get_tile_coords(version="v3") -> pd.DataFrame:
    return pd.read_parquet(fetchers[version]("tile_coords", **kwargs))


def get_meta_data(version="v3") -> pd.DataFrame:
    return pd.read_parquet(fetchers[version]("metadata", **kwargs))


def get_region_names() -> pd.DataFrame:
    "only v1 exists"
    return pd.read_parquet(v1.fetch("region_names", **kwargs))


def get_tile_urls() -> pd.DataFrame:
    "only v1 exists"
    return pd.read_parquet(v1.fetch("tile_urls", **kwargs))


mars_years = {
    28: "2006-01-23",
    29: "2007-12-10",
    30: "2009-10-27",
    31: "2011-09-15",
    32: "2013-08-01",
    33: "2015-06-19",
    34: "2017-05-05",
    35: "2019-03-24",
    36: "2021-02-07",
    37: "2022-12-26",
    38: "2024-11-24",
    39: "2026-09-30",
    40: "2028-08-17",
}


def define_martian_year(df, time_col_name):
    """Add Martian Year (MY) column to dataframe based on time column.

    Parameters
    ----------
    df : pd.DataFrame
        DataFrame to modify
    time_col_name : str
        Name of the time column to use for MY calculation

    Returns
    -------
    pd.DataFrame
        DataFrame with added 'MY' column
    """
    mars_timestamps = {k: pd.to_datetime(v) for k, v in mars_years.items()}

    # Convert time column to datetime if it's not already
    if not pd.api.types.is_datetime64_any_dtype(df[time_col_name]):
        df[time_col_name] = pd.to_datetime(df[time_col_name])

    df["MY"] = 0
    for yr, t in mars_timestamps.items():
        df.loc[df[time_col_name] > t, "MY"] = yr

    return df


# %% ../notebooks/00_io.ipynb 23
def normalize_tile_id(tile_id: str) -> str:
    """Normalize a tile ID by adding 'APF' prefix and leading zeros if necessary.

    Parameters
    ----------
    tile_id : str
        Full or partial tile ID. If partial, it will be padded with 'APF' and leading zeros.

    Returns
    -------
    str
        Complete tile ID in format 'APF0000xxx' (always 9 characters)

    Examples
    --------
    >>> normalize_tile_id('r8y')
    'APF0000r8y'
    >>> normalize_tile_id('0000r8y')
    'APF0000r8y'
    >>> normalize_tile_id('APF0000r8y')
    'APF0000r8y'
    >>> normalize_tile_id('123r8y')
    'APF0123r8y'
    """
    # Remove 'APF' prefix if present
    if tile_id.upper().startswith("APF"):
        tile_id = tile_id[3:]

    # Calculate how many zeros we need to add
    target_length = 7  # length without 'APF'
    current_length = len(tile_id)

    if current_length > target_length:
        raise ValueError(f"Tile ID too long: {tile_id}")

    # Add necessary leading zeros
    padded_id = "0" * (target_length - current_length) + tile_id

    # Add APF prefix
    return f"APF{padded_id}"

# %% ../notebooks/00_io.ipynb 25
def get_subframe(url):
    targetpath = pooch.retrieve(
        url, path=pooch.os_cache("p4tools/tiles"), known_hash=None, progressbar=True
    )
    im = mplimg.imread(targetpath)
    return im

# %% ../notebooks/00_io.ipynb 26
def get_url_for_tile_id(tile_id):
    return get_tile_urls().set_index("tile_id").squeeze().at[normalize_tile_id(tile_id)]


def get_url_for_tile(tile_id):
    # alias for get_url_for_tile_id
    return get_url_for_tile_id(tile_id)

# %% ../notebooks/00_io.ipynb 30
def get_subframe_by_tile_id(tile_id):
    url = get_url_for_tile_id(tile_id)
    return get_subframe(url)


def get_subframe_for_tile(tile_id):
    # alias for get_subframe_by_tile_id for consistency
    return get_subframe_by_tile_id(tile_id)

# %% ../notebooks/00_io.ipynb 32
def get_fans_for_tile(tile_id, version="v3"):
    tile_id = normalize_tile_id(tile_id)
    fans = get_fan_catalog(version)
    return fans.query("tile_id == @tile_id")

# %% ../notebooks/00_io.ipynb 35
def get_blotches_for_tile(tile_id, version="v3"):
    tile_id = normalize_tile_id(tile_id)
    blotches = get_blotch_catalog(version)
    return blotches.query("tile_id == @tile_id")

# %% ../notebooks/00_io.ipynb 38
def get_hirise_id_for_tile(tile_id, version="v3"):
    tile_id = normalize_tile_id(tile_id)
    try:
        obsid = get_fan_catalog(version).query("tile_id == @tile_id").obsid.iloc[0]
    except IndexError:
        try:
            obsid = (
                get_blotch_catalog(version).query("tile_id == @tile_id").obsid.iloc[0]
            )
        except IndexError:
            raise ValueError(f"No obsid found for tile {tile_id}")
    else:
        return obsid
