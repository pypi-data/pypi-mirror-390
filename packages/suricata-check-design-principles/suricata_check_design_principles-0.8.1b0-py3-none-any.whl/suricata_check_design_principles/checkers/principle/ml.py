"""`PrincipleMLChecker`."""

import copy
import logging
import os
import pickle
from collections import Counter
from collections.abc import Iterable
from typing import Any, Literal, Optional, Union, overload

import suricata_check
import xgboost
from pandas import DataFrame, Series
from sklearn.metrics import f1_score, make_scorer, precision_score, recall_score
from sklearn.model_selection import (
    GridSearchCV,
    RepeatedStratifiedKFold,
    cross_val_score,
)
from sklearn.pipeline import Pipeline
from suricata_check.checkers.interface.checker import CheckerInterface
from suricata_check.utils.checker import get_rule_option, get_rule_suboptions
from suricata_check.utils.checker_typing import ISSUES_TYPE, Issue
from suricata_check.utils.rule import Rule

from suricata_check_design_principles._version import SURICATA_CHECK_DIR
from suricata_check_design_principles.checkers.principle._utils import get_message

_PICKLE_PATH = os.path.join(SURICATA_CHECK_DIR, "data", "principle_ml_checker.pkl")
N_JOBS = 8


_logger = logging.getLogger(__name__)


COUNT_COLUMNS = (
    "flowbits.isset.count",
    "flowbits.isntoset.count",
    "flowint.isset.count",
    "flowint.isntoset.count",
    "xbits.isset.count",
    "xbits.uisnotset.count",
    "http.uri.count",
    "http.method.count",
    "dns.query.count",
    "content.count",
    "pcre.count",
    "startswith.count",
    "bsize.count",
    "depth.count",
    "urilen.count",
    "flow.from_server.count",
    "flow.to_server.count",
    "flow.from_client.count",
    "flow.to_client.count",
)
STRING_COLUMNS = ()
DROPDOWN_COLUMNS = (
    "proto",
    "threshold.type",
)
NUMERICAL_COLUMNS = ("threshold.count",)
SPLITTABLE_FEATURES = (
    "metadata",
    "flow",
    "threshold",
)
MSG_KEYWORDS = ("Suspicious", "CVE", "Vulnerability", "Response")
MSG_COLUMNS = ("msg.contains." + keyword for keyword in MSG_KEYWORDS)
IP_KEYWORDS = ("$HOME_NET", "$HTTP_SERVERS", "$EXTERNAL_NET", "any")
IP_COLUMNS = tuple(
    ["source_addr.contains." + keyword for keyword in IP_KEYWORDS]
    + ["dest_addr.contains." + keyword for keyword in IP_KEYWORDS]
)


PIPELINE = Pipeline(
    [
        (
            "classify",
            xgboost.XGBClassifier(),
        )
    ]
)
# https://shengyg.github.io/repository/machine%20learning/2017/02/25/Complete-Guide-to-Parameter-Tuning-xgboost.html
PARAM_GRID: list[dict] = [
    {
        # Fixed parameters for problem / desired complexity
        "classify__n_estimators": [1000],
        "classify__objective": ["binary:logistic"],
        ###
        # Parameters to optimize
        ## Learning rate
        "classify__eta": [0.01, 0.1, 0.3],
        ## Tree parameters
        "classify__subsample": [1.0],
        "classify__colsample_bytree": [0.25, 0.5, 0.75, 1.0],
        "classify__scale_pos_weight": [0.1, 0.25, 0.5, 1.0, 2.0, 4.0, 10.0],
        "classify__max_depth": [1, 3],
        "classify__min_child_weight": [1],
        "classify__gamma": [0, 0.1],
        ## Regularization
        "classify__lambda": [0, 0.01, 0.1],
        "classify__alpha": [0, 0.01, 0.1],
    },
]

PRECISION_WEIGHT = 10
SCORER = make_scorer(
    lambda y, y_pred: (PRECISION_WEIGHT + 1)
    / (
        PRECISION_WEIGHT / (precision_score(y, y_pred, zero_division=1) + 1e-10)  # type: ignore reportArgumentType
        + 1 / (recall_score(y, y_pred, zero_division=0) + 1e-10)  # type: ignore reportArgumentType
    )
)
SPLITTER = RepeatedStratifiedKFold(n_splits=2, n_repeats=10)
GRIDSEARCHCV = GridSearchCV(
    PIPELINE, PARAM_GRID, cv=SPLITTER, scoring=SCORER, error_score="raise", n_jobs=N_JOBS, verbose=1  # type: ignore reportArgumentType
)


class PrincipleMLChecker(CheckerInterface):
    """The `PrincipleChecker` contains several checks based on the Ruling the Unruly paper and target specificity and coverage.

    Codes Q000-Q009 report on non-adherence to rule design principles similar to Q000-Q009.
    Differently, they are the result of machine learning analysis of the rules.
    """

    count_columns = COUNT_COLUMNS
    string_columns = STRING_COLUMNS
    dropdown_columns = DROPDOWN_COLUMNS
    numerical_columns = NUMERICAL_COLUMNS
    splittable_features = SPLITTABLE_FEATURES
    msg_keywords = MSG_KEYWORDS
    msg_columns = MSG_COLUMNS
    ip_keywords = IP_KEYWORDS
    ip_columns = IP_COLUMNS

    codes = {
        "Q000": {"severity": logging.INFO},
        "Q001": {"severity": logging.INFO},
        "Q002": {"severity": logging.INFO},
        "Q003": {"severity": logging.INFO},
        "Q004": {"severity": logging.INFO},
        "Q005": {"severity": logging.INFO},
    }

    enabled_by_default = (
        False  # Since the checker is relatively slow, it is disabled by default
    )

    _dtypes: Optional[dict[str, Any]] = None
    _models: dict[str, Pipeline] = {}

    def __new__(
        cls: type["PrincipleMLChecker"],
        filepath: Optional[str] = _PICKLE_PATH,
        *args: tuple,
        **kwargs: dict,
    ) -> "PrincipleMLChecker":
        """Returns a new or unpickled instance of the class."""
        if filepath:
            if os.path.exists(filepath):
                with open(filepath, "rb") as f:
                    inst = pickle.load(f)

                if not inst.__class__.__name__ == cls.__name__:
                    _logger.error("Unpickled object is not of type %s", cls)
                    inst = super().__new__(cls, *args, **kwargs)  # type: ignore reportargumentType
                elif not hasattr(inst, "_models") or len(inst._models) == 0:
                    _logger.error("Unpickled object does not have trained models")
                    inst = super().__new__(cls, *args, **kwargs)  # type: ignore reportargumentType
                else:
                    if "include" in kwargs:
                        inst.include = kwargs["include"]
                    _logger.info("Unpickled object with trained models successfully")
            else:
                _logger.warning("No model found for PrincipleMLChecker at %s", filepath)
                inst = super().__new__(cls, *args, **kwargs)  # type: ignore reportargumentType
        else:
            inst = super().__new__(cls, *args, **kwargs)  # type: ignore reportargumentType

        return inst

    def __getnewargs__(
        self: "PrincipleMLChecker",
    ) -> tuple:
        """Returns the arguments to be passed to the __new__ method when unpickling."""
        return (None,)

    def _check_rule(
        self: "PrincipleMLChecker",
        rule: Rule,
    ) -> ISSUES_TYPE:
        issues: ISSUES_TYPE = []

        if len(self._models) == 0:
            return issues

        for code, model in self._models.items():
            if model.predict(self._get_features(rule, True))[0]:
                issues.append(
                    Issue(
                        code=code,
                        message=get_message(code),
                    )
                )

        return issues

    def train(  # noqa: C901
        self: "PrincipleMLChecker",
        df: DataFrame,
        rule_col: str = "rule.rule",
        principle_cols: dict[str, str] = {
            "Q000": "labelled.no_proxy",
            "Q001": "labelled.success",
            "Q002": "labelled.thresholded",
            "Q003": "labelled.exceptions",
            "Q004": "labelled.generalized_match_content",
            "Q005": "labelled.generalized_match_location",
        },
        reuse_models: bool = False,
    ) -> None:
        """Train several models for the checker to detect issues in rules.

        The checker class with trained models is stored in a pickle file (`_PICKLE_PATH`).
        """
        self._dtypes = None
        if not reuse_models:
            self._models = {}

        # Extract features and determine feature dtypes
        X_train = self._get_train_df(df[rule_col])  # noqa: N806

        for col in X_train.columns:
            try:
                X_train[col].var()
                _logger.debug("Detected column: %s", col)
            except:
                _logger.error("Error with column %s", col)
                _logger.error(X_train[col])
                raise

        # # Drop zero variance columns
        X_train = X_train.drop(  # noqa: N806
            X_train.columns[(X_train.fillna(-1337).var(axis=0) <= 0)].to_list(),  # type: ignore reportAttributeAccessIssue
            axis=1,
        )

        # Drop columns with too few occurrences of possible values
        for col in X_train.columns:
            if (
                not col.endswith(".count")
                and not col.endswith(".num")
                and not col.endswith(".len")
            ):
                if X_train[col].value_counts().min() <= 1:
                    X_train = X_train.drop(  # noqa: N806
                        [col],
                        axis=1,
                    )

        for col in X_train.columns:
            try:
                X_train[col].var()
                _logger.info("Using column: %s", col)
            except:
                _logger.error("Error with column %s", col)
                _logger.error(X_train[col])
                raise

        # Store used features and their dtypes
        self._dtypes = X_train.dtypes.to_dict()
        _logger.debug(self._dtypes)

        # Redo feature extraction now that FE parameters are set
        X_train = self._get_train_df(df[rule_col])  # noqa: N806

        _logger.info(
            "Training model with features: [%s]",
            ", ".join([str(x) for x in X_train.columns]),
        )

        _logger.info(X_train)

        for code, col in principle_cols.items():
            y_true = df[col].to_numpy() == 0

            if not reuse_models or code not in self._models:
                # Train new model with grid search to find optimal parameters
                gridsearchcv: GridSearchCV = copy.deepcopy(GRIDSEARCHCV)

                gridsearchcv.fit(X_train, y_true)

                _logger.info("Code %s params: %s", code, gridsearchcv.best_params_)
                _logger.info(
                    "Code %s Weighted F1-score: %s", code, gridsearchcv.best_score_
                )

                self._models[code] = gridsearchcv.best_estimator_

            precision = cross_val_score(
                self._models[code],
                X_train,
                y_true,
                scoring=make_scorer(precision_score, zero_division=0.0),
                cv=SPLITTER,
                n_jobs=N_JOBS,
            ).mean()
            recall = cross_val_score(
                self._models[code],
                X_train,
                y_true,
                scoring=make_scorer(recall_score, zero_division=0.0),
                cv=SPLITTER,
                n_jobs=N_JOBS,
            ).mean()
            f1 = cross_val_score(
                self._models[code],
                X_train,
                y_true,
                scoring=make_scorer(f1_score, zero_division=0.0),
                cv=SPLITTER,
                n_jobs=N_JOBS,
            ).mean()
            _logger.info("Code %s Precision score: %s", code, precision)
            _logger.info("Code %s Recall score: %s", code, recall)
            _logger.info("Code %s F1-score: %s", code, f1)

            # Refit model with training data.
            self._models[code].fit(X_train, y_true)

        pickle.dump(self, open(_PICKLE_PATH, "wb"))

    def _get_train_df(self: "PrincipleMLChecker", rules: Iterable[str]) -> DataFrame:
        feature_vectors = []
        for rule in rules:
            parsed_rule = suricata_check.utils.rule.parse(rule)
            assert parsed_rule is not None
            feature_vectors.append(self._get_features(parsed_rule, False))

        return DataFrame(feature_vectors)

    def _get_raw_features(  # noqa: C901
        self: "PrincipleMLChecker", rule: Rule
    ) -> Series:
        d: dict[str, Optional[Union[str, int]]] = {
            "proto": get_rule_option(rule, "proto")
        }

        options = rule.options

        for option in options:
            d[option.name] = option.value

        counter = Counter([option.name for option in options])
        for option, count in counter.items():
            d[option + ".count"] = count

        for option in options:
            if option.name not in self.splittable_features:
                continue

            suboptions = [
                {"name": k, "value": v}
                for k, v in get_rule_suboptions(rule, option.name, warn=False)
            ]

            if len(suboptions) == 0:
                continue

            for suboption in suboptions:
                d[option.name + "." + suboption["name"]] = suboption["value"]

            counter = Counter([suboption["name"] for suboption in suboptions])
            for suboption, count in counter.items():
                d[option.name + "." + suboption + ".count"] = count

        msg = get_rule_option(rule, "msg")
        assert msg is not None
        msg = msg.lower()
        for col, keyword in zip(self.msg_columns, self.msg_keywords):
            d[col] = keyword.lower() in msg

        source_addr = get_rule_option(rule, "source_addr")
        assert source_addr is not None
        source_addr = source_addr.lower()
        for keyword in self.ip_keywords:
            col = "source_addr.contains." + keyword
            d[col] = keyword.lower() in source_addr

        dest_addr = get_rule_option(rule, "dest_addr")
        assert dest_addr is not None
        dest_addr = dest_addr.lower()
        for keyword in self.ip_keywords:
            col = "dest_addr.contains." + keyword
            d[col] = keyword.lower() in dest_addr

        return Series(d)

    def _preprocess_features(self: "PrincipleMLChecker", data: Series) -> Series:
        original_cols: set[str] = set(data.index)

        for col in self.string_columns:
            if col not in data:
                continue
            data[col + ".len"] = len(data[col])
            data = data.drop(col)

        for col in self.dropdown_columns:
            if col not in data:
                continue
            data[col + "." + data[col] + ".bool"] = 1
            data = data.drop(col)

        for col in self.numerical_columns:
            if col not in data:
                continue
            data[col + ".num"] = float(data[col])  # type: ignore reportArgumentType
            data = data.drop(col)

        remaining_cols = (
            original_cols
            - set(self.count_columns)
            - set(self.string_columns)
            - set(self.dropdown_columns)
            - set(self.numerical_columns)
            - set(self.msg_columns)
            - set(self.ip_columns)
        )

        for col in remaining_cols:
            data = data.drop(col)

        return data

    @overload
    def _get_features(
        self: "PrincipleMLChecker", rule: Rule, frame: Literal[True]
    ) -> DataFrame:
        pass

    @overload
    def _get_features(
        self: "PrincipleMLChecker", rule: Rule, frame: Literal[False]
    ) -> Series:
        pass

    def _get_features_frame(self: "PrincipleMLChecker", features: Series) -> DataFrame:
        features_frame = features.to_frame().transpose()

        if self._dtypes is None:
            return features_frame

        for col, dtype in self._dtypes.items():
            if features_frame.dtypes[col] != dtype:
                features_frame[col] = features_frame[col].astype(dtype)

        return features_frame

    def _get_features(
        self: "PrincipleMLChecker", rule: Rule, frame: bool
    ) -> Union[Series, DataFrame]:
        features: Series = self._get_raw_features(rule)
        features = self._preprocess_features(features)

        features["custom.negated.count"] = rule.raw.count(':!"')

        if self._dtypes is None:
            return features

        for col, dtype in self._dtypes.items():
            if col not in features:
                if col.endswith(".count"):
                    features[col] = 0
                elif col.endswith(".bool"):
                    features[col] = 0
                elif col.endswith(".num"):
                    features[col] = -1
                else:
                    _logger.error(
                        "Unsure how to handle missing feature %s of type %s",
                        col,
                        dtype,
                    )

        features = features[list(self._dtypes.keys())]  # type: ignore reportAssignmentType

        if not frame:
            return features

        return self._get_features_frame(features)
