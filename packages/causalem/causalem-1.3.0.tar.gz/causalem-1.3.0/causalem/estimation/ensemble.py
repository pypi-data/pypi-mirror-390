"""
Ensemble-matching TE estimator
===============================

Features
--------
* **Deterministic reproducibility** with a single `random_state_master`.
  Every source of randomness (CV shuffling, stochastic
  matching, Random-Forest, bootstrap) is driven by integers drawn from a
  master NumPy Generator, so results are bit-identical across platforms
  and parallel runs.
* Optional **bootstrap percentile CIs** (`nboot`, `alpha`, `n_jobs`,
  `random_state_boot`).  During bootstrap the *same* reproducibility
  logic is applied inside each worker.
* When `niter==1` and the outcome is not survival the meta-learner is
  bypassed automatically.

!!!  When `nboot>0` the `groups` argument is ignored (row-level resampling).

---------------------------------------------------------------------------
"""

from __future__ import annotations

import math
import warnings
from typing import Any, Optional

import numpy as np
import pandas as pd
from joblib import Parallel, delayed
from sklearn.base import BaseEstimator, clone
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.linear_model import LinearRegression, LogisticRegression
from sklearn.model_selection import GroupKFold, KFold, cross_val_predict
from sklearn.preprocessing import OneHotEncoder
from sksurv.ensemble import RandomSurvivalForest
from sksurv.functions import StepFunction
from sksurv.linear_model import CoxPHSurvivalAnalysis

from causalem import stochastic_match
from causalem.utils._survival import fit_cox_marginal_weighted
from causalem.utils._weights import appearance_weights, fit_with_appearance_weights


# --------------------------------------------------------------------------- #
# Helpers                                                                     #
# --------------------------------------------------------------------------- #
def _make_splitter(
    *,
    n_splits: int,
    shuffle: bool,
    seed: int,
    groups: Optional[np.ndarray],
):
    """Return a cross-validation splitter.

    When ``groups`` is ``None`` a :class:`~sklearn.model_selection.KFold`
    instance seeded with ``seed`` is returned.  Otherwise a
    :class:`~sklearn.model_selection.GroupKFold` is used.  Newer versions of
    scikit-learn allow ``shuffle`` and ``random_state`` for ``GroupKFold`` but
    older ones do not, so those arguments are ignored when unsupported.
    """
    if groups is None:
        return KFold(n_splits=n_splits, shuffle=shuffle, random_state=seed)

    try:
        return GroupKFold(n_splits=n_splits, shuffle=shuffle, random_state=seed)
    except TypeError:  # older scikit-learn
        return GroupKFold(n_splits=n_splits)


# --------------------------------------------------------------------------- #
# Numerical utilities                                                         #
# --------------------------------------------------------------------------- #
def clip_logit(
    p: np.ndarray,
    eps: float,
) -> np.ndarray:
    """
    Clip probabilities to [eps, 1-eps], then return log(p / (1-p)).
    """
    p_clipped = np.clip(p, eps, 1 - eps)
    return np.log(p_clipped / (1 - p_clipped))


# --------------------------------------------------------------------------- #
# Survival helpers                                                            #
# --------------------------------------------------------------------------- #
def _simulate_from_sf(
    sf: StepFunction,
    n_draws: int,
    tau: float,
    rng: np.random.Generator,
):
    """
    Draw (time, event) samples from a survival curve.

    Parameters
    ----------
    sf       : sksurv.functions.StepFunction – survival curve S(t)
    n_draws  : int                          – # Monte-Carlo draws
    tau      : float                        – admin-censoring horizon (inf → none)
    rng      : np.random.Generator          – RNG

    Returns
    -------
    times  : ndarray (n_draws,)  – observed follow-up times
    events : ndarray (n_draws,)  – event indicators {0,1}
    """
    u = rng.random(n_draws)
    S = sf.y
    t_knots = sf.x
    rev_idx = np.searchsorted(S[::-1], u, side="right")
    idx = len(S) - rev_idx

    times = np.full(n_draws, np.inf)
    valid = idx < len(t_knots)
    times[valid] = t_knots[idx[valid]]

    if math.isfinite(tau):
        events = times <= tau
        times = np.minimum(times, tau)
    else:
        events = np.isfinite(times)
    return times, events


def _hr_dict_to_df(hr_dict: dict[tuple[str, str], float]) -> pd.DataFrame:
    """Convert pairwise HR dictionary to DataFrame with ``te`` column."""
    rows = [(k[0], k[1], v) for k, v in hr_dict.items()]
    return pd.DataFrame(rows, columns=["treatment_1", "treatment_2", "te"])


# --------------------------------------------------------------------------- #
# Stage-1 single iteration                                                    #
# --------------------------------------------------------------------------- #
def stage_1_single_iter(
    Xraw: np.ndarray,
    treatment: np.ndarray,
    y: np.ndarray,
    *,
    rng: np.random.Generator,
    outcome_is_binary: bool,
    groups: Optional[np.ndarray] = None,
    n_splits_propensity: int = 5,
    model_propensity=LogisticRegression(solver="newton-cg"),
    matching_scale: float = 1.0,
    matching_caliper: Optional[float] = None,
    n_splits_outcome: int = 5,
    model_outcome=RandomForestRegressor(n_estimators=100),
    matching_is_stochastic: bool = True,
    prob_clip_eps: float = 1e-6,
):
    """One iteration of outcome modelling with matching.

    Parameters
    ----------
    Xraw, treatment, y : arrays
        Raw covariates, treatment indicator and outcome.
    rng : numpy.random.Generator
        Source of randomness for CV splits and matching.
    outcome_is_binary : bool
        ``True`` for binary outcomes in which case class probabilities are
        predicted.
    groups : ndarray or None, optional
        Group labels for cross-validation, triggering ``GroupKFold`` when not
        ``None``.

    Returns
    -------
    cluster_ids : ndarray
        Cluster identifiers for the matching draw (``-1`` for unmatched).
    y_pred, y_pred_cf : ndarray
        Out-of-sample predictions for factual and counterfactual outcomes.
    """

    # Provide a default learner when none is supplied
    if model_outcome is None:
        model_outcome = RandomForestRegressor(n_estimators=100)

    # No feature subsetting
    X = Xraw

    X_t = np.hstack((X, treatment.reshape(-1, 1)))
    X_t_cf = np.hstack((X, (1 - treatment).reshape(-1, 1)))

    # --- propensity CV -----------------------------------------------------
    splitter_prop = _make_splitter(
        n_splits=n_splits_propensity,
        shuffle=True,
        seed=int(rng.integers(2**32)),
        groups=groups,
    )

    # --- propensity CV & logit --------------------------------------------
    oos_proba = cross_val_predict(
        clone(model_propensity),
        X,
        treatment,
        cv=splitter_prop,
        method="predict_proba",
        groups=groups,
    )[:, 1]
    oos_scores = clip_logit(oos_proba, eps=prob_clip_eps)

    # --- matching ----------------------------------------------------------
    cluster_ids = stochastic_match(
        treatment=treatment,
        score=oos_scores,
        scale=matching_scale,
        caliper=matching_caliper,
        nsmp=1 if matching_is_stochastic else 0,
        random_state=int(rng.integers(2**32)),
    ).ravel()
    matched_idx = np.where(cluster_ids != -1)[0]

    # --- outcome cross-fitting --------------------------------------------
    splitter_out = _make_splitter(
        n_splits=n_splits_outcome,
        shuffle=True,
        seed=int(rng.integers(2**32)),
        groups=groups,
    )
    y_pred = np.full(X_t.shape[0], np.nan)
    y_pred_cf = np.full(X_t.shape[0], np.nan)

    for tr_idx, te_idx in splitter_out.split(X_t, groups=groups):
        matched_tr = np.intersect1d(matched_idx, tr_idx)
        if matched_tr.size == 0:
            continue
        rf = clone(model_outcome)
        if "random_state" in rf.get_params(deep=False):
            rf.set_params(random_state=int(rng.integers(2**32)))
        rf.fit(X_t[matched_tr], y[matched_tr])

        if outcome_is_binary and hasattr(rf, "predict_proba"):
            y_pred[te_idx] = rf.predict_proba(X_t[te_idx])[:, 1]
            y_pred_cf[te_idx] = rf.predict_proba(X_t_cf[te_idx])[:, 1]
        else:
            y_pred[te_idx] = rf.predict(X_t[te_idx])
            y_pred_cf[te_idx] = rf.predict(X_t_cf[te_idx])

    return cluster_ids, y_pred, y_pred_cf


# ------------------------------------------------------------------ #
# Outcome-model templating                                           #
# ------------------------------------------------------------------ #


def _setup_outcome_models(model_outcome, niter: int):
    """
    Expand *model_outcome* into a list of length `niter`.

    Accepts
    -------
    • single estimator ........................ cloned `niter` times
    • list / tuple of estimators .............. must have ≥ niter items
    • generator / iterator yielding estimators  first `niter` are consumed

    Returns
    -------
    list[BaseEstimator]
    """
    # single estimator -------------------------------------------------
    if isinstance(model_outcome, BaseEstimator):
        return [clone(model_outcome) for _ in range(niter)]

    # list / tuple -----------------------------------------------------
    if isinstance(model_outcome, (list, tuple)):
        if len(model_outcome) < niter:
            raise ValueError("`model_outcome` list shorter than niter.")
        return [clone(m) for m in model_outcome[:niter]]

    # generator / iterator --------------------------------------------
    if hasattr(model_outcome, "__iter__"):
        templates = []
        it = iter(model_outcome)
        for _ in range(niter):
            try:
                templates.append(clone(next(it)))
            except StopIteration:
                raise ValueError(
                    "Generator for model_outcome yielded fewer than niter estimators."
                )
        return templates

    raise TypeError(
        "`model_outcome` must be an estimator, a list/tuple of estimators, "
        "or a generator yielding estimators."
    )


# --------------------------------------------------------------------------- #
# Stage-1 single iteration – SURVIVAL (cross-fit, covariate-adjusted HR)      #
# --------------------------------------------------------------------------- #
def _estimate_te_survival_single_iter(
    Xraw: np.ndarray,
    treatment: np.ndarray,
    y: np.ndarray,
    *,
    rng: np.random.Generator,
    # ---- design & matching -------------------------------------------------
    n_splits_propensity: int = 5,
    model_propensity=LogisticRegression(solver="newton-cg"),
    matching_scale: float = 1.0,
    matching_caliper: Optional[float] = None,
    groups: Optional[np.ndarray] = None,
    matching_is_stochastic: bool = True,
    # ---- outcome modelling -------------------------------------------------
    n_splits_outcome: int = 5,
    model_outcome: Optional[BaseEstimator] = None,
    n_mc: int = 1,
    administrative_censoring: bool = True,
    **kwargs,
) -> tuple[float, np.ndarray]:
    """Single iteration survival TE on that iteration's matched set.

    Cross-fits a survival outcome model on the matched units, simulates
    ``n_mc`` draws from the factual and counterfactual survival curves and
    fits a marginal Cox model on those rows to obtain a hazard ratio.
    """
    # No feature subsetting
    X = Xraw

    # ------------ 1. Propensity CV  ----------------------------------------
    splitter_prop = _make_splitter(
        n_splits=n_splits_propensity,
        shuffle=True,
        seed=int(rng.integers(2**32)),
        groups=groups,
    )
    oos_scores = cross_val_predict(
        clone(model_propensity),
        X,
        treatment,
        cv=splitter_prop,
        method="predict_proba",
        groups=groups,
    )
    eps = 1e-6
    p1 = np.clip(oos_scores[:, 1], eps, 1 - eps)
    logit_ps = np.log(p1 / (1 - p1))

    # ------------ 2. Matching ----------------------------------------------
    cluster_ids = stochastic_match(
        treatment=treatment,
        score=logit_ps,
        scale=matching_scale,
        caliper=matching_caliper,
        nsmp=1 if matching_is_stochastic else 0,
        random_state=int(rng.integers(2**32)),
    ).ravel()
    matched_idx = np.where(cluster_ids != -1)[0]
    if matched_idx.size == 0:
        raise ValueError("No matches found – relax caliper/scale.")

    # ------------ 3. Cross-fit outcome model -------------------------------

    # ---- choose survival learner -------------------------------------
    if model_outcome is None:
        model_outcome = RandomSurvivalForest(
            n_estimators=200,
            min_samples_split=10,
            min_samples_leaf=5,
            n_jobs=1,
        )

    X_t = np.hstack((X, treatment.reshape(-1, 1)))
    X_t_cf = np.hstack((X, (1 - treatment).reshape(-1, 1)))

    # containers for predicted survival functions
    sf_factual = [None] * X.shape[0]
    sf_counter = [None] * X.shape[0]

    surv_y = np.array(
        list(zip(y[:, 1] == 1, y[:, 0])),
        dtype=[("event", "bool"), ("time", "f8")],
    )

    splitter_out = _make_splitter(
        n_splits=n_splits_outcome,
        shuffle=True,
        seed=int(rng.integers(2**32)),
        groups=groups,
    )

    for fold, (tr_idx, te_idx) in enumerate(splitter_out.split(X_t, groups=groups)):
        matched_tr = np.intersect1d(matched_idx, tr_idx)
        if matched_tr.size == 0:
            # No matched data in training part of this fold – skip
            continue

        mdl = clone(model_outcome)
        if "random_state" in mdl.get_params(deep=False):
            mdl.set_params(random_state=int(rng.integers(2**32)))
        mdl.fit(X_t[matched_tr], surv_y[matched_tr])

        sf_te_f = mdl.predict_survival_function(X_t[te_idx], return_array=False)
        sf_te_cf = mdl.predict_survival_function(X_t_cf[te_idx], return_array=False)
        for pos, idx in enumerate(te_idx):
            sf_factual[idx] = sf_te_f[pos]
            sf_counter[idx] = sf_te_cf[pos]

    # ensure every matched observation received predictions
    if any(sf_factual[i] is None or sf_counter[i] is None for i in matched_idx):
        raise ValueError(
            "Some matched units lack survival predictions – "
            "increase n_splits_outcome or review data."
        )
    idx = matched_idx

    # ------------ 4. Monte-Carlo simulation --------------------------------
    t_f = np.empty((len(idx), n_mc))
    e_f = np.empty((len(idx), n_mc), dtype=bool)
    t_cf = np.empty_like(t_f)
    e_cf = np.empty_like(e_f)

    tau = float(y[:, 0].max()) if administrative_censoring else math.inf
    for pos, i in enumerate(idx):
        if sf_factual[i] is None or sf_counter[i] is None:
            raise ValueError("Missing survival prediction for a matched row.")
        rng_i = np.random.default_rng(int(rng.integers(2**32)))
        t_f[pos], e_f[pos] = _simulate_from_sf(sf_factual[i], n_mc, tau, rng_i)
        t_cf[pos], e_cf[pos] = _simulate_from_sf(sf_counter[i], n_mc, tau, rng_i)

    times = np.concatenate([t_f.ravel(), t_cf.ravel()])
    events = np.concatenate([e_f.ravel(), e_cf.ravel()])
    d = np.concatenate([np.ones_like(t_f).ravel(), np.zeros_like(t_cf).ravel()])

    synth = np.array(
        list(zip(events == 1, times)), dtype=[("event", "bool"), ("time", "f8")]
    )
    cox = CoxPHSurvivalAnalysis().fit(pd.DataFrame({"d": d}), synth)
    hr = float(np.exp(cox.coef_[0]))
    return hr, cluster_ids


def stage_1_meta_survival(
    Xraw: np.ndarray,
    treatment: np.ndarray,
    y: np.ndarray,
    *,
    rng_master: np.random.Generator,
    outcome_templates: list[BaseEstimator],
    niter: int,
    model_meta: Optional[BaseEstimator] = None,
    n_splits_propensity: int = 5,
    model_propensity=LogisticRegression(solver="newton-cg"),
    matching_scale: float = 1.0,
    matching_caliper: Optional[float] = None,
    n_splits_outcome: int = 5,
    matching_is_stochastic: bool = True,
    groups: Optional[np.ndarray] = None,
    prob_clip_eps: float = 1e-6,
    include_covariates_in_stacking: bool = False,
) -> tuple[
    np.ndarray,
    list[StepFunction | None],
    list[StepFunction | None],
    np.ndarray,
]:
    """Cross-fit a survival meta-learner with appearance weights.

    Base survival models are fitted ``niter`` times. Their predictions form the
    design for a Cox meta-learner that is trained on the **matched union** and
    weighted by each row's appearance count.  Returns the matched-union indices,
    lists of factual and counterfactual survival functions and the cluster
    matrix of matches.
    """

    surv_y = np.array(
        list(zip(y[:, 1] == 1, y[:, 0])),
        dtype=[("event", "bool"), ("time", "f8")],
    )

    results = []
    cluster_list = []
    for i in range(niter):
        rng_iter = np.random.default_rng(int(rng_master.integers(2**32)))
        res = stage_1_single_iter(
            Xraw,
            treatment,
            surv_y,
            rng=rng_iter,
            outcome_is_binary=False,
            groups=groups,
            n_splits_propensity=n_splits_propensity,
            model_propensity=model_propensity,
            matching_scale=matching_scale,
            matching_caliper=matching_caliper,
            n_splits_outcome=n_splits_outcome,
            model_outcome=outcome_templates[i],
            matching_is_stochastic=matching_is_stochastic,
            prob_clip_eps=prob_clip_eps,
        )
        results.append(res)
        cluster_list.append(res[0])
    cluster_mat = np.column_stack(cluster_list)
    weights = appearance_weights(cluster_mat).astype(float)
    matched_union = np.where(weights > 0)[0]

    pred_mat = np.column_stack([r[1] for r in results])
    pred_cf_mat = np.column_stack([r[2] for r in results])

    if include_covariates_in_stacking:
        X_meta = np.hstack((pred_mat, Xraw, treatment.reshape(-1, 1)))
        X_meta_cf = np.hstack((pred_cf_mat, Xraw, (1 - treatment).reshape(-1, 1)))
    else:
        X_meta = np.hstack((pred_mat, treatment.reshape(-1, 1)))
        X_meta_cf = np.hstack((pred_cf_mat, (1 - treatment).reshape(-1, 1)))

    if model_meta is None:
        model_meta = CoxPHSurvivalAnalysis()

    splitter_meta = _make_splitter(
        n_splits=n_splits_outcome,
        shuffle=True,
        seed=int(rng_master.integers(2**32)),
        groups=groups,
    )

    sf_factual = [None] * Xraw.shape[0]
    sf_counter = [None] * Xraw.shape[0]

    for tr_idx, te_idx in splitter_meta.split(X_meta, groups=groups):
        matched_tr = np.intersect1d(matched_union, tr_idx)
        if matched_tr.size == 0:
            raise ValueError(
                "No matched units in training set. "
                "Try increasing `matching_scale` or `matching_caliper`."
            )
        mdl = clone(model_meta)
        if "random_state" in mdl.get_params(deep=False):
            mdl.set_params(random_state=int(rng_master.integers(2**32)))
        fit_with_appearance_weights(
            mdl,
            X_meta[matched_tr],
            surv_y[matched_tr],
            sample_weight=weights[matched_tr],
        )

        sf_te = mdl.predict_survival_function(X_meta[te_idx], return_array=False)
        sf_te_cf = mdl.predict_survival_function(X_meta_cf[te_idx], return_array=False)
        for pos, idx in enumerate(te_idx):
            sf_factual[idx] = sf_te[pos]
            sf_counter[idx] = sf_te_cf[pos]

    return matched_union, sf_factual, sf_counter, cluster_mat


def stage_1_meta_survival_multi(
    Xraw: np.ndarray,
    treatment: np.ndarray,
    y: np.ndarray,
    *,
    rng_master: np.random.Generator,
    outcome_templates: list[BaseEstimator],
    niter: int,
    model_meta: Optional[BaseEstimator] = None,
    n_splits_propensity: int = 5,
    model_propensity=LogisticRegression(solver="newton-cg"),
    matching_scale: float = 1.0,
    matching_caliper: Optional[float] = None,
    n_splits_outcome: int = 5,
    matching_is_stochastic: bool = True,
    groups: Optional[np.ndarray] = None,
    prob_clip_eps: float = 1e-6,
    ref_group: int | str | None = None,
    estimand: str = "ATM",
    include_covariates_in_stacking: bool = False,
) -> tuple[np.ndarray, list[list[StepFunction | None]], np.ndarray, np.ndarray,]:
    """Cross-fit a survival meta-learner for multiple treatments.

    Iterative base learners supply survival predictions for each arm. A Cox
    meta-learner is fitted on the **matched union** with appearance weights and
    produces survival functions per treatment. Returns matched-union indices,
    survival-function lists for all arms, treatment names and the cluster
    matrix.
    """

    surv_y = np.array(
        list(zip(y[:, 1] == 1, y[:, 0])), dtype=[("event", "bool"), ("time", "f8")]
    )

    results = []
    cluster_list = []
    for i in range(niter):
        rng_iter = np.random.default_rng(int(rng_master.integers(2**32)))
        res = stage_1_single_iter_multi(
            Xraw,
            treatment,
            surv_y,
            rng=rng_iter,
            outcome_is_binary=False,
            groups=groups,
            n_splits_propensity=n_splits_propensity,
            model_propensity=model_propensity,
            matching_scale=matching_scale,
            matching_caliper=matching_caliper,
            n_splits_outcome=n_splits_outcome,
            model_outcome=outcome_templates[i],
            matching_is_stochastic=matching_is_stochastic,
            prob_clip_eps=prob_clip_eps,
            ref_group=ref_group,
        )
        results.append(res)
        cluster_list.append(res[0])

    treatment_names = results[0][3]
    cluster_mat = np.column_stack(cluster_list)
    weights = appearance_weights(cluster_mat).astype(float)
    matched_union = np.where(weights > 0)[0]

    pred_mat = np.column_stack([r[1] for r in results])
    pred_all_arms = []
    for j in range(len(treatment_names)):
        pred_all_arms.append(np.column_stack([r[2][j] for r in results]))

    enc = OneHotEncoder(sparse_output=False, handle_unknown="error")
    enc.fit(treatment.reshape(-1, 1))
    treatment_levels = enc.categories_[0]
    treatment_level_lookup = {str(level): level for level in treatment_levels}

    if include_covariates_in_stacking:
        X_meta = np.hstack((pred_mat, Xraw, enc.transform(treatment.reshape(-1, 1))))
        X_meta_all_arms = []
        for name in treatment_names:
            level = treatment_level_lookup.get(str(name))
            if level is None:
                raise ValueError(
                    "Treatment name from stage-1 predictions not found in encoder categories."
                )
            X_meta_all_arms.append(
                np.hstack(
                    (
                        pred_all_arms[treatment_names.tolist().index(name)],
                        Xraw,
                        enc.transform(np.full(treatment.shape, level).reshape(-1, 1)),
                    )
                )
            )
    else:
        X_meta = np.hstack((pred_mat, enc.transform(treatment.reshape(-1, 1))))
        X_meta_all_arms = []
        for name in treatment_names:
            level = treatment_level_lookup.get(str(name))
            if level is None:
                raise ValueError(
                    "Treatment name from stage-1 predictions not found in encoder categories."
                )
            X_meta_all_arms.append(
                np.hstack(
                    (
                        pred_all_arms[treatment_names.tolist().index(name)],
                        enc.transform(np.full(treatment.shape, level).reshape(-1, 1)),
                    )
                )
            )

    if model_meta is None:
        model_meta = CoxPHSurvivalAnalysis()

    splitter_meta = _make_splitter(
        n_splits=n_splits_outcome,
        shuffle=True,
        seed=int(rng_master.integers(2**32)),
        groups=groups,
    )

    sf_all_arms = [[None] * Xraw.shape[0] for _ in treatment_names]

    for tr_idx, te_idx in splitter_meta.split(X_meta, groups=groups):
        matched_tr = np.intersect1d(matched_union, tr_idx)
        if matched_tr.size == 0:
            raise ValueError(
                "No matched units in training set. "
                "Try increasing `matching_scale` or `matching_caliper`."
            )
        mdl = clone(model_meta)
        if "random_state" in mdl.get_params(deep=False):
            mdl.set_params(random_state=int(rng_master.integers(2**32)))
        fit_with_appearance_weights(
            mdl,
            X_meta[matched_tr],
            surv_y[matched_tr],
            sample_weight=weights[matched_tr],
        )

        for k, X_meta_single_arm in enumerate(X_meta_all_arms):
            sf_tmp = mdl.predict_survival_function(
                X_meta_single_arm[te_idx], return_array=False
            )
            for pos, idx in enumerate(te_idx):
                sf_all_arms[k][idx] = sf_tmp[pos]

    return matched_union, sf_all_arms, treatment_names, cluster_mat


# --------------------------------------------------------------------------- #
# Survival-path placeholder (multi-iter loop)                                 #
# --------------------------------------------------------------------------- #
def _estimate_te_survival(
    Xraw: np.ndarray,
    treatment: np.ndarray,
    y: np.ndarray,
    *,
    rng_master: np.random.Generator,
    outcome_templates: list[BaseEstimator],
    niter: int,
    model_meta: Optional[BaseEstimator] = None,
    n_mc: int = 1,
    administrative_censoring: bool = True,
    n_splits_propensity: int = 5,
    model_propensity=LogisticRegression(solver="newton-cg"),
    matching_scale: float = 1.0,
    matching_caliper: Optional[float] = None,
    n_splits_outcome: int = 5,
    matching_is_stochastic: bool = True,
    groups: Optional[np.ndarray] = None,
    prob_clip_eps: float = 1e-6,
    do_stacking: bool = True,
    estimand: str = "ATM",
    include_covariates_in_stacking: bool = False,
) -> dict:
    """Estimate TE for survival outcomes.

    When ``do_stacking`` is ``True`` a Cox meta-learner is fitted on stacked
    predictions from ``niter`` base learners.  The meta-learner and final
    marginal hazard ratio are **appearance-weighted** over the matched union.
    When ``do_stacking`` is ``False`` each iteration produces an HR on that
    iteration's matched set and the geometric mean (unweighted) is returned.
    """
    if niter == 1:
        do_stacking = False

    if not do_stacking:
        hr_list = []
        cluster_list = []
        for i in range(niter):
            rng_iter = np.random.default_rng(int(rng_master.integers(2**32)))
            hr_i, cid = _estimate_te_survival_single_iter(
                Xraw,
                treatment,
                y,
                rng=rng_iter,
                n_splits_propensity=n_splits_propensity,
                model_propensity=model_propensity,
                matching_scale=matching_scale,
                matching_caliper=matching_caliper,
                n_splits_outcome=n_splits_outcome,
                model_outcome=outcome_templates[i],
                matching_is_stochastic=matching_is_stochastic,
                n_mc=n_mc,
                administrative_censoring=administrative_censoring,
            )
            hr_list.append(math.log(hr_i))
            cluster_list.append(cid)

        hr = float(np.exp(np.mean(hr_list)))
        cluster_mat = np.column_stack(cluster_list)
        return {"te": hr, "matching": cluster_mat}

    matched_union, sf_fact, sf_cf, cluster_mat = stage_1_meta_survival(
        Xraw,
        treatment,
        y,
        rng_master=rng_master,
        outcome_templates=outcome_templates,
        niter=niter,
        model_meta=model_meta,
        n_splits_propensity=n_splits_propensity,
        model_propensity=model_propensity,
        matching_scale=matching_scale,
        matching_caliper=matching_caliper,
        n_splits_outcome=n_splits_outcome,
        matching_is_stochastic=matching_is_stochastic,
        groups=groups,
        prob_clip_eps=prob_clip_eps,
        include_covariates_in_stacking=include_covariates_in_stacking,
    )

    # Filter matched units based on estimand
    if estimand == "ATT":
        # MATCHED TREATED ONLY for survival
        treated_idx = np.where(treatment == 1)[0]
        idx = np.intersect1d(matched_union, treated_idx)
    elif estimand == "ATM":
        idx = matched_union  # All matched (current)
    else:
        raise ValueError(f"estimand='{estimand}' not yet implemented")
    t_f = np.empty((len(idx), n_mc))
    e_f = np.empty((len(idx), n_mc), dtype=bool)
    t_cf = np.empty_like(t_f)
    e_cf = np.empty_like(e_f)

    tau = float(y[:, 0].max()) if administrative_censoring else math.inf
    for pos, i in enumerate(idx):
        if sf_fact[i] is None or sf_cf[i] is None:
            raise ValueError("Missing survival prediction for a matched row.")
        rng_i = np.random.default_rng(int(rng_master.integers(2**32)))
        t_f[pos], e_f[pos] = _simulate_from_sf(sf_fact[i], n_mc, tau, rng_i)
        t_cf[pos], e_cf[pos] = _simulate_from_sf(sf_cf[i], n_mc, tau, rng_i)

    times = np.concatenate([t_f.ravel(), t_cf.ravel()])
    events = np.concatenate([e_f.ravel(), e_cf.ravel()])
    d = np.concatenate([np.ones_like(t_f).ravel(), np.zeros_like(t_cf).ravel()])

    synth = np.array(
        list(zip(events == 1, times)), dtype=[("event", "bool"), ("time", "f8")]
    )
    w_all = appearance_weights(cluster_mat)
    w_rows = np.concatenate([np.repeat(w_all[idx], n_mc), np.repeat(w_all[idx], n_mc)])
    hr = fit_cox_marginal_weighted(d, synth, w_rows)
    return {"te": hr, "matching": cluster_mat}


# --------------------------------------------------------------------------- #
# Full estimator (bootstrap + fast-path)                                      #
# --------------------------------------------------------------------------- #
def estimate_te(
    Xraw: np.ndarray,
    treatment: np.ndarray,
    y: np.ndarray,
    *,
    groups: Optional[np.ndarray] = None,
    n_splits_propensity: int = 5,
    model_propensity=LogisticRegression(solver="newton-cg"),
    matching_scale: float = 1.0,
    matching_caliper: Optional[float] = None,
    n_splits_outcome: int = 5,
    model_outcome=None,
    outcome_type: Optional[str] = None,  # "continuous" | "binary" | "survival"
    niter: int = 10,
    matching_is_stochastic: bool = True,
    do_stacking: bool = True,
    prob_clip_eps: float = 1e-6,
    n_mc: int = 1,
    # --- RNG control ------------------------------------------------------
    random_state_master: Optional[int] = None,
    # --- bootstrap options -----------------------------------------------
    nboot: int = 0,
    alpha: float = 0.05,
    n_jobs: int = -1,
    random_state_boot: Optional[int] = None,
    model_meta: Optional[object] = None,
    ref_group: int | str | None = None,
    estimand: str = "ATM",
    include_covariates_in_stacking: bool = False,
):
    """Estimate the average treatment effect via ensemble matching.

    The estimator performs cross-fitted propensity modelling, stochastic (or
    deterministic) matching, outcome modelling and optionally bootstrap
    resampling.  When ``niter`` is ``1`` and ``outcome_type`` is not
    ``"survival"`` the meta-learner is bypassed automatically.

    Parameters
    ----------
    Xraw : ndarray of shape (n, p)
        Raw covariate matrix.
    treatment : ndarray of shape (n,)
        Binary or multi-level treatment indicator.
    y : ndarray
        Outcome.  For survival outcomes pass a ``(time, event)`` pair per
        observation.
    groups : ndarray or None, optional
        Group labels used for cross-validation splits.
    n_splits_propensity : int, default ``5``
        Number of folds for propensity-score cross-fitting.
    model_propensity : estimator, default ``LogisticRegression``
        Classifier used to estimate propensity scores.
    matching_scale : float, default ``1.0``
        Scale parameter passed to :func:`stochastic_match`.
    matching_caliper : float or None, default ``None``
        Maximum allowable matching distance.
    n_splits_outcome : int, default ``5``
        Number of folds for outcome-model cross-fitting.
    model_outcome : estimator or list/tuple/generator of estimators, optional
        Base learner(s) for outcome prediction. Supports heterogeneous ensembles
        where different models are used across iterations. Three input formats:

        - **Single estimator**: Cloned ``niter`` times (homogeneous ensemble).
          All iterations use the same model type with different random seeds.

        - **List or tuple**: Must contain at least ``niter`` estimators. The first
          ``niter`` models are used for successive iterations (heterogeneous
          ensemble). Allows mixing different model types (e.g., Random Forest +
          Gradient Boosting + Linear models).

        - **Generator/iterator**: Must yield at least ``niter`` estimators. The
          first ``niter`` models are consumed for successive iterations.

        When ``None`` (default), appropriate models are selected based on
        ``outcome_type``:

        - Continuous: :class:`sklearn.ensemble.RandomForestRegressor`
        - Binary: :class:`sklearn.ensemble.RandomForestClassifier`
        - Survival: :class:`sksurv.ensemble.RandomSurvivalForest`

        Heterogeneous ensembles can improve robustness by combining models with
        different inductive biases, especially when optimal model choice is
        uncertain.
    outcome_type : {"continuous", "binary", "survival"} or None, optional
        Type of ``y``.  ``None`` triggers automatic detection.
    niter : int, default ``10``
        Number of stage‑1 iterations before meta-learning.
    matching_is_stochastic : bool, default ``True``
        Use stochastic matching when ``True`` otherwise deterministic.
    do_stacking : bool, default ``True``
        When ``False`` bypass the meta-learner and average treatment effects
        across iterations.
    prob_clip_eps : float, default ``1e-6``
        Epsilon for probability clipping before taking logits.
    n_mc : int, default ``1``
        Number of Monte Carlo draws per matched unit for survival outcomes.
    random_state_master : int or None, optional
        Seed controlling all stochastic elements except the bootstrap.
    nboot : int, default ``0``
        Number of bootstrap resamples.  ``0`` disables bootstrapping.
    alpha : float, default ``0.05``
        Significance level for percentile confidence intervals.
    n_jobs : int, default ``-1``
        Parallel jobs for the bootstrap.
    random_state_boot : int or None, optional
        Seed for bootstrap resampling.
    model_meta : object, optional
        Meta-learner fitted on stacked predictions.
    ref_group : int or str or None, optional
        Reference treatment arm when more than two levels are present.
    estimand : str, default 'ATM'
        Which population to target for treatment effect estimation.

        - 'ATM': Average Treatment Effect on Matched sample (default)
          Effect averaged over all units that appear in matched sets.

        - 'ATT': Average Treatment Effect on Treated (matched subset)
          For binary treatment: Effect on T=1 units that were successfully matched.
          For multi-arm: Effect on ref_group units that were successfully matched.

          IMPORTANT: Computes effect on MATCHED treated units only, not all
          treated units. This is "ATT on the common support" - standard in
          matching literature. Excludes unmatched treated units (e.g., when
          using caliper) as their counterfactual predictions are unreliable.

        - 'ATE': Average Treatment Effect (full population) - not yet implemented
    include_covariates_in_stacking : bool, default False
        When ``True`` and ``do_stacking=True``, include baseline covariates
        (``Xraw``) in the stage-2 meta-learner design matrix alongside stage-1
        predictions and treatment indicators. This allows the meta-learner to
        directly leverage covariate information when combining base predictions,
        potentially improving estimates when base learners incompletely adjust
        for confounding.

        When ``False`` (default), the meta-learner receives only predictions
        and treatment indicators, matching the original implementation.

        Ignored when ``do_stacking=False`` (no meta-learner used).

        .. versionadded:: 1.2.0

    Returns
    -------
    dict
        Dictionary containing the key ``"te"`` with the estimated effect and a
        ``"matching"`` matrix of cluster identifiers with shape ``(n, niter)``.
        When ``nboot`` is greater than zero an additional ``"ci"`` tuple with
        percentile bounds and ``"boot"`` array of bootstrap estimates are
        included.

    Notes
    -----
    * **Two-arm designs** – the result is ``{"te": float}`` (always with a
    ``"matching"`` matrix) plus optional keys ``"ci"`` and ``"boot"`` when
    bootstrapping is requested.
    * **Multi-arm designs** – the result is
    ``{"pairwise": pandas.DataFrame, ...}`` (again with ``"matching"``)
    where each row holds one treatment comparison (and optional ``"boot"``
    dict).
    * Use :pyfunc:`causalem.as_pairwise` to obtain a uniform pairwise dataframe
    from either form.
    * **Estimands:**
      - **No-stacking:** Each iteration yields a treatment effect on that
        iteration's matched set. Results are averaged across iterations with no
        appearance weighting (geometric mean for survival HRs).
      - **Stacking:** Base predictions are combined via a meta-learner fitted
        with appearance weights. Final non-survival averages and survival
        hazard ratios are appearance-weighted over the matched union.

    Examples
    --------
    **Heterogeneous ensemble with list of models:**

    >>> from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
    >>> from sklearn.linear_model import LinearRegression
    >>> outcome_models = [
    ...     RandomForestRegressor(n_estimators=100),
    ...     GradientBoostingRegressor(n_estimators=100),
    ...     LinearRegression()
    ... ]
    >>> result = estimate_te(
    ...     X, t, y,
    ...     outcome_type="continuous",
    ...     model_outcome=outcome_models,
    ...     niter=3,
    ...     random_state_master=42
    ... )

    **Heterogeneous ensemble with generator:**

    >>> def outcome_generator():
    ...     for max_depth in [3, 5, 7]:
    ...         yield RandomForestRegressor(n_estimators=100, max_depth=max_depth)
    >>> result = estimate_te(
    ...     X, t, y,
    ...     outcome_type="continuous",
    ...     model_outcome=outcome_generator(),
    ...     niter=3,
    ...     random_state_master=42
    ... )
    """
    # Validate estimand parameter
    allowed_estimands = {"ATM", "ATT", "ATE"}
    if estimand not in allowed_estimands:
        raise ValueError(
            f"estimand='{estimand}' not recognized. Must be one of {allowed_estimands}."
        )
    if estimand == "ATE":
        raise NotImplementedError(
            "estimand='ATE' is not yet implemented. Use 'ATM' or 'ATT'."
        )

    # Quick branch: multi-treatment → dedicated pipeline
    if _is_multi_treatment(treatment):
        return estimate_te_multi(  # defined further down in this file
            Xraw=Xraw,
            treatment=treatment,
            y=y,
            groups=groups,
            n_splits_propensity=n_splits_propensity,
            model_propensity=model_propensity,
            matching_scale=matching_scale,
            matching_caliper=matching_caliper,
            n_splits_outcome=n_splits_outcome,
            model_outcome=model_outcome,
            outcome_type=outcome_type,
            niter=niter,
            matching_is_stochastic=matching_is_stochastic,
            do_stacking=do_stacking,
            prob_clip_eps=prob_clip_eps,
            n_mc=n_mc,
            random_state_master=random_state_master,
            nboot=nboot,
            alpha=alpha,
            n_jobs=n_jobs,
            random_state_boot=random_state_boot,
            model_meta=model_meta,
            ref_group=ref_group,
            estimand=estimand,
            include_covariates_in_stacking=include_covariates_in_stacking,
        )

    # -------------------------------------------------------------- #
    # Determine outcome_type  ("continuous" | "binary" | "survival") #
    # -------------------------------------------------------------- #
    if outcome_type is None:
        # crude auto-detection
        if isinstance(y, np.ndarray) and y.ndim == 2 and y.shape[1] == 2:
            outcome_type = "survival"
        elif np.array_equal(np.unique(y), [0, 1]) or np.array_equal(
            np.unique(y), [0.0, 1.0]
        ):
            outcome_type = "binary"
        else:
            outcome_type = "continuous"
    else:
        allowed = {"continuous", "binary", "survival"}
        if outcome_type not in allowed:
            raise ValueError(f"`outcome_type` must be one of {allowed}.")
        if outcome_type == "binary" and not np.array_equal(np.unique(y), [0, 1]):
            raise ValueError("`outcome_type='binary'` but y is not {0,1}.")

    is_binary = outcome_type == "binary"

    if niter == 1 and outcome_type != "survival":
        do_stacking = False

    # Validate include_covariates_in_stacking parameter
    if include_covariates_in_stacking and not do_stacking:
        warnings.warn(
            "include_covariates_in_stacking=True ignored when do_stacking=False",
            stacklevel=2,
        )

    # ------------------------------------------------------------------ #
    # Global RNG for this call                                           #
    # ------------------------------------------------------------------ #
    rng_master = np.random.default_rng(random_state_master)

    # ------------------------------------------------------------------ #
    # Default learner, then build templates for *all* outcome types      #
    # ------------------------------------------------------------------ #
    if model_outcome is None:
        if outcome_type == "survival":
            model_outcome = RandomSurvivalForest(n_estimators=100)
        elif outcome_type == "continuous":
            model_outcome = RandomForestRegressor(n_estimators=100)
        else:
            model_outcome = RandomForestClassifier(n_estimators=100)

    outcome_templates = _setup_outcome_models(model_outcome, niter)

    # ------------------------------------------------------------------ #
    # 1. Bootstrap wrapper (recursion)                                   #
    # ------------------------------------------------------------------ #
    if nboot > 0:
        if groups is not None:
            warnings.warn("`groups` ignored when bootstrapping.", stacklevel=2)

        rng_boot = np.random.default_rng(random_state_boot)
        seeds_worker = rng_boot.integers(0, 2**32, size=nboot)

        def _single_boot(seed: int) -> float:
            rng_local = np.random.default_rng(seed)
            idx = rng_local.integers(0, Xraw.shape[0], size=Xraw.shape[0])
            te = estimate_te(
                Xraw[idx],
                treatment[idx],
                y[idx],
                groups=np.asarray(idx),
                n_splits_propensity=n_splits_propensity,
                model_propensity=model_propensity,
                matching_scale=matching_scale,
                matching_caliper=matching_caliper,
                n_splits_outcome=n_splits_outcome,
                model_outcome=model_outcome,
                outcome_type=outcome_type,
                niter=niter,
                matching_is_stochastic=matching_is_stochastic,
                do_stacking=do_stacking,
                prob_clip_eps=prob_clip_eps,
                n_mc=n_mc,
                random_state_master=seed,
                nboot=0,
                model_meta=model_meta,
                ref_group=ref_group,
                estimand=estimand,
                include_covariates_in_stacking=include_covariates_in_stacking,
            )["te"]
            return te

        boot_stats = Parallel(n_jobs=n_jobs, backend="loky")(
            delayed(_single_boot)(int(s)) for s in seeds_worker
        )
        boot_stats = np.asarray(boot_stats)

        theta_res = estimate_te(
            Xraw,
            treatment,
            y,
            groups=None,
            n_splits_propensity=n_splits_propensity,
            model_propensity=model_propensity,
            matching_scale=matching_scale,
            matching_caliper=matching_caliper,
            n_splits_outcome=n_splits_outcome,
            model_outcome=model_outcome,
            outcome_type=outcome_type,
            niter=niter,
            matching_is_stochastic=matching_is_stochastic,
            do_stacking=do_stacking,
            prob_clip_eps=prob_clip_eps,
            n_mc=n_mc,
            random_state_master=random_state_master,
            nboot=0,
            model_meta=model_meta,
            ref_group=ref_group,
            estimand=estimand,
            include_covariates_in_stacking=include_covariates_in_stacking,
        )
        theta_hat = theta_res["te"]
        cluster_mat = theta_res["matching"]

        lo, hi = np.percentile(boot_stats, [100 * alpha / 2, 100 * (1 - alpha / 2)])
        return {
            "te": theta_hat,
            "ci": (lo, hi),
            "boot": boot_stats,
            "matching": cluster_mat,
        }

    # ------------------------------------------------------------------ #
    # Survival pathway (placeholder)                                     #
    # ------------------------------------------------------------------ #
    if outcome_type == "survival":
        return _estimate_te_survival(
            Xraw,
            treatment,
            y,
            rng_master=rng_master,
            n_splits_propensity=n_splits_propensity,
            model_propensity=model_propensity,
            matching_scale=matching_scale,
            matching_caliper=matching_caliper,
            n_splits_outcome=n_splits_outcome,
            outcome_templates=outcome_templates,
            niter=niter,
            matching_is_stochastic=matching_is_stochastic,
            model_meta=model_meta,
            do_stacking=do_stacking,
            groups=groups,
            n_mc=n_mc,
            estimand=estimand,
            include_covariates_in_stacking=include_covariates_in_stacking,
        )

    # ------------------------------------------------------------------ #
    # 2. Multi-iteration pipeline                                        #
    # ------------------------------------------------------------------ #
    results = []
    cluster_list = []
    te_list = []
    for _ in range(niter):
        rng_iter = np.random.default_rng(int(rng_master.integers(2**32)))
        res = stage_1_single_iter(
            Xraw,
            treatment,
            y,
            rng=rng_iter,
            outcome_is_binary=is_binary,
            groups=groups,
            n_splits_propensity=n_splits_propensity,
            model_propensity=model_propensity,
            matching_scale=matching_scale,
            matching_caliper=matching_caliper,
            n_splits_outcome=n_splits_outcome,
            model_outcome=outcome_templates[_],  # <-- heterogeneous!
            matching_is_stochastic=matching_is_stochastic,
            prob_clip_eps=prob_clip_eps,
        )
        results.append(res)
        cluster_list.append(res[0])
        if not do_stacking:
            cid, yp, yp_cf = res
            # Determine matched units based on estimand
            if estimand == "ATT":
                # MATCHED TREATED ONLY: (cid != -1) ensures matched, (treatment == 1) ensures treated
                # This gives "ATT on common support" - standard in matching literature
                matched_idx = np.where((cid != -1) & (treatment == 1))[0]
            elif estimand == "ATM":
                matched_idx = np.where(cid != -1)[0]  # ALL matched (current)
            else:
                raise ValueError(
                    f"estimand='{estimand}' not supported in no-stacking mode"
                )

            yp_treat = np.where(treatment == 1, yp, yp_cf)
            yp_ctrl = np.where(treatment == 1, yp_cf, yp)
            te_list.append(
                np.mean(yp_treat[matched_idx]) - np.mean(yp_ctrl[matched_idx])
            )

    cluster_mat = np.column_stack(cluster_list)
    if not do_stacking:
        te_hat = float(np.mean(te_list))
        return {"te": te_hat, "matching": cluster_mat}

    weights = appearance_weights(cluster_mat)
    # Get ALL matched units for training (used by meta-learner)
    matched_union = np.where(weights > 0)[0]

    y_pred_mat = np.column_stack([r[1] for r in results])
    y_pred_cf_mat = np.column_stack([r[2] for r in results])

    # for binary outcomes, clip+logit each column of the predictions
    if is_binary:
        y_pred_mat = clip_logit(y_pred_mat, eps=prob_clip_eps)
        y_pred_cf_mat = clip_logit(y_pred_cf_mat, eps=prob_clip_eps)

    # add covariates and treatment indicator to y_pred_mat and y_pred_cf_mat
    if include_covariates_in_stacking:
        y_pred_mat = np.hstack((y_pred_mat, Xraw, treatment.reshape(-1, 1)))
        y_pred_cf_mat = np.hstack((y_pred_cf_mat, Xraw, (1 - treatment).reshape(-1, 1)))
    else:
        y_pred_mat = np.hstack((y_pred_mat, treatment.reshape(-1, 1)))
        y_pred_cf_mat = np.hstack((y_pred_cf_mat, (1 - treatment).reshape(-1, 1)))

    splitter_meta = _make_splitter(
        n_splits=n_splits_outcome,
        shuffle=True,
        seed=int(rng_master.integers(2**32)),
        groups=groups,
    )
    if model_meta is None:
        if is_binary:
            model_meta = LogisticRegression(solver="newton-cg")
        else:
            model_meta = LinearRegression()
    assert isinstance(model_meta, BaseEstimator)

    y_final = np.full(Xraw.shape[0], np.nan)
    y_final_cf = np.full(Xraw.shape[0], np.nan)

    for tr_idx, te_idx in splitter_meta.split(Xraw, groups=groups):
        matched_tr = np.intersect1d(matched_union, tr_idx)
        if matched_tr.size == 0:
            raise ValueError(
                "No matched units in training set. "
                "Try increasing `matching_scale` or `matching_caliper`."
            )
        fit_with_appearance_weights(
            model_meta,
            y_pred_mat[matched_tr],
            y[matched_tr],
            sample_weight=weights[matched_tr],
        )
        if is_binary and hasattr(model_meta, "predict_proba"):
            y_final[te_idx] = model_meta.predict_proba(y_pred_mat[te_idx])[:, 1]
            y_final_cf[te_idx] = model_meta.predict_proba(y_pred_cf_mat[te_idx])[:, 1]
        else:
            y_final[te_idx] = model_meta.predict(y_pred_mat[te_idx])
            y_final_cf[te_idx] = model_meta.predict(y_pred_cf_mat[te_idx])

    y_final_treat = np.where(treatment == 1, y_final, y_final_cf)
    y_final_ctrl = np.where(treatment == 1, y_final_cf, y_final)

    # Determine which units to average over based on estimand
    # CRITICAL: This filtering happens AFTER meta-learner training
    if estimand == "ATT":
        # MATCHED TREATED ONLY for final averaging
        # Meta-learner was trained on ALL matched (both T and C) for better accuracy
        # Now we average predictions over treated units only
        treated_idx = np.where(treatment == 1)[0]
        matched_union_for_avg = np.intersect1d(matched_union, treated_idx)
        weights_for_avg = weights[matched_union_for_avg]
    elif estimand == "ATM":
        # ALL matched for final averaging
        matched_union_for_avg = matched_union
        weights_for_avg = weights[matched_union_for_avg]
    else:
        raise ValueError(f"estimand='{estimand}' not yet implemented")

    te_hat = np.average(
        y_final_treat[matched_union_for_avg], weights=weights_for_avg
    ) - np.average(y_final_ctrl[matched_union_for_avg], weights=weights_for_avg)
    return {"te": te_hat, "matching": cluster_mat}


def _is_multi_treatment(t):
    """Return True if *t* has more than two unique, non-nan values."""
    u = np.unique(t[~pd.isna(t)])
    return len(u) > 2


def stage_1_single_iter_multi(
    Xraw: np.ndarray,
    treatment: np.ndarray,
    y: np.ndarray,
    *,
    rng: np.random.Generator,
    outcome_is_binary: bool,
    groups: Optional[np.ndarray] = None,
    n_splits_propensity: int = 5,
    model_propensity=LogisticRegression(solver="newton-cg"),
    matching_scale: float = 1.0,
    matching_caliper: Optional[float] = None,
    n_splits_outcome: int = 5,
    model_outcome=RandomForestRegressor(n_estimators=100),
    matching_is_stochastic: bool = True,
    prob_clip_eps: float = 1e-6,
    ref_group: int | str | None = None,
):
    """Single iteration for multi-arm treatment matching.

    Parameters are analogous to :func:`stage_1_single_iter` with the addition of
    ``ref_group`` which specifies the reference treatment arm when more than two
    levels are present.

    Returns
    -------
    cluster_ids : ndarray
        Cluster identifiers for the matching draw (``-1`` for unmatched).
    y_pred : ndarray
        Out-of-sample predictions for the factual outcome.
    y_pred_all_arms : list of ndarray
        Per-arm predictions for every unit with the treatment code clamped to each arm (including the observed arm).
    treatment_names : ndarray
        Names of the treatment levels in the order corresponding to
        ``y_pred_all_arms``.
    """

    # Provide a default learner when none is supplied
    if model_outcome is None:
        model_outcome = RandomForestRegressor(n_estimators=100)

    # No feature subsetting
    X = Xraw

    # --- propensity CV -----------------------------------------------------
    splitter_prop = _make_splitter(
        n_splits=n_splits_propensity,
        shuffle=True,
        seed=int(rng.integers(2**32)),
        groups=groups,
    )

    # --- propensity CV & logit --------------------------------------------
    oos_proba = cross_val_predict(
        clone(model_propensity),
        X,
        treatment,
        cv=splitter_prop,
        method="predict_proba",
        groups=groups,
    )
    oos_scores = clip_logit(oos_proba, eps=prob_clip_eps)

    # --- matching ----------------------------------------------------------
    cluster_ids = stochastic_match(
        treatment=treatment,
        score=oos_scores,
        scale=matching_scale,
        caliper=matching_caliper,
        nsmp=1 if matching_is_stochastic else 0,
        random_state=int(rng.integers(2**32)),
        ref_group=ref_group,
    ).ravel()
    # return cluster_ids
    matched_idx = np.where(cluster_ids != -1)[0]

    # --- outcome cross-fitting --------------------------------------------
    splitter_out = _make_splitter(
        n_splits=n_splits_outcome,
        shuffle=True,
        seed=int(rng.integers(2**32)),
        groups=groups,
    )

    enc = OneHotEncoder(sparse_output=False, handle_unknown="error")
    enc.fit(treatment.reshape(-1, 1))
    treatment_levels = enc.categories_[0]
    treatment_names = treatment_levels.astype(str)

    X_t_actual = np.hstack((X, enc.transform(treatment.reshape(-1, 1))))
    X_t_all_arms = []
    for treatment_value in treatment_levels:
        X_t_single_arm = np.hstack(
            (X, enc.transform(np.full(treatment.shape, treatment_value).reshape(-1, 1)))
        )
        X_t_all_arms.append(X_t_single_arm)

    y_pred = np.full(X_t_actual.shape[0], np.nan)
    y_pred_all_arms = []
    for X_t_single_arm in X_t_all_arms:
        y_pred_all_arms.append(np.full(X_t_single_arm.shape[0], np.nan))

    for tr_idx, te_idx in splitter_out.split(X_t_actual, groups=groups):
        matched_tr = np.intersect1d(matched_idx, tr_idx)
        if matched_tr.size == 0:
            continue
        rf = clone(model_outcome)
        if "random_state" in rf.get_params(deep=False):
            rf.set_params(random_state=int(rng.integers(2**32)))
        rf.fit(X_t_actual[matched_tr], y[matched_tr])

        if outcome_is_binary and hasattr(rf, "predict_proba"):
            y_pred[te_idx] = rf.predict_proba(X_t_actual[te_idx])[:, 1]
            for i, X_t_single_arm in enumerate(X_t_all_arms):
                y_pred_all_arms[i][te_idx] = rf.predict_proba(X_t_single_arm[te_idx])[
                    :, 1
                ]
        else:
            y_pred[te_idx] = rf.predict(X_t_actual[te_idx])
            for i, X_t_single_arm in enumerate(X_t_all_arms):
                y_pred_all_arms[i][te_idx] = rf.predict(X_t_single_arm[te_idx])

    return cluster_ids, y_pred, y_pred_all_arms, treatment_names


def estimate_te_multi(
    Xraw: np.ndarray,
    treatment: np.ndarray,
    y: np.ndarray,
    *,
    groups: Optional[np.ndarray] = None,
    n_splits_propensity: int = 5,
    model_propensity=LogisticRegression(solver="newton-cg"),
    matching_scale: float = 1.0,
    matching_caliper: Optional[float] = None,
    n_splits_outcome: int = 5,
    model_outcome=None,
    outcome_type: Optional[str] = None,  # "continuous" | "binary" | "survival"
    niter: int = 10,
    matching_is_stochastic: bool = True,
    do_stacking: bool = True,
    prob_clip_eps: float = 1e-6,
    n_mc: int = 1,
    # --- RNG control ------------------------------------------------------
    random_state_master: Optional[int] = None,
    # --- bootstrap options -----------------------------------------------
    nboot: int = 0,
    alpha: float = 0.05,
    n_jobs: int = -1,
    random_state_boot: Optional[int] = None,
    model_meta: Optional[object] = None,
    ref_group: int | str | None = None,
    estimand: str = "ATM",
    include_covariates_in_stacking: bool = False,
) -> dict:
    """Estimate treatment effects for multi-arm designs.

    ``n_mc`` controls the number of Monte Carlo draws per matched unit when the
    outcome is survival.

    Parameters
    ----------
    include_covariates_in_stacking : bool, default False
        When ``True`` and ``do_stacking=True``, include baseline covariates
        (``Xraw``) in the stage-2 meta-learner design matrix alongside stage-1
        predictions and treatment indicators. This allows the meta-learner to
        directly leverage covariate information when combining base predictions,
        potentially improving estimates when base learners incompletely adjust
        for confounding.

        When ``False`` (default), the meta-learner receives only predictions
        and treatment indicators, matching the original implementation.

        Ignored when ``do_stacking=False`` (no meta-learner used).

        .. versionadded:: 1.2.0

    Returns
    -------
    dict
        Always returns the keys ``"per_treatment"``, ``"pairwise"``, ``"boot"``
        and ``"matching"``.

    ``per_treatment``
        DataFrame with columns ``["treatment", "mean"]`` and optional
        confidence interval columns ``"lo"`` and ``"hi"`` when bootstrapping.
        For survival outcomes this dataframe is empty.

    ``pairwise``
        DataFrame with columns ``["treatment_1", "treatment_2", "te"]`` and
        optional ``"lo"``/``"hi"`` when bootstrapping.

    ``boot``
        Dictionary of bootstrap draws.  For non-survival outcomes the keys are
        treatment names; for survival outcomes the keys are treatment pairs.

    ``matching``
        Matrix of cluster identifiers with shape ``(n, niter)``.

    When ``do_stacking`` is ``False`` predictions from each iteration are
    averaged instead of fitted via a meta-learner.

    See :pyfunc:`causalem.as_pairwise` for a helper that extracts/standardises
    the pairwise table.

    Notes
    -----
    ``"matching"`` is always present in the returned dictionary.
    * **Estimands:**
      - **No-stacking:** Each iteration produces per-arm means or pairwise HRs
        on that iteration's matched set. Results are averaged across iterations
        without appearance weighting.
      - **Stacking:** The meta-learner is fitted with appearance weights and
        final per-arm averages or marginal Cox HRs are appearance-weighted over
        the matched union.
    """
    # -------------------------------------------------------------- #
    # Determine outcome_type  ("continuous" | "binary" | "survival") #
    # -------------------------------------------------------------- #
    if outcome_type is None:
        # crude auto-detection
        if isinstance(y, np.ndarray) and y.ndim == 2 and y.shape[1] == 2:
            outcome_type = "survival"
        elif np.array_equal(np.unique(y), [0, 1]) or np.array_equal(
            np.unique(y), [0.0, 1.0]
        ):
            outcome_type = "binary"
        else:
            outcome_type = "continuous"
    else:
        allowed = {"continuous", "binary", "survival"}
        if outcome_type not in allowed:
            raise ValueError(f"`outcome_type` must be one of {allowed}.")
        if outcome_type == "binary" and not np.array_equal(np.unique(y), [0, 1]):
            raise ValueError("`outcome_type='binary'` but y is not {0,1}.")
    if niter == 1:
        do_stacking = False

    is_binary = outcome_type == "binary"

    # -------------------------------------------------------------- #
    # Validate estimand parameter                                    #
    # -------------------------------------------------------------- #
    allowed_estimands = {"ATM", "ATT", "ATE"}
    if estimand not in allowed_estimands:
        raise ValueError(
            f"estimand='{estimand}' not recognized. Must be one of {allowed_estimands}."
        )
    if estimand == "ATE":
        raise NotImplementedError(
            "estimand='ATE' is not yet implemented. Use 'ATM' or 'ATT'."
        )
    if estimand == "ATT" and ref_group is None:
        raise ValueError(
            "ref_group is required when estimand='ATT' for multi-arm treatment. "
            "Specify which treatment arm is the 'treated' group."
        )

    # Validate include_covariates_in_stacking parameter
    if include_covariates_in_stacking and not do_stacking:
        warnings.warn(
            "include_covariates_in_stacking=True ignored when do_stacking=False",
            stacklevel=2,
        )

    # ------------------------------------------------------------------ #
    # Global RNG for this call                                           #
    # ------------------------------------------------------------------ #
    rng_master = np.random.default_rng(random_state_master)

    # ------------------------------------------------------------------ #
    # Default learner, then build templates for *all* outcome types      #
    # ------------------------------------------------------------------ #
    if model_outcome is None:
        if outcome_type == "survival":
            model_outcome = RandomSurvivalForest(n_estimators=100)
        elif outcome_type == "continuous":
            model_outcome = RandomForestRegressor(n_estimators=100)
        else:
            model_outcome = RandomForestClassifier(n_estimators=100)

    outcome_templates = _setup_outcome_models(model_outcome, niter)

    # ------------------------------------------------------------------ #
    # 1. Bootstrap wrapper (recursion)                                   #
    # ------------------------------------------------------------------ #
    if nboot > 0:
        if groups is not None:
            warnings.warn("`groups` is ignored when bootstrapping.", stacklevel=2)

        # --- RNG for bootstrap resampling --------------------------------
        rng_boot = np.random.default_rng(random_state_boot)
        seeds_worker = rng_boot.integers(0, 2**32, size=nboot)

        # --- helper run on each worker -----------------------------------
        def _single_boot(seed: int):
            rng_local = np.random.default_rng(seed)
            idx = rng_local.integers(0, Xraw.shape[0], size=Xraw.shape[0])
            return estimate_te_multi(
                Xraw[idx],
                treatment[idx],
                y[idx],
                groups=np.asarray(idx),
                n_splits_propensity=n_splits_propensity,
                model_propensity=model_propensity,
                matching_scale=matching_scale,
                matching_caliper=matching_caliper,
                n_splits_outcome=n_splits_outcome,
                model_outcome=model_outcome,
                estimand=estimand,
                outcome_type=outcome_type,
                niter=niter,
                matching_is_stochastic=matching_is_stochastic,
                do_stacking=do_stacking,
                prob_clip_eps=prob_clip_eps,
                n_mc=n_mc,
                random_state_master=int(seed),
                nboot=0,  # terminate recursion
                model_meta=model_meta,
                ref_group=ref_group,
                include_covariates_in_stacking=include_covariates_in_stacking,
            )

        # --- run bootstrap in parallel -----------------------------------
        boot_list = Parallel(n_jobs=n_jobs, backend="loky")(
            delayed(_single_boot)(int(s)) for s in seeds_worker
        )

        # --- collate bootstrap draws -------------------------------------
        if outcome_type == "survival":
            pairs = [
                tuple(x)
                for x in boot_list[0]["pairwise"][
                    ["treatment_1", "treatment_2"]
                ].to_numpy()
            ]
            boot_mat = np.vstack([d["pairwise"]["te"].to_numpy() for d in boot_list])
        else:
            trt_names = boot_list[0]["per_treatment"]["treatment"].tolist()
            boot_mat = np.vstack(
                [d["per_treatment"]["mean"].to_numpy() for d in boot_list]
            )

        pct_lo = 100 * alpha / 2
        pct_hi = 100 * (1 - alpha / 2)

        # --- point estimate on original data -----------------------------
        theta_hat_res = estimate_te_multi(
            Xraw,
            treatment,
            y,
            groups=None,
            n_splits_propensity=n_splits_propensity,
            model_propensity=model_propensity,
            matching_scale=matching_scale,
            matching_caliper=matching_caliper,
            n_splits_outcome=n_splits_outcome,
            model_outcome=model_outcome,
            outcome_type=outcome_type,
            niter=niter,
            matching_is_stochastic=matching_is_stochastic,
            do_stacking=do_stacking,
            prob_clip_eps=prob_clip_eps,
            n_mc=n_mc,
            random_state_master=random_state_master,
            nboot=0,
            model_meta=model_meta,
            ref_group=ref_group,
            estimand=estimand,
            include_covariates_in_stacking=include_covariates_in_stacking,
        )
        theta_hat = theta_hat_res
        cluster_mat = theta_hat_res.get("matching")
        if outcome_type == "survival":
            est_vec = theta_hat["pairwise"]["te"].to_numpy()
            lo_vec, hi_vec = np.percentile(boot_mat, [pct_lo, pct_hi], axis=0)
            df_pairs = theta_hat["pairwise"].copy()
            df_pairs["lo"] = lo_vec
            df_pairs["hi"] = hi_vec
            boot_dict = {pair: boot_mat[:, i] for i, pair in enumerate(pairs)}
            return {
                "per_treatment": pd.DataFrame(
                    columns=["treatment", "mean", "lo", "hi"]
                ),
                "pairwise": df_pairs,
                "boot": boot_dict,
                "matching": cluster_mat,
            }
        else:
            boot_dict = {k: boot_mat[:, i] for i, k in enumerate(trt_names)}

            est_vec = theta_hat["per_treatment"]["mean"].to_numpy()
            lo_vec, hi_vec = np.percentile(boot_mat, [pct_lo, pct_hi], axis=0)

            df_means = pd.DataFrame(
                {
                    "treatment": trt_names,
                    "mean": est_vec,
                    "lo": lo_vec,
                    "hi": hi_vec,
                }
            )

            pair_rows = []
            pair_boot: list[np.ndarray] = []
            for i, a in enumerate(trt_names):
                for j in range(i + 1, len(trt_names)):
                    b = trt_names[j]
                    pair_rows.append((a, b, est_vec[i] - est_vec[j]))
                    pair_boot.append(boot_mat[:, i] - boot_mat[:, j])
            df_pairs = pd.DataFrame(
                pair_rows, columns=["treatment_1", "treatment_2", "te"]
            )
            if pair_boot:
                pair_boot_mat = np.column_stack(pair_boot)
                lo_p, hi_p = np.percentile(pair_boot_mat, [pct_lo, pct_hi], axis=0)
                df_pairs["lo"] = lo_p
                df_pairs["hi"] = hi_p

            boot_dict = {k: v for k, v in boot_dict.items()}

            return {
                "per_treatment": df_means,
                "pairwise": df_pairs,
                "boot": boot_dict,
                "matching": cluster_mat,
            }

    # ------------------------------------------------------------------ #
    # Survival pathway (placeholder)                                     #
    # ------------------------------------------------------------------ #
    if outcome_type == "survival":
        df_pairs, cluster_mat = _estimate_te_survival_multi(
            Xraw=Xraw,
            treatment=treatment,
            y=y,
            rng_master=rng_master,
            outcome_templates=outcome_templates,
            niter=niter,
            model_meta=model_meta,
            n_splits_propensity=n_splits_propensity,
            model_propensity=model_propensity,
            matching_scale=matching_scale,
            matching_caliper=matching_caliper,
            groups=groups,
            matching_is_stochastic=matching_is_stochastic,
            n_splits_outcome=n_splits_outcome,
            n_mc=n_mc,
            administrative_censoring=True,
            prob_clip_eps=prob_clip_eps,
            do_stacking=do_stacking,
            ref_group=ref_group,
            estimand=estimand,
            include_covariates_in_stacking=include_covariates_in_stacking,
        )
        return {
            "per_treatment": pd.DataFrame(columns=["treatment", "mean"]),
            "pairwise": df_pairs,
            "boot": {},
            "matching": cluster_mat,
        }

    # ------------------------------------------------------------------ #
    # 2. Iterative pipeline                                              #
    # ------------------------------------------------------------------ #
    results = []
    cluster_list = []
    avg_list = []
    for i in range(niter):
        rng_iter = np.random.default_rng(int(rng_master.integers(2**32)))
        res = stage_1_single_iter_multi(
            Xraw,
            treatment,
            y,
            rng=rng_iter,
            outcome_is_binary=is_binary,
            groups=groups,
            n_splits_propensity=n_splits_propensity,
            model_propensity=model_propensity,
            matching_scale=matching_scale,
            matching_caliper=matching_caliper,
            n_splits_outcome=n_splits_outcome,
            model_outcome=outcome_templates[i],  # <-- heterogeneous!
            matching_is_stochastic=matching_is_stochastic,
            prob_clip_eps=prob_clip_eps,
            ref_group=ref_group,
        )
        results.append(res)
        cluster_list.append(res[0])
        if not do_stacking:
            # Determine matched units based on estimand
            if estimand == "ATT":
                # MATCHED REF_GROUP ONLY: intersect matched with ref_group
                # This gives effect on ref_group population (the "treated" arm)
                # Excludes: (1) unmatched ref_group units, (2) all other arms
                ref_idx = np.where(treatment == ref_group)[0]
                all_matched_idx = np.where(res[0] != -1)[0]
                matched_idx = np.intersect1d(all_matched_idx, ref_idx)
            elif estimand == "ATM":
                matched_idx = np.where(res[0] != -1)[0]  # ALL matched
            else:
                raise ValueError(
                    f"estimand='{estimand}' not supported in no-stacking mode"
                )

            avg_list.append(
                [float(np.mean(res[2][j][matched_idx])) for j in range(len(res[2]))]
            )

    cluster_mat = np.column_stack(cluster_list)

    if not do_stacking:
        avg_arr = np.mean(np.vstack(avg_list), axis=0)
        treatment_names = results[0][3]
        df_means = pd.DataFrame({"treatment": treatment_names, "mean": avg_arr})
        pair_rows = []
        for i, a in enumerate(treatment_names):
            for j in range(i + 1, len(treatment_names)):
                b = treatment_names[j]
                pair_rows.append((a, b, avg_arr[i] - avg_arr[j]))
        df_pairs = pd.DataFrame(pair_rows, columns=["treatment_1", "treatment_2", "te"])
        return {
            "per_treatment": df_means,
            "pairwise": df_pairs,
            "boot": {},
            "matching": cluster_mat,
        }

    weights = appearance_weights(cluster_mat)
    # Get ALL matched units for training (used by meta-learner)
    matched_union = np.where(weights > 0)[0]

    y_pred_mat = np.column_stack([r[1] for r in results])
    y_pred_all_arms = []
    for i in range(len(results[0][2])):
        y_pred_all_arms.append(np.column_stack([r[2][i] for r in results]))

    # for binary outcomes, clip+logit each column of the predictions
    if is_binary:
        y_pred_mat = clip_logit(y_pred_mat, eps=prob_clip_eps)
        for i in range(len(y_pred_all_arms)):
            y_pred_all_arms[i] = clip_logit(y_pred_all_arms[i], eps=prob_clip_eps)

    # add covariates and treatment indicators to the predictions
    enc = OneHotEncoder(sparse_output=False, handle_unknown="error")
    enc.fit(treatment.reshape(-1, 1))
    treatment_levels = enc.categories_[0]
    treatment_names = treatment_levels.astype(str)
    treatment_level_lookup = dict(
        zip(treatment_names.tolist(), treatment_levels.tolist())
    )

    if include_covariates_in_stacking:
        y_pred_mat = np.hstack(
            (y_pred_mat, Xraw, enc.transform(treatment.reshape(-1, 1)))
        )
        for i in range(len(y_pred_all_arms)):
            level = treatment_level_lookup[treatment_names[i]]
            y_pred_all_arms[i] = np.hstack(
                (
                    y_pred_all_arms[i],
                    Xraw,
                    enc.transform(np.full(treatment.shape, level).reshape(-1, 1)),
                )
            )
    else:
        y_pred_mat = np.hstack((y_pred_mat, enc.transform(treatment.reshape(-1, 1))))
        for i in range(len(y_pred_all_arms)):
            level = treatment_level_lookup[treatment_names[i]]
            y_pred_all_arms[i] = np.hstack(
                (
                    y_pred_all_arms[i],
                    enc.transform(np.full(treatment.shape, level).reshape(-1, 1)),
                )
            )

    splitter_meta = _make_splitter(
        n_splits=n_splits_outcome,
        shuffle=True,
        seed=int(rng_master.integers(2**32)),
        groups=groups,
    )
    if model_meta is None:
        if is_binary:
            model_meta = LogisticRegression(solver="newton-cg")
        else:
            model_meta = LinearRegression()

    y_final_all_arms = []
    for i in range(len(y_pred_all_arms)):
        y_final_all_arms.append(np.full(Xraw.shape[0], np.nan))

    for tr_idx, te_idx in splitter_meta.split(Xraw, groups=groups):
        model_meta_clone = clone(model_meta)
        matched_tr = np.intersect1d(matched_union, tr_idx)
        if matched_tr.size == 0:
            raise ValueError(
                "No matched units in training set. "
                "Try increasing `matching_scale` or `matching_caliper`."
            )
        fit_with_appearance_weights(
            model_meta_clone,
            y_pred_mat[matched_tr],
            y[matched_tr],
            sample_weight=weights[matched_tr],
        )
        if is_binary and hasattr(model_meta_clone, "predict_proba"):
            for i in range(len(y_pred_all_arms)):
                y_final_all_arms[i][te_idx] = model_meta_clone.predict_proba(
                    y_pred_all_arms[i][te_idx]
                )[:, 1]
        else:
            for i in range(len(y_pred_all_arms)):
                y_final_all_arms[i][te_idx] = model_meta_clone.predict(
                    y_pred_all_arms[i][te_idx]
                )

    # Determine which units to average over based on estimand
    # CRITICAL: This filtering happens AFTER meta-learner training
    if estimand == "ATT":
        # MATCHED REF_GROUP ONLY for final averaging
        # Meta-learner was trained on ALL matched (all arms) for better accuracy
        # Now we average predictions over ref_group units only
        ref_idx = np.where(treatment == ref_group)[0]
        matched_union_for_avg = np.intersect1d(matched_union, ref_idx)
        weights_for_avg = weights[matched_union_for_avg]
    elif estimand == "ATM":
        # ALL matched for final averaging
        matched_union_for_avg = matched_union
        weights_for_avg = weights[matched_union_for_avg]
    else:
        raise ValueError(f"estimand='{estimand}' not yet implemented")

    means = []
    for i, treatment_name in enumerate(treatment_names):
        means.append(
            (
                treatment_name,
                float(
                    np.average(
                        y_final_all_arms[i][matched_union_for_avg],
                        weights=weights_for_avg,
                    )
                ),
            )
        )

    df_means = pd.DataFrame(means, columns=["treatment", "mean"])

    pair_rows = []
    for i, a in enumerate(treatment_names):
        for j in range(i + 1, len(treatment_names)):
            b = treatment_names[j]
            pair_rows.append((a, b, df_means.loc[i, "mean"] - df_means.loc[j, "mean"]))
    df_pairs = pd.DataFrame(pair_rows, columns=["treatment_1", "treatment_2", "te"])

    return {
        "per_treatment": df_means,
        "pairwise": df_pairs,
        "boot": {},
        "matching": cluster_mat,
    }


def stage_1_single_iter_survival_multi(
    Xraw: np.ndarray,
    treatment: np.ndarray,
    y: np.ndarray,
    *,
    rng: np.random.Generator,
    # ---- design & matching -------------------------------------------------
    n_splits_propensity: int = 5,
    model_propensity=LogisticRegression(solver="newton-cg"),
    matching_scale: float = 1.0,
    matching_caliper: Optional[float] = None,
    groups: Optional[np.ndarray] = None,
    matching_is_stochastic: bool = True,
    # ---- outcome modelling -------------------------------------------------
    n_splits_outcome: int = 5,
    model_outcome: Optional[BaseEstimator] = None,
    n_mc: int = 1,
    administrative_censoring: bool = True,
    prob_clip_eps: float = 1e-6,
    ref_group: int | str | None = None,
    **kwargs,
) -> tuple[list[tuple[tuple[Any, Any], float]], np.ndarray]:
    """Single survival iteration for multi-arm no-stacking.

    Fits a survival model on the matched units, simulates ``n_mc`` draws for
    every treatment arm and computes pairwise hazard ratios on that iteration's
    matched set.
    """

    # No feature subsetting
    X = Xraw

    # ------------ 1. Propensity CV  ----------------------------------------
    splitter_prop = _make_splitter(
        n_splits=n_splits_propensity,
        shuffle=True,
        seed=int(rng.integers(2**32)),
        groups=groups,
    )

    # --- propensity CV & logit --------------------------------------------
    oos_proba = cross_val_predict(
        clone(model_propensity),
        X,
        treatment,
        cv=splitter_prop,
        method="predict_proba",
        groups=groups,
    )
    oos_scores = clip_logit(oos_proba, eps=prob_clip_eps)

    # --- matching ----------------------------------------------------------
    cluster_ids = stochastic_match(
        treatment=treatment,
        score=oos_scores,
        scale=matching_scale,
        caliper=matching_caliper,
        nsmp=1 if matching_is_stochastic else 0,
        random_state=int(rng.integers(2**32)),
        ref_group=ref_group,
    ).ravel()
    # return cluster_ids
    matched_idx = np.where(cluster_ids != -1)[0]
    if matched_idx.size == 0:
        raise ValueError("No matches found – relax caliper/scale.")

    # --- outcome cross-fitting --------------------------------------------
    splitter_out = _make_splitter(
        n_splits=n_splits_outcome,
        shuffle=True,
        seed=int(rng.integers(2**32)),
        groups=groups,
    )

    # ---- choose survival learner -------------------------------------
    if model_outcome is None:
        model_outcome = RandomSurvivalForest(
            n_estimators=200,
            min_samples_split=10,
            min_samples_leaf=5,
            n_jobs=1,
        )

    enc = OneHotEncoder(sparse_output=False, handle_unknown="error")
    enc.fit(treatment.reshape(-1, 1))
    treatment_levels = enc.categories_[0]
    treatment_names = treatment_levels.astype(str)

    X_t_actual = np.hstack((X, enc.transform(treatment.reshape(-1, 1))))
    X_t_all_arms = []
    for treatment_value in treatment_levels:
        X_t_single_arm = np.hstack(
            (X, enc.transform(np.full(treatment.shape, treatment_value).reshape(-1, 1)))
        )
        X_t_all_arms.append(X_t_single_arm)

    # containers for predicted survival functions
    sf_all_arms = [[None] * x.shape[0] for x in X_t_all_arms]

    surv_y = np.array(
        list(zip(y[:, 1] == 1, y[:, 0])),
        dtype=[("event", "bool"), ("time", "f8")],
    )

    for tr_idx, te_idx in splitter_out.split(X_t_actual, groups=groups):
        matched_tr = np.intersect1d(matched_idx, tr_idx)
        if matched_tr.size == 0:
            continue

        mdl = clone(model_outcome)
        if "random_state" in mdl.get_params(deep=False):
            mdl.set_params(random_state=int(rng.integers(2**32)))
        mdl.fit(X_t_actual[matched_tr], surv_y[matched_tr])

        for i, X_t_single_arm in enumerate(X_t_all_arms):
            sf_tmp = mdl.predict_survival_function(
                X_t_single_arm[te_idx], return_array=False
            )
            for pos, idx in enumerate(te_idx):
                sf_all_arms[i][idx] = sf_tmp[pos]

    # ------------ 4. Monte-Carlo simulation of (time, event) tuples --------------------------------
    tau = float(y[:, 0].max()) if administrative_censoring else math.inf
    idx = matched_idx
    t_list = [np.empty((len(idx), n_mc)) for _ in X_t_all_arms]
    e_list = [np.empty((len(idx), n_mc), dtype=bool) for _ in X_t_all_arms]
    for i, sf_single_arm in enumerate(sf_all_arms):
        for pos, j in enumerate(idx):
            if sf_single_arm[j] is None:
                raise ValueError("Missing survival prediction for a matched row.")
            rng_tmp = np.random.default_rng(int(rng.integers(2**32)))
            t_list[i][pos], e_list[i][pos] = _simulate_from_sf(
                sf=sf_single_arm[j],
                tau=tau,
                n_draws=n_mc,
                rng=rng_tmp,
            )

    # loop over pairs of treatment values, fit Cox and extract HR from fitted model
    hr_list = []
    for i, treatment_value in enumerate(treatment_names):
        for j in range(i + 1, len(treatment_names)):
            if i == j:
                continue
            times = np.concatenate([t_list[i].ravel(), t_list[j].ravel()])
            events = np.concatenate([e_list[i].ravel(), e_list[j].ravel()])
            treatment_combined = np.concatenate(
                [
                    np.zeros_like(t_list[i].ravel()),
                    np.ones_like(t_list[j].ravel()),
                ]
            )
            synth = np.array(
                list(zip(events == 1, times)), dtype=[("event", "bool"), ("time", "f8")]
            )
            df = pd.DataFrame({"d": treatment_combined})
            cox = CoxPHSurvivalAnalysis().fit(df, synth)
            hr = float(np.exp(cox.coef_[0]))
            hr_list.append(((treatment_value, treatment_names[j]), hr))

    return hr_list, cluster_ids


def _estimate_te_survival_multi(
    Xraw: np.ndarray,
    treatment: np.ndarray,
    y: np.ndarray,
    *,
    rng_master: np.random.Generator,
    outcome_templates: list[BaseEstimator],
    niter: int,
    model_meta: Optional[BaseEstimator] = None,
    n_mc: int = 1,
    administrative_censoring: bool = True,
    n_splits_propensity: int = 5,
    model_propensity=LogisticRegression(solver="newton-cg"),
    matching_scale: float = 1.0,
    matching_caliper: Optional[float] = None,
    n_splits_outcome: int = 5,
    matching_is_stochastic: bool = True,
    groups: Optional[np.ndarray] = None,
    prob_clip_eps: float = 1e-6,
    do_stacking: bool = True,
    ref_group: int | str | None = None,
    estimand: str = "ATM",
    include_covariates_in_stacking: bool = False,
) -> tuple[pd.DataFrame, np.ndarray]:
    """Estimate HRs for all treatment pairs via survival meta-learning.

    When ``do_stacking`` is ``True`` a Cox meta-learner is fitted on stacked
    predictions using appearance weights, and pairwise contrasts are computed
    on the **matched union**.  When ``do_stacking`` is ``False`` each iteration
    yields HRs on that iteration's matched set and geometric means (without
    appearance weighting) are returned.  ``niter == 1`` disables stacking
    automatically.
    """

    if niter == 1:
        do_stacking = False
    if not do_stacking:
        hr_log: dict[tuple[str, str], list[float]] = {}
        cluster_list = []
        for i in range(niter):
            rng_iter = np.random.default_rng(int(rng_master.integers(2**32)))
            hr_iter, cid = stage_1_single_iter_survival_multi(
                Xraw,
                treatment,
                y,
                rng=rng_iter,
                n_splits_propensity=n_splits_propensity,
                model_propensity=model_propensity,
                matching_scale=matching_scale,
                matching_caliper=matching_caliper,
                n_splits_outcome=n_splits_outcome,
                model_outcome=outcome_templates[i],
                matching_is_stochastic=matching_is_stochastic,
                groups=groups,
                n_mc=n_mc,
                administrative_censoring=administrative_censoring,
                prob_clip_eps=prob_clip_eps,
                ref_group=ref_group,
            )
            for pair, hr in hr_iter:
                hr_log.setdefault(pair, []).append(math.log(hr))
            cluster_list.append(cid)
        hr_dict = {pair: float(np.exp(np.mean(vals))) for pair, vals in hr_log.items()}
        cluster_mat = np.column_stack(cluster_list)
        return _hr_dict_to_df(hr_dict), cluster_mat

    (
        matched_idx,
        sf_all_arms,
        treatment_names,
        cluster_mat,
    ) = stage_1_meta_survival_multi(
        Xraw,
        treatment,
        y,
        rng_master=rng_master,
        outcome_templates=outcome_templates,
        niter=niter,
        model_meta=model_meta,
        n_splits_propensity=n_splits_propensity,
        model_propensity=model_propensity,
        matching_scale=matching_scale,
        matching_caliper=matching_caliper,
        n_splits_outcome=n_splits_outcome,
        matching_is_stochastic=matching_is_stochastic,
        groups=groups,
        prob_clip_eps=prob_clip_eps,
        ref_group=ref_group,
        estimand=estimand,
        include_covariates_in_stacking=include_covariates_in_stacking,
    )

    tau = float(y[:, 0].max()) if administrative_censoring else math.inf
    # Filter matched units based on estimand
    if estimand == "ATT":
        # MATCHED REF_GROUP ONLY for survival multi-arm
        if ref_group is None:
            raise ValueError("ref_group required for ATT in multi-arm survival")
        ref_idx = np.where(treatment == ref_group)[0]
        idx = np.intersect1d(matched_idx, ref_idx)
    elif estimand == "ATM":
        idx = matched_idx  # All matched (current)
    else:
        raise ValueError(f"estimand='{estimand}' not yet implemented")
    t_list = [np.empty((len(idx), n_mc)) for _ in treatment_names]
    e_list = [np.empty((len(idx), n_mc), dtype=bool) for _ in treatment_names]

    for i, sf_arm in enumerate(sf_all_arms):
        for pos, j in enumerate(idx):
            if sf_arm[j] is None:
                raise ValueError("Missing survival prediction for a matched row.")
            rng_tmp = np.random.default_rng(int(rng_master.integers(2**32)))
            t_list[i][pos], e_list[i][pos] = _simulate_from_sf(
                sf_arm[j], n_mc, tau, rng_tmp
            )

    w_all = appearance_weights(cluster_mat)
    w_idx = w_all[idx]
    hr_dict = {}
    for i, name_i in enumerate(treatment_names):
        for j in range(i + 1, len(treatment_names)):
            times = np.concatenate([t_list[i].ravel(), t_list[j].ravel()])
            events = np.concatenate([e_list[i].ravel(), e_list[j].ravel()])
            trt_combined = np.concatenate(
                [
                    np.zeros_like(t_list[i].ravel()),
                    np.ones_like(t_list[j].ravel()),
                ]
            )
            synth = np.array(
                list(zip(events == 1, times)), dtype=[("event", "bool"), ("time", "f8")]
            )
            w_rows = np.concatenate([np.repeat(w_idx, n_mc), np.repeat(w_idx, n_mc)])
            hr_dict[(name_i, treatment_names[j])] = fit_cox_marginal_weighted(
                trt_combined, synth, w_rows
            )

    return _hr_dict_to_df(hr_dict), cluster_mat
