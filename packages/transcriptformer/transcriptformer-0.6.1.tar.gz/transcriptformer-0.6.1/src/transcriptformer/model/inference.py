import logging
import warnings

import anndata
import numpy as np
import pandas as pd
import pytorch_lightning as pl
import torch
from pytorch_lightning.loggers import CSVLogger
from torch.utils.data import DataLoader

from transcriptformer.data.dataloader import AnnDataset, AnnDatasetOOM
from transcriptformer.model.embedding_surgery import change_embedding_layer
from transcriptformer.tokenizer.vocab import load_vocabs_and_embeddings
from transcriptformer.utils.utils import stack_dict

# Set float32 matmul precision for better performance with Tensor Cores
torch.set_float32_matmul_precision("high")

torch._dynamo.config.optimize_ddp = False
torch._dynamo.config.cache_size_limit = 1000


# Set up logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")


def run_inference(cfg, data_files: list[str] | list[anndata.AnnData]):
    """Run inference using the provided config and AnnData object.

    Args:
        cfg: OmegaConf configuration object
        data_files: list of data files to load
    Returns:
        AnnData: Processed data with embeddings and likelihood scores
    """
    warnings.filterwarnings(
        "ignore", message="The 'predict_dataloader' does not have many workers which may be a bottleneck"
    )
    warnings.filterwarnings(
        "ignore",
        message="Your `IterableDataset` has `__len__` defined. In combination with multi-process data loading",
        category=UserWarning,
    )
    warnings.filterwarnings("ignore", message="Transforming to str index.", category=UserWarning)

    # Load vocabs and embeddings
    (gene_vocab, aux_vocab), emb_matrix = load_vocabs_and_embeddings(cfg)

    # Determine model class based on config
    model_type = cfg.model.get("model_type", "transcriptformer")  # Default to transcriptformer if not set
    if model_type == "esm2ce":
        from transcriptformer.model.model import ESM2CE as ModelClass

        logging.info("Instantiating ESM2CE model")
    elif model_type == "transcriptformer":
        from transcriptformer.model.model import Transcriptformer as ModelClass

        logging.info("Instantiating Transcriptformer model")
    else:
        raise ValueError(f"Unknown model_type: {model_type}")

    # Instantiate the selected model
    logging.info(f"Instantiating the {model_type} model")
    model = ModelClass(
        data_config=cfg.model.data_config,
        model_config=cfg.model.model_config,
        loss_config=cfg.model.loss_config,
        inference_config=cfg.model.inference_config,
        gene_vocab_dict=gene_vocab,
        aux_vocab_dict=aux_vocab,
        emb_matrix=emb_matrix,
    )
    model.eval()

    logging.info("Model instantiated successfully")

    # Check if checkpoint is supplied
    if not hasattr(cfg.model.inference_config, "load_checkpoint") or not cfg.model.inference_config.load_checkpoint:
        raise ValueError(
            "No checkpoint provided for inference. Please specify a checkpoint path in "
            "model.inference_config.load_checkpoint"
        )

    logging.info("Loading model checkpoint")
    # Determine target device for loading checkpoint
    device_preference = getattr(cfg.model.inference_config, "device", "auto")
    if device_preference == "cpu":
        map_location = "cpu"
    elif device_preference == "cuda":
        map_location = "cuda" if torch.cuda.is_available() else "cpu"
    elif device_preference == "mps":
        map_location = "mps" if torch.backends.mps.is_available() else "cpu"
    else:  # auto
        if torch.cuda.is_available():
            map_location = "cuda"
        elif torch.backends.mps.is_available():
            map_location = "mps"
        else:
            map_location = "cpu"

    # Instead of loading full checkpoint, just load weights with proper device mapping
    state_dict = torch.load(cfg.model.inference_config.load_checkpoint, weights_only=True, map_location=map_location)

    # Validate and load weights
    # converter.validate_loaded_weights(model, state_dict)
    model.load_state_dict(state_dict)
    logging.info("Model weights loaded successfully")

    # Perform embedding surgery if specified in config
    if cfg.model.inference_config.pretrained_embedding is not None:
        logging.info("Performing embedding surgery")
        # Check if pretrained_embedding_paths is a list, if not convert it to a list
        if not isinstance(cfg.model.inference_config.pretrained_embedding, list):
            pretrained_embedding_paths = [cfg.model.inference_config.pretrained_embedding]
        else:
            pretrained_embedding_paths = cfg.model.inference_config.pretrained_embedding
        model, gene_vocab = change_embedding_layer(model, pretrained_embedding_paths)

    # Load dataset
    data_kwargs = {
        "gene_vocab": gene_vocab,
        "aux_vocab": aux_vocab,
        "max_len": cfg.model.model_config.seq_len,
        "pad_zeros": cfg.model.data_config.pad_zeros,
        "pad_token": cfg.model.data_config.gene_pad_token,
        "sort_genes": cfg.model.data_config.sort_genes,
        "filter_to_vocab": cfg.model.data_config.filter_to_vocabs,
        "filter_outliers": cfg.model.data_config.filter_outliers,
        "gene_col_name": cfg.model.data_config.gene_col_name,
        "normalize_to_scale": cfg.model.data_config.normalize_to_scale,
        "randomize_order": cfg.model.data_config.randomize_genes,
        "min_expressed_genes": cfg.model.data_config.min_expressed_genes,
        "clip_counts": cfg.model.data_config.clip_counts,
        "obs_keys": cfg.model.inference_config.obs_keys,
        "remove_duplicate_genes": cfg.model.data_config.remove_duplicate_genes,
        "use_raw": cfg.model.data_config.use_raw,
    }
    if getattr(cfg.model.inference_config, "use_oom_dataloader", False):
        # Use OOM-safe map-style dataset
        dataset = AnnDatasetOOM(
            data_files,
            gene_vocab,
            aux_vocab=aux_vocab,
            max_len=cfg.model.model_config.seq_len,
            normalize_to_scale=cfg.model.data_config.normalize_to_scale,
            sort_genes=cfg.model.data_config.sort_genes,
            randomize_order=cfg.model.data_config.randomize_genes,
            pad_zeros=cfg.model.data_config.pad_zeros,
            gene_col_name=cfg.model.data_config.gene_col_name,
            filter_to_vocab=cfg.model.data_config.filter_to_vocabs,
            clip_counts=cfg.model.data_config.clip_counts,
            obs_keys=cfg.model.inference_config.obs_keys,
            use_raw=cfg.model.data_config.use_raw,
            remove_duplicate_genes=cfg.model.data_config.remove_duplicate_genes,
        )
    else:
        dataset = AnnDataset(data_files, **data_kwargs)

    # Create dataloader
    dataloader = DataLoader(
        dataset,
        batch_size=cfg.model.inference_config.batch_size,
        num_workers=cfg.model.data_config.n_data_workers,
        drop_last=False,
        pin_memory=True,
        collate_fn=dataset.collate_fn,
    )

    # Determine device and accelerator based on user preference
    device_preference = getattr(cfg.model.inference_config, "device", "auto")
    num_gpus = getattr(cfg.model.inference_config, "num_gpus", 1)

    # Handle device preference
    if device_preference == "cpu":
        # Force CPU usage
        accelerator = "cpu"
        devices = 1
        logging.info("Forcing CPU usage based on device preference")
    elif device_preference == "cuda":
        # Force CUDA usage
        if not torch.cuda.is_available():
            raise RuntimeError("CUDA requested but not available")
        accelerator = "gpu"
        if num_gpus == -1:
            devices = torch.cuda.device_count()
        elif num_gpus > 1:
            available_gpus = torch.cuda.device_count()
            if available_gpus < num_gpus:
                logging.warning(
                    f"Requested {num_gpus} CUDA devices but only {available_gpus} available. Using {available_gpus}."
                )
                devices = available_gpus
            else:
                devices = num_gpus
        else:
            devices = 1
        logging.info(f"Forcing CUDA usage with {devices} device(s)")
    elif device_preference == "mps":
        # Force MPS usage (Apple Silicon)
        if not torch.backends.mps.is_available():
            raise RuntimeError("MPS requested but not available")
        accelerator = "mps"
        devices = 1  # MPS typically supports single device
        logging.info("Forcing MPS usage for Apple Silicon")
    else:  # device_preference == "auto"
        # Auto-detect best available device (existing logic)
        if num_gpus == -1:
            # Use all available GPUs
            devices = torch.cuda.device_count() if torch.cuda.is_available() else 1
            accelerator = (
                "gpu" if torch.cuda.is_available() else ("mps" if torch.backends.mps.is_available() else "cpu")
            )
        elif num_gpus > 1:
            # Use specified number of GPUs
            available_gpus = torch.cuda.device_count() if torch.cuda.is_available() else 0
            if available_gpus < num_gpus:
                logging.warning(
                    f"Requested {num_gpus} GPUs but only {available_gpus} available. Using {available_gpus} GPUs."
                )
                devices = available_gpus if available_gpus > 0 else 1
                accelerator = "gpu" if available_gpus > 0 else ("mps" if torch.backends.mps.is_available() else "cpu")
            else:
                devices = num_gpus
                accelerator = "gpu"
        else:
            # Use single GPU or CPU
            devices = 1
            if torch.cuda.is_available():
                accelerator = "gpu"
            elif torch.backends.mps.is_available():
                accelerator = "mps"
            else:
                accelerator = "cpu"

    logging.info(f"Using {devices} device(s) with accelerator: {accelerator}")

    # Create Trainer
    trainer = pl.Trainer(
        accelerator=accelerator,
        devices=devices,
        num_nodes=1,
        precision=cfg.model.inference_config.precision,
        limit_predict_batches=None,
        logger=CSVLogger("logs", name="inference"),
        strategy="ddp" if devices > 1 else "auto",  # Use DDP for multi-GPU
    )

    # Run prediction
    output = trainer.predict(model, dataloaders=dataloader)

    # Combine predictions
    logging.info("Combining predictions")
    concat_output = stack_dict(output)

    # Create pandas DataFrames from the obs and uns data in concat_output
    obs_df = pd.DataFrame(concat_output["obs"])
    uns = {"llh": pd.DataFrame({"llh": concat_output["llh"]})} if "llh" in concat_output else None
    obsm = {}

    # Add all other output keys to the obsm
    for k in cfg.model.inference_config.output_keys:
        if k in concat_output:
            if k == "embeddings" and cfg.model.inference_config.emb_type == "cge":
                # Handle contextual gene embeddings - store in a flattened format for HDF5 compatibility
                cge_list = concat_output[k]

                # Flatten all embeddings and track their metadata
                all_embeddings = []
                cell_indices = []
                gene_names = []

                for cell_idx, cell_dict in enumerate(cge_list):
                    for gene_name, embedding in cell_dict.items():
                        all_embeddings.append(embedding)
                        cell_indices.append(cell_idx)
                        gene_names.append(gene_name)

                # Convert to numpy arrays
                if all_embeddings:
                    embeddings_array = np.stack(all_embeddings)
                    cell_indices_array = np.array(cell_indices)
                    gene_names_array = np.array(gene_names)

                    # Store in uns (unstructured data)
                    if uns is None:
                        uns = {}
                    uns["cge_embeddings"] = embeddings_array
                    uns["cge_cell_indices"] = cell_indices_array
                    uns["cge_gene_names"] = gene_names_array
                else:
                    # Handle empty case
                    if uns is None:
                        uns = {}
                    uns["cge_embeddings"] = np.array([])
                    uns["cge_cell_indices"] = np.array([])
                    uns["cge_gene_names"] = np.array([])
            else:
                # Regular embeddings or other outputs
                obsm[k] = concat_output[k].numpy() if hasattr(concat_output[k], "numpy") else concat_output[k]

    # Create a new AnnData object with the embeddings
    output_adata = anndata.AnnData(
        obs=obs_df,
        obsm=obsm,
        uns=uns,
    )

    return output_adata
