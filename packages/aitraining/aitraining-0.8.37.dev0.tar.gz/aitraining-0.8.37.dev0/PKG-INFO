Metadata-Version: 2.4
Name: aitraining
Version: 0.8.37.dev0
Summary: Advanced Machine Learning Training Platform
Home-page: https://github.com/huggingface/autotrain-advanced
Download-URL: https://github.com/huggingface/autotrain-advanced/tags
Author: HuggingFace Inc.
Author-email: Andrew Correa <andrew@monostate.ai>
License: Apache 2.0
Project-URL: Homepage, https://github.com/huggingface/autotrain-advanced
Project-URL: Documentation, https://github.com/huggingface/autotrain-advanced
Project-URL: Repository, https://github.com/huggingface/autotrain-advanced
Project-URL: Bug Tracker, https://github.com/huggingface/autotrain-advanced/issues
Keywords: automl,autonlp,autotrain,huggingface,machine learning,deep learning,fine-tuning,llm
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Education
Classifier: Intended Audience :: Science/Research
Classifier: License :: OSI Approved :: Apache Software License
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Requires-Python: >=3.8
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: torch>=1.10.0
Requires-Dist: transformers==4.48.0
Requires-Dist: accelerate==1.2.1
Requires-Dist: peft==0.14.0
Requires-Dist: trl==0.13.0
Requires-Dist: datasets[vision]~=3.2.0
Requires-Dist: tensorboard==2.18.0
Requires-Dist: scikit-learn==1.6.0
Requires-Dist: evaluate==0.4.3
Requires-Dist: sentence-transformers==3.3.1
Requires-Dist: albumentations==1.4.23
Requires-Dist: Pillow==11.0.0
Requires-Dist: pandas==2.2.3
Requires-Dist: numpy
Requires-Dist: sentencepiece>=0.1.99
Requires-Dist: tiktoken==0.8.0
Requires-Dist: nltk==3.9.1
Requires-Dist: sacremoses==0.1.1
Requires-Dist: seqeval==1.2.2
Requires-Dist: ipadic==1.0.0
Requires-Dist: timm==1.0.12
Requires-Dist: torchmetrics==1.6.0
Requires-Dist: faster-coco-eval==1.7.0
Requires-Dist: pycocotools==2.0.8
Requires-Dist: optuna==4.1.0
Requires-Dist: xgboost==2.1.3
Requires-Dist: huggingface_hub==0.27.0
Requires-Dist: hf-transfer
Requires-Dist: fastapi==0.115.6
Requires-Dist: starlette
Requires-Dist: uvicorn==0.34.0
Requires-Dist: python-multipart==0.0.20
Requires-Dist: werkzeug==3.1.3
Requires-Dist: pyngrok==7.2.1
Requires-Dist: authlib==1.4.0
Requires-Dist: itsdangerous==2.2.0
Requires-Dist: httpx==0.28.1
Requires-Dist: loguru==0.7.3
Requires-Dist: requests==2.32.3
Requires-Dist: pydantic==2.10.4
Requires-Dist: pyyaml==6.0.2
Requires-Dist: tqdm==4.67.1
Requires-Dist: joblib==1.4.2
Requires-Dist: einops==0.8.0
Requires-Dist: packaging==24.2
Requires-Dist: cryptography==44.0.0
Requires-Dist: nvitop==1.3.2
Requires-Dist: py7zr==0.22.0
Requires-Dist: safetensors
Requires-Dist: psutil
Requires-Dist: diffusers
Requires-Dist: ipywidgets
Requires-Dist: textual==0.86.1
Requires-Dist: rich==13.9.4
Requires-Dist: rouge_score==0.1.2
Requires-Dist: jiwer==3.0.5
Requires-Dist: bitsandbytes==0.42.0; sys_platform == "linux"
Provides-Extra: dev
Requires-Dist: black; extra == "dev"
Requires-Dist: isort; extra == "dev"
Requires-Dist: flake8; extra == "dev"
Requires-Dist: pytest; extra == "dev"
Provides-Extra: quality
Requires-Dist: black; extra == "quality"
Requires-Dist: isort; extra == "quality"
Requires-Dist: flake8; extra == "quality"
Provides-Extra: docs
Requires-Dist: recommonmark; extra == "docs"
Requires-Dist: sphinx; extra == "docs"
Requires-Dist: sphinx-markdown-tables; extra == "docs"
Requires-Dist: sphinx-rtd-theme; extra == "docs"
Requires-Dist: sphinx-copybutton; extra == "docs"
Dynamic: author
Dynamic: download-url
Dynamic: home-page
Dynamic: license-file

# ðŸ¤— AITraining (formerly AutoTrain Advanced)

AITraining: faster and easier training and deployments of state-of-the-art machine learning models. AITraining is a no-code solution that allows you to train machine learning models in just a few clicks. Please note that you must upload data in correct format for project to be created. For help regarding proper data format and pricing, check out the documentation. 

NOTE: AutoTrain is free! You only pay for the resources you use in case you decide to run AutoTrain on Hugging Face Spaces. When running locally, you only pay for the resources you use on your own infrastructure.

## Supported Tasks

| Task | Status | Python Notebook | Example Configs |
| --- | --- | --- | --- |
| LLM SFT Finetuning | âœ… | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/autotrain-advanced/blob/main/notebooks/llm_finetuning.ipynb) | [llm_sft_finetune.yaml](https://github.com/huggingface/autotrain-advanced/blob/main/configs/llm_finetuning/smollm2.yml) |
| LLM ORPO Finetuning | âœ… | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/autotrain-advanced/blob/main/notebooks/llm_finetuning.ipynb) | [llm_orpo_finetune.yaml](https://github.com/huggingface/autotrain-advanced/blob/main/configs/llm_finetuning/llama3-8b-orpo.yml) |
| LLM DPO Finetuning | âœ… | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/autotrain-advanced/blob/main/notebooks/llm_finetuning.ipynb) | [llm_dpo_finetune.yaml](https://github.com/huggingface/autotrain-advanced/blob/main/configs/llm_finetuning/llama3-8b-dpo-qlora.yml) |
| LLM Reward Finetuning | âœ… | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/autotrain-advanced/blob/main/notebooks/llm_finetuning.ipynb) | [llm_reward_finetune.yaml](https://github.com/huggingface/autotrain-advanced/blob/main/configs/llm_finetuning/llama32-1b-sft.yml) |
| LLM Generic/Default Finetuning | âœ… | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/autotrain-advanced/blob/main/notebooks/llm_finetuning.ipynb) | [llm_generic_finetune.yaml](https://github.com/huggingface/autotrain-advanced/blob/main/configs/llm_finetuning/gpt2_sft.yml) |
| Text Classification | âœ… | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/autotrain-advanced/blob/main/notebooks/text_classification.ipynb) | [text_classification.yaml](https://github.com/huggingface/autotrain-advanced/tree/main/configs/text_classification) |
| Text Regression | âœ… | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/autotrain-advanced/blob/main/notebooks/text_regression.ipynb) | [text_regression.yaml](https://github.com/huggingface/autotrain-advanced/tree/main/configs/text_regression) |
| Token Classification | âœ… | Coming Soon | [token_classification.yaml](https://github.com/huggingface/autotrain-advanced/tree/main/configs/token_classification) |
| Seq2Seq | âœ… | Coming Soon | [seq2seq.yaml](https://github.com/huggingface/autotrain-advanced/tree/main/configs/seq2seq) |
| Extractive Question Answering | âœ… | Coming Soon | [extractive_qa.yaml](https://github.com/huggingface/autotrain-advanced/tree/main/configs/extractive_question_answering) |
| Image Classification | âœ… | Coming Soon | [image_classification.yaml](https://github.com/huggingface/autotrain-advanced/tree/main/configs/image_classification) |
| Image Scoring/Regression | âœ… | Coming Soon | [image_regression.yaml](https://github.com/huggingface/autotrain-advanced/tree/main/configs/image_scoring) |
| VLM | ðŸŸ¥ | Coming Soon | [vlm.yaml](https://github.com/huggingface/autotrain-advanced/tree/main/configs/vlm) |


## Running UI on Colab or Hugging Face Spaces

- Deploy AutoTrain on Hugging Face Spaces: [![Deploy on Spaces](https://huggingface.co/datasets/huggingface/badges/resolve/main/deploy-on-spaces-md.svg)](https://huggingface.co/login?next=%2Fspaces%2Fautotrain-projects%2Fautotrain-advanced%3Fduplicate%3Dtrue)


- Run AutoTrain UI on Colab via ngrok: [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/autotrain-advanced/blob/main/colabs/AutoTrain_ngrok.ipynb)


## Local Installation

You can Install AITraining python package via PIP. Please note you will need python >= 3.10 for AITraining to work properly.

    pip install aitraining
    
Please make sure that you have git lfs installed. Check out the instructions here: https://github.com/git-lfs/git-lfs/wiki/Installation

You also need to install torch, torchaudio and torchvision.

The best way to run autotrain is in a conda environment. You can create a new conda environment with the following command:

    conda create -n autotrain python=3.10
    conda activate autotrain
    pip install aitraining
    conda install pytorch torchvision torchaudio pytorch-cuda=12.1 -c pytorch -c nvidia
    conda install -c "nvidia/label/cuda-12.1.0" cuda-nvcc

Once done, you can start the application using:

    autotrain app --port 8080 --host 127.0.0.1


If you are not fond of UI, you can use AutoTrain Configs to train using command line or simply AutoTrain CLI.

To use config file for training, you can use the following command:

    autotrain --config <path_to_config_file>


You can find sample config files in the `configs` directory of this repository.

Example config file for finetuning SmolLM2:

```yaml
task: llm-sft
base_model: HuggingFaceTB/SmolLM2-1.7B-Instruct
project_name: autotrain-smollm2-finetune
log: tensorboard
backend: local

data:
  path: HuggingFaceH4/no_robots
  train_split: train
  valid_split: null
  chat_template: tokenizer
  column_mapping:
    text_column: messages

params:
  block_size: 2048
  model_max_length: 4096
  epochs: 2
  batch_size: 1
  lr: 1e-5
  peft: true
  quantization: int4
  target_modules: all-linear
  padding: right
  optimizer: paged_adamw_8bit
  scheduler: linear
  gradient_accumulation: 8
  mixed_precision: bf16
  merge_adapter: true

hub:
  username: ${HF_USERNAME}
  token: ${HF_TOKEN}
  push_to_hub: true
```

To fine-tune a model using the config file above, you can use the following command:

```bash
$ export HF_USERNAME=<your_hugging_face_username>
$ export HF_TOKEN=<your_hugging_face_write_token>
$ autotrain --config <path_to_config_file>
```


## Documentation

Documentation is available at https://hf.co/docs/autotrain/

## CLI Quickstart (LLM)

Use the LLM CLI to train or run inference from the command line:

- Train (SFT by default):
  `python -m autotrain.cli.autotrain llm --train --model gpt2 --data-path <dataset_or_path> --project-name my-run`
- Choose a trainer: `--trainer {default|sft|reward|dpo|orpo|distillation|ppo}`
- Inference mode: `python -m autotrain.cli.autotrain llm --inference --model gpt2 --inference-prompts prompts.txt --project-name my-run`

Trainer options map to the following implementations:
- `default` and `sft`: TRL SFT-based finetuning
- `reward`: TRL Reward modeling
- `dpo`: TRL DPO trainer
- `orpo`: ORPO trainer
- `distillation`: Prompt distillation mode (teacher/student with KL + CE)
- `ppo`: PPO RL trainer (experimental)

Common flags (subset):
- `--block_size`, `--epochs`, `--batch_size`, `--lr`, `--warmup_ratio`, `--mixed_precision {fp16|bf16}`
- `--peft`, `--quantization {int4|int8}`, `--target_modules`, `--merge_adapter`
- `--chat_template {chatml|zephyr|tokenizer}`
- `--use-enhanced-eval`, `--eval-metrics perplexity,bleu`, `--eval-save-predictions`
- Distillation-specific: `--teacher-model`, `--teacher-prompt-template`, `--distill-temperature`, `--distill-alpha`
- PPO-specific: `--rl-gamma`, `--rl-gae-lambda`, `--rl-kl-coef`, `--rl-clip-range`, `--rl-env-type`

The CLI is covered by tests under `tests/test_cli_integration.py` and `tests/test_cli_end_to_end.py`.

## API/Endpoints

AutoTrain can be run as a local app with `autotrain app` and exposes training flows through the CLI module `autotrain.cli.autotrain`. For service integration, see the repository root `server.py`/`service.py` examples and deployment artifacts (e.g., `bentofile.yaml`) in the monorepo. These show how to wrap model inference behind HTTP endpoints.

## Citation

```
@inproceedings{thakur-2024-autotrain,
    title = "{A}uto{T}rain: No-code training for state-of-the-art models",
    author = "Thakur, Abhishek",
    booktitle = "Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.emnlp-demo.44",
    pages = "419--423",
    abstract = "With the advancements in open-source models, training(or finetuning) models on custom datasets has become a crucial part of developing solutions which are tailored to specific industrial or open-source applications. Yet, there is no single tool which simplifies the process of training across different types of modalities or tasks.We introduce AutoTrain(aka AutoTrain Advanced){---}an open-source, no code tool/library which can be used to train (or finetune) models for different kinds of tasks such as: large language model (LLM) finetuning, text classification/regression, token classification, sequence-to-sequence task, finetuning of sentence transformers, visual language model (VLM) finetuning, image classification/regression and even classification and regression tasks on tabular data. AutoTrain Advanced is an open-source library providing best practices for training models on custom datasets. The library is available at https://github.com/huggingface/autotrain-advanced. AutoTrain can be used in fully local mode or on cloud machines and works with tens of thousands of models shared on Hugging Face Hub and their variations.",
}
```
