"""
Notebook ä»£ç ç”Ÿæˆå™¨

åœ¨ AutoML è®­ç»ƒå®Œæˆåï¼Œç”Ÿæˆä¸€ä¸ªå¯æ‰§è¡Œçš„ Notebook ä»£ç ï¼Œ
ä½¿ç”¨æœ€ä½³ä¼°è®¡å™¨å’Œæœ€ä½³è¶…å‚æ•°é‡æ–°è®­ç»ƒæ¨¡å‹ã€‚
"""

import json
from typing import Dict, Any, List


def generate_notebook_code(
    cfg: Dict[str, Any],
    best_estimator: str,
    best_config: Dict[str, Any],
    features: List[str],
    label: str,
    run_id: str,
    experiment_name: str,
) -> str:
    """
    ç”Ÿæˆ Notebook ä»£ç 
    
    Args:
        cfg: AutoML é…ç½®
        best_estimator: æœ€ä½³ä¼°è®¡å™¨åç§°
        best_config: æœ€ä½³è¶…å‚æ•°é…ç½®
        features: ç‰¹å¾åˆ—è¡¨
        label: æ ‡ç­¾åˆ—å
        run_id: MLflow run ID
        experiment_name: MLflow å®éªŒåç§°
    
    Returns:
        str: Notebook ä»£ç ï¼ˆPython æ ¼å¼ï¼‰
    """
    
    # æå–é…ç½®ä¿¡æ¯
    table_name = cfg.get("table", "your_table")
    task = cfg.get("task", "classification")
    metric = cfg.get("metric", "accuracy")
    split_config = cfg.get("split", {})
    train_ratio = split_config.get("train_ratio", 0.6)
    val_ratio = split_config.get("val_ratio", 0.2)
    test_ratio = split_config.get("test_ratio", 0.2)
    stratify = split_config.get("stratify", True)
    seed = cfg.get("seed", 42)
    
    # æ³¨å†Œé…ç½®
    register_config = cfg.get("register", {})
    model_name = register_config.get("model_name", "automl_model")
    
    # ç‰¹å¾å·¥ç¨‹é…ç½®
    feature_store_config = cfg.get("feature_store", {})
    use_feature_store = feature_store_config.get("use_training_set", False)
    feature_table_name = feature_store_config.get("table_name", "")
    primary_keys = feature_store_config.get("primary_keys", [])
    
    # ä¼°è®¡å™¨æ˜ å°„
    estimator_map = {
        "lgbm": "LGBMClassifier" if task == "classification" else "LGBMRegressor",
        "xgboost": "XGBClassifier" if task == "classification" else "XGBRegressor",
        "rf": "RandomForestClassifier" if task == "classification" else "RandomForestRegressor",
        "extra_tree": "ExtraTreesClassifier" if task == "classification" else "ExtraTreesRegressor",
        "lrl1": "LogisticRegression" if task == "classification" else "Ridge",
        "lrl2": "LogisticRegression" if task == "classification" else "Ridge",
    }
    
    estimator_class = estimator_map.get(best_estimator, "RandomForestClassifier")
    
    # å¯¼å…¥è¯­å¥æ˜ å°„
    import_map = {
        "LGBMClassifier": "from lightgbm import LGBMClassifier",
        "LGBMRegressor": "from lightgbm import LGBMRegressor",
        "XGBClassifier": "from xgboost import XGBClassifier",
        "XGBRegressor": "from xgboost import XGBRegressor",
        "RandomForestClassifier": "from sklearn.ensemble import RandomForestClassifier",
        "RandomForestRegressor": "from sklearn.ensemble import RandomForestRegressor",
        "ExtraTreesClassifier": "from sklearn.ensemble import ExtraTreesClassifier",
        "ExtraTreesRegressor": "from sklearn.ensemble import ExtraTreesRegressor",
        "LogisticRegression": "from sklearn.linear_model import LogisticRegression",
        "Ridge": "from sklearn.linear_model import Ridge",
    }
    
    estimator_import = import_map.get(estimator_class, "from sklearn.ensemble import RandomForestClassifier")
    
    # æ ¼å¼åŒ–è¶…å‚æ•°
    params_str = json.dumps(best_config, indent=4)
    
    # æ ¼å¼åŒ–ç‰¹å¾åˆ—è¡¨
    features_str = json.dumps(features, indent=4)
    
    # ç”Ÿæˆä»£ç 
    code = f'''# %% [markdown]
# # AutoML æœ€ä½³æ¨¡å‹è®­ç»ƒ Notebook
# 
# æœ¬ Notebook ç”± WeData AutoML è‡ªåŠ¨ç”Ÿæˆï¼Œä½¿ç”¨æœ€ä½³ä¼°è®¡å™¨å’Œæœ€ä½³è¶…å‚æ•°é‡æ–°è®­ç»ƒæ¨¡å‹ã€‚
# 
# **AutoML è®­ç»ƒä¿¡æ¯**:
# - å®éªŒåç§°: {experiment_name}
# - Run ID: {run_id}
# - æœ€ä½³ä¼°è®¡å™¨: {best_estimator}
# - ä»»åŠ¡ç±»å‹: {task}
# - è¯„ä¼°æŒ‡æ ‡: {metric}

# %% [markdown]
# ## 1. å®‰è£…ä¾èµ–åŒ…

# %%
# å®‰è£… WeData ç‰¹å¾å·¥ç¨‹åŒ…ï¼ˆå¦‚æœä½¿ç”¨ç‰¹å¾åº“ï¼‰
%pip install tencent-wedata-feature-engineering==0.1.33 -i https://mirrors.tencent.com/pypi/simple --trusted-host mirrors.tencent.com

# %% [markdown]
# ## 2. å¯¼å…¥å¿…è¦çš„åº“

# %%
import os
import pandas as pd
import numpy as np
import mlflow
import mlflow.sklearn
from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    accuracy_score, f1_score, precision_score, recall_score, 
    roc_auc_score, confusion_matrix, classification_report
)
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
{estimator_import}

# å¯è§†åŒ–åº“
import matplotlib.pyplot as plt
import seaborn as sns

# è®¾ç½®éšæœºç§å­
np.random.seed({seed})

print("âœ“ åº“å¯¼å…¥å®Œæˆ")

# %% [markdown]
# ## 3. é…ç½®ä¿¡æ¯

# %%
# æ•°æ®é…ç½®
TABLE_NAME = "{table_name}"
LABEL_COL = "{label}"
FEATURE_COLS = {features_str}

# è®­ç»ƒé…ç½®
TASK = "{task}"
METRIC = "{metric}"
RANDOM_SEED = {seed}

# æ•°æ®åˆ’åˆ†é…ç½®
TRAIN_RATIO = {train_ratio}
VAL_RATIO = {val_ratio}
TEST_RATIO = {test_ratio}
STRATIFY = {stratify}

# æ¨¡å‹é…ç½®
BEST_ESTIMATOR = "{best_estimator}"
BEST_PARAMS = {params_str}

# æ³¨å†Œé…ç½®
MODEL_NAME = "{model_name}"
EXPERIMENT_NAME = "{experiment_name}_retrain"

# ç‰¹å¾åº“é…ç½®
USE_FEATURE_STORE = {use_feature_store}
FEATURE_TABLE_NAME = "{feature_table_name}"
PRIMARY_KEYS = {json.dumps(primary_keys)}

print("âœ“ é…ç½®åŠ è½½å®Œæˆ")
print(f"  - è¡¨å: {{TABLE_NAME}}")
print(f"  - æ ‡ç­¾åˆ—: {{LABEL_COL}}")
print(f"  - ç‰¹å¾æ•°: {{len(FEATURE_COLS)}}")
print(f"  - æœ€ä½³ä¼°è®¡å™¨: {{BEST_ESTIMATOR}}")

# %% [markdown]
# ## 4. è¯»å–æ•°æ®

# %%
if USE_FEATURE_STORE:
    # ä½¿ç”¨ WeData ç‰¹å¾åº“
    from wedata.feature_store.client import FeatureStoreClient
    from wedata.feature_store.entities.feature_lookup import FeatureLookup
    
    # æ„å»ºç‰¹å¾å·¥ç¨‹å®¢æˆ·ç«¯
    client = FeatureStoreClient(spark)
    
    # è¯»å–åŸºç¡€æ•°æ®ï¼ˆåŒ…å«æ ‡ç­¾ï¼‰
    base_df = spark.read.table(TABLE_NAME)
    
    # å®šä¹‰ç‰¹å¾æŸ¥æ‰¾
    feature_lookup = FeatureLookup(
        table_name=FEATURE_TABLE_NAME,
        lookup_key=PRIMARY_KEYS[0] if PRIMARY_KEYS else "id"
    )
    
    # åˆ›å»ºè®­ç»ƒé›†
    training_set = client.create_training_set(
        df=base_df,
        feature_lookups=[feature_lookup],
        label=LABEL_COL,
        exclude_columns=PRIMARY_KEYS
    )
    
    # åŠ è½½è®­ç»ƒæ•°æ®
    df = training_set.load_df().toPandas()
    
    print(f"âœ“ ä»ç‰¹å¾åº“åŠ è½½æ•°æ®: {{len(df)}} è¡Œ")
else:
    # ç›´æ¥ä»è¡¨è¯»å–
    df = spark.read.table(TABLE_NAME).toPandas()
    print(f"âœ“ ä»è¡¨åŠ è½½æ•°æ®: {{len(df)}} è¡Œ")

# æ˜¾ç¤ºæ•°æ®ä¿¡æ¯
print(f"\\næ•°æ®å½¢çŠ¶: {{df.shape}}")
print(f"æ ‡ç­¾åˆ†å¸ƒ:\\n{{df[LABEL_COL].value_counts()}}")

# æ˜¾ç¤ºå‰å‡ è¡Œ
display(df.head())

# %% [markdown]
# ## 5. æ•°æ®åˆ’åˆ†

# %%
# å‡†å¤‡ç‰¹å¾å’Œæ ‡ç­¾
X = df[FEATURE_COLS]
y = df[LABEL_COL]

# ç¬¬ä¸€æ¬¡åˆ’åˆ†: train+val vs test
if STRATIFY and TASK == "classification":
    X_temp, X_test, y_temp, y_test = train_test_split(
        X, y, test_size=TEST_RATIO, random_state=RANDOM_SEED, stratify=y
    )
else:
    X_temp, X_test, y_temp, y_test = train_test_split(
        X, y, test_size=TEST_RATIO, random_state=RANDOM_SEED
    )

# ç¬¬äºŒæ¬¡åˆ’åˆ†: train vs val
val_ratio_adjusted = VAL_RATIO / (TRAIN_RATIO + VAL_RATIO)
if STRATIFY and TASK == "classification":
    X_train, X_val, y_train, y_val = train_test_split(
        X_temp, y_temp, test_size=val_ratio_adjusted, random_state=RANDOM_SEED, stratify=y_temp
    )
else:
    X_train, X_val, y_train, y_val = train_test_split(
        X_temp, y_temp, test_size=val_ratio_adjusted, random_state=RANDOM_SEED
    )

print("âœ“ æ•°æ®åˆ’åˆ†å®Œæˆ")
print(f"  - è®­ç»ƒé›†: {{len(X_train)}} è¡Œ ({{len(X_train)/len(X)*100:.1f}}%)")
print(f"  - éªŒè¯é›†: {{len(X_val)}} è¡Œ ({{len(X_val)/len(X)*100:.1f}}%)")
print(f"  - æµ‹è¯•é›†: {{len(X_test)}} è¡Œ ({{len(X_test)/len(X)*100:.1f}}%)")

# %% [markdown]
# ## 6. åˆ›å»ºå’Œè®­ç»ƒæ¨¡å‹

# %%
# è®¾ç½® MLflow å®éªŒ
mlflow.set_experiment(EXPERIMENT_NAME)

# å¼€å§‹ MLflow run
with mlflow.start_run(run_name=f"{{BEST_ESTIMATOR}}_retrain") as run:
    run_id = run.info.run_id
    print(f"MLflow Run ID: {{run_id}}")
    
    # è®°å½•å‚æ•°
    mlflow.log_params({{
        "table": TABLE_NAME,
        "label": LABEL_COL,
        "n_rows": len(df),
        "n_features": len(FEATURE_COLS),
        "task": TASK,
        "metric": METRIC,
        "estimator": BEST_ESTIMATOR,
        "train_ratio": TRAIN_RATIO,
        "val_ratio": VAL_RATIO,
        "test_ratio": TEST_RATIO,
        "random_seed": RANDOM_SEED,
    }})
    
    # è®°å½•è¶…å‚æ•°
    for key, value in BEST_PARAMS.items():
        mlflow.log_param(f"model__{{key}}", value)
    
    # åˆ›å»ºé¢„å¤„ç†å™¨
    preprocessor = StandardScaler()
    
    # åˆ›å»ºæ¨¡å‹
    model = {estimator_class}(**BEST_PARAMS, random_state=RANDOM_SEED)
    
    # åˆ›å»º Pipeline
    pipeline = Pipeline([
        ("preprocess", preprocessor),
        ("clf", model)
    ])
    
    print("\\nå¼€å§‹è®­ç»ƒ...")
    pipeline.fit(X_train, y_train)
    print("âœ“ è®­ç»ƒå®Œæˆ")
    
    # é¢„æµ‹
    y_train_pred = pipeline.predict(X_train)
    y_val_pred = pipeline.predict(X_val)
    y_test_pred = pipeline.predict(X_test)
    
    # è®¡ç®—æŒ‡æ ‡
    train_acc = accuracy_score(y_train, y_train_pred)
    val_acc = accuracy_score(y_val, y_val_pred)
    test_acc = accuracy_score(y_test, y_test_pred)
    
    train_f1 = f1_score(y_train, y_train_pred, average='weighted', zero_division=0)
    val_f1 = f1_score(y_val, y_val_pred, average='weighted', zero_division=0)
    test_f1 = f1_score(y_test, y_test_pred, average='weighted', zero_division=0)
    
    train_precision = precision_score(y_train, y_train_pred, average='weighted', zero_division=0)
    val_precision = precision_score(y_val, y_val_pred, average='weighted', zero_division=0)
    test_precision = precision_score(y_test, y_test_pred, average='weighted', zero_division=0)
    
    train_recall = recall_score(y_train, y_train_pred, average='weighted', zero_division=0)
    val_recall = recall_score(y_val, y_val_pred, average='weighted', zero_division=0)
    test_recall = recall_score(y_test, y_test_pred, average='weighted', zero_division=0)
    
    # è®°å½•æŒ‡æ ‡
    mlflow.log_metrics({{
        "train_accuracy": train_acc,
        "val_accuracy": val_acc,
        "test_accuracy": test_acc,
        "train_f1": train_f1,
        "val_f1": val_f1,
        "test_f1": test_f1,
        "train_precision": train_precision,
        "val_precision": val_precision,
        "test_precision": test_precision,
        "train_recall": train_recall,
        "val_recall": val_recall,
        "test_recall": test_recall,
    }})
    
    print("\\nâœ“ è¯„ä¼°æŒ‡æ ‡:")
    print(f"  è®­ç»ƒé›† - Accuracy: {{train_acc:.4f}}, F1: {{train_f1:.4f}}")
    print(f"  éªŒè¯é›† - Accuracy: {{val_acc:.4f}}, F1: {{val_f1:.4f}}")
    print(f"  æµ‹è¯•é›† - Accuracy: {{test_acc:.4f}}, F1: {{test_f1:.4f}}")

# %% [markdown]
# ## 7. ç”Ÿæˆå¯è§†åŒ–

# %%
# æ··æ·†çŸ©é˜µ - éªŒè¯é›†
cm_val = confusion_matrix(y_val, y_val_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_val, annot=True, fmt='d', cmap='Blues')
plt.title('Validation Set Confusion Matrix')
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.tight_layout()
mlflow.log_figure(plt.gcf(), "artifacts/val_confusion_matrix.png")
plt.show()

# æ··æ·†çŸ©é˜µ - æµ‹è¯•é›†
cm_test = confusion_matrix(y_test, y_test_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm_test, annot=True, fmt='d', cmap='Blues')
plt.title('Test Set Confusion Matrix')
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.tight_layout()
mlflow.log_figure(plt.gcf(), "artifacts/test_confusion_matrix.png")
plt.show()

print("âœ“ æ··æ·†çŸ©é˜µå·²ç”Ÿæˆ")

# %%
# ç‰¹å¾é‡è¦æ€§ï¼ˆå¦‚æœæ¨¡å‹æ”¯æŒï¼‰
if hasattr(pipeline.named_steps['clf'], 'feature_importances_'):
    importances = pipeline.named_steps['clf'].feature_importances_
    feature_importance_df = pd.DataFrame({{
        'feature': FEATURE_COLS,
        'importance': importances
    }}).sort_values('importance', ascending=False)
    
    # ä¿å­˜ JSON
    mlflow.log_dict(feature_importance_df.to_dict('records'), "artifacts/feature_importance.json")
    
    # ç»˜åˆ¶å›¾è¡¨
    plt.figure(figsize=(10, 6))
    top_n = min(20, len(feature_importance_df))
    sns.barplot(data=feature_importance_df.head(top_n), x='importance', y='feature')
    plt.title(f'Top {{top_n}} Feature Importances')
    plt.xlabel('Importance')
    plt.tight_layout()
    mlflow.log_figure(plt.gcf(), "artifacts/feature_importance.png")
    plt.show()
    
    print("âœ“ ç‰¹å¾é‡è¦æ€§å·²ç”Ÿæˆ")
else:
    print("âš ï¸  æ¨¡å‹ä¸æ”¯æŒç‰¹å¾é‡è¦æ€§")

# %% [markdown]
# ## 8. æ³¨å†Œæ¨¡å‹

# %%
with mlflow.start_run(run_id=run_id):
    # æ¨æ–­æ¨¡å‹ç­¾å
    from mlflow.models.signature import infer_signature
    signature = infer_signature(X_train, pipeline.predict(X_train))
    
    # å‡†å¤‡è¾“å…¥ç¤ºä¾‹
    input_example = X_train.head(5)
    
    # è®°å½•æ¨¡å‹
    model_info = mlflow.sklearn.log_model(
        sk_model=pipeline,
        artifact_path="model",
        signature=signature,
        input_example=input_example,
        registered_model_name=MODEL_NAME,
        metadata={{
            "task": TASK,
            "metric": METRIC,
            "best_estimator": BEST_ESTIMATOR,
            "framework": "sklearn",
            "source": "wedata_automl_retrain",
        }}
    )
    
    # è·å–æ¨¡å‹ç‰ˆæœ¬
    if hasattr(model_info, 'registered_model_version'):
        version = int(model_info.registered_model_version)
    else:
        version = None
    
    # è®°å½•é¢å¤–çš„ artifacts
    mlflow.log_dict({{
        "feature_columns": FEATURE_COLS,
        "label_column": LABEL_COL,
        "best_params": BEST_PARAMS,
    }}, "artifacts/model_config.json")
    
    mlflow.log_dict({{
        "total_samples": len(df),
        "train_samples": len(X_train),
        "val_samples": len(X_val),
        "test_samples": len(X_test),
        "num_features": len(FEATURE_COLS),
    }}, "artifacts/dataset_stats.json")
    
    # æ·»åŠ æ ‡ç­¾
    mlflow.set_tags({{
        "framework": "sklearn",
        "task": TASK,
        "estimator": BEST_ESTIMATOR,
        "metric": METRIC,
        "source": "wedata_automl_retrain",
    }})
    
    print(f"\\nâœ“ æ¨¡å‹å·²æ³¨å†Œ: {{MODEL_NAME}}")
    print(f"  - ç‰ˆæœ¬: {{version if version else 'N/A'}}")
    print(f"  - URI: {{model_info.model_uri}}")

# %% [markdown]
# ## 9. æ¨¡å‹æ¨ç†æµ‹è¯•

# %%
# åŠ è½½æ³¨å†Œçš„æ¨¡å‹
model_uri = f"models:/{{MODEL_NAME}}/{{version if version else 'latest'}}"
loaded_model = mlflow.pyfunc.load_model(model_uri)

print(f"âœ“ æ¨¡å‹å·²åŠ è½½: {{model_uri}}")

# å‡†å¤‡æ¨ç†æ•°æ®ï¼ˆä½¿ç”¨æµ‹è¯•é›†çš„å‰ 5 è¡Œï¼‰
inference_data = X_test.head(5)

# æ‰§è¡Œæ¨ç†
predictions = loaded_model.predict(inference_data)

# æ˜¾ç¤ºç»“æœ
result_df = inference_data.copy()
result_df['prediction'] = predictions
result_df['actual'] = y_test.head(5).values

print("\\næ¨ç†ç»“æœ:")
display(result_df)

# %% [markdown]
# ## 10. æ€»ç»“

# %%
print("="*80)
print("æ¨¡å‹è®­ç»ƒå’Œæ³¨å†Œå®Œæˆ!")
print("="*80)
print(f"å®éªŒåç§°: {{EXPERIMENT_NAME}}")
print(f"Run ID: {{run_id}}")
print(f"æ¨¡å‹åç§°: {{MODEL_NAME}}")
print(f"æ¨¡å‹ç‰ˆæœ¬: {{version if version else 'N/A'}}")
print(f"\\nå…³é”®æŒ‡æ ‡:")
print(f"  - éªŒè¯é›† Accuracy: {{val_acc:.4f}}")
print(f"  - æµ‹è¯•é›† Accuracy: {{test_acc:.4f}}")
print(f"  - éªŒè¯é›† F1: {{val_f1:.4f}}")
print(f"  - æµ‹è¯•é›† F1: {{test_f1:.4f}}")
print("="*80)
print("\\nğŸ’¡ ä¸‹ä¸€æ­¥:")
print("  1. åœ¨ MLflow UI ä¸­æŸ¥çœ‹å®éªŒè¯¦æƒ…")
print("  2. éƒ¨ç½²æ¨¡å‹åˆ°ç”Ÿäº§ç¯å¢ƒ")
print("  3. ç›‘æ§æ¨¡å‹æ€§èƒ½")
print("="*80)
'''
    
    return code


def save_notebook_code(code: str, output_path: str):
    """
    ä¿å­˜ Notebook ä»£ç åˆ°æ–‡ä»¶
    
    Args:
        code: Notebook ä»£ç 
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(code)

