# CodeLens

### *Give your LLM glasses to understand meaning, not just read words*

[![Tests](https://github.com/cornelcroi/codelens/workflows/Tests/badge.svg)](https://github.com/cornelcroi/codelens/actions)
[![PyPI version](https://badge.fury.io/py/context-lens.svg)](https://badge.fury.io/py/context-lens)
[![Python 3.11+](https://img.shields.io/badge/python-3.11+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

**CodeLens is semantic search for AI assistants. Drop in any knowledge source - documentation, repositories, notes, or local files - and your AI can instantly understand and answer questions about it. No configuration, no build step - it just works.**

CodeLens is a Model Context Protocol (MCP) server that gives AI assistants the ability to semantically search and understand any content using vector embeddings and LanceDB.

Works with Claude Desktop, Kiro IDE, Continue.dev, and other MCP clients.

## Setup with Your LLM

No installation needed! Just configure your AI assistant to use CodeLens:

### Claude Desktop (Recommended)

Add to `~/Library/Application Support/Claude/claude_desktop_config.json` (macOS) or `%APPDATA%/Claude/claude_desktop_config.json` (Windows):

```json
{
  "mcpServers": {
    "context-lens": {
      "command": "uvx",
      "args": ["context-lens"]
    }
  }
}
```

Restart Claude Desktop and you're ready!

### Kiro IDE

Add to `.kiro/settings/mcp.json` in your workspace:

```json
{
  "mcpServers": {
    "context-lens": {
      "command": "uvx",
      "args": ["context-lens"],
      "disabled": false,
      "autoApprove": ["list_documents", "search_documents"]
    }
  }
}
```

Reload MCP servers (Command Palette â†’ "MCP: Reload Servers") and start using it!

### Continue.dev

Edit `~/.continue/config.json`:

```json
{
  "mcpServers": [
    {
      "name": "context-lens",
      "command": "uvx",
      "args": ["context-lens"]
    }
  ]
}
```

### Other MCP Clients

For any MCP-compatible client, use:

```json
{
  "command": "uvx",
  "args": ["context-lens"]
}
```

### Custom Database Location (Optional)

```json
{
  "mcpServers": {
    "context-lens": {
      "command": "uvx",
      "args": ["context-lens"],
      "env": {
        "LANCE_DB_PATH": "./my_knowledge_base.db"
      }
    }
  }
}
```

## What You Can Add

CodeLens works with any text-based content:

- **ğŸ“„ Single files**: `./README.md`, `/path/to/document.txt`
- **ğŸ“ Local folders**: `./docs/`, `/path/to/project/`
- **ğŸ’» Local repositories**: `./my-project/`, `/Users/you/code/app/`
- **ğŸŒ GitHub URLs**: 
  - Repositories: `https://github.com/user/repo`
  - Specific files: `https://github.com/user/repo/blob/main/file.py`
  - Directories: `https://github.com/user/repo/tree/main/src`
- **ğŸ“š Documentation sites**: Any markdown, text, or code files
- **ğŸ“ Notes and wikis**: Personal knowledge bases, team wikis

## Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         Your LLM Client                              â”‚
â”‚              (Claude Desktop, Kiro IDE, Continue.dev)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚ MCP Protocol
                                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                          CodeLens Server                             â”‚
â”‚                                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  add_document   â”‚  â”‚ search_documents â”‚  â”‚ list_documents   â”‚  â”‚
â”‚  â”‚                 â”‚  â”‚                  â”‚  â”‚                  â”‚  â”‚
â”‚  â”‚  Ingests files  â”‚  â”‚  Semantic search â”‚  â”‚  Browse indexed  â”‚  â”‚
â”‚  â”‚  (.py, .txt)    â”‚  â”‚  with vectors    â”‚  â”‚  documents       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚           â”‚                    â”‚                      â”‚             â”‚
â”‚           â–¼                    â–¼                      â–¼             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚              Document Processing Pipeline                     â”‚  â”‚
â”‚  â”‚                                                                â”‚  â”‚
â”‚  â”‚  1. Content Extraction  â†’  2. Chunking  â†’  3. Embedding      â”‚  â”‚
â”‚  â”‚     â€¢ File reading          â€¢ Smart split    â€¢ Sentence       â”‚  â”‚
â”‚  â”‚     â€¢ Encoding detect       â€¢ Overlap        Transformers     â”‚  â”‚
â”‚  â”‚     â€¢ Hash generation       â€¢ Metadata       â€¢ Local model    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                â–¼                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                    LanceDB Vector Store                       â”‚  â”‚
â”‚  â”‚                                                                â”‚  â”‚
â”‚  â”‚  ğŸ“„ Documents Table          ğŸ“¦ Chunks Table                  â”‚  â”‚
â”‚  â”‚  â€¢ Metadata                  â€¢ Text content                   â”‚  â”‚
â”‚  â”‚  â€¢ File paths                â€¢ 384-dim vectors                â”‚  â”‚
â”‚  â”‚  â€¢ Timestamps                â€¢ Document refs                  â”‚  â”‚
â”‚  â”‚  â€¢ Chunk counts              â€¢ Fast ANN search                â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                                 â–¼
                    ğŸ’¾ Local Storage (100% Offline)
                    â€¢ knowledge_base.db
                    â€¢ Embedding model cache
                    â€¢ No external API calls
```


## Manual Installation (Optional)

Most users don't need to install anything - just configure your LLM client as shown above and `uvx` will handle everything automatically.

If you prefer to install locally:

```bash
pip install context-lens
```

Or install from source:

```bash
git clone https://github.com/cornelcroi/codelens.git
cd codelens
pip install -e .
```

## What You Can Add

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Supported Input Types                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                       â”‚
â”‚  ğŸ“ Local Files & Directories                                        â”‚
â”‚  â”œâ”€ Single file:      /path/to/script.py                            â”‚
â”‚  â”œâ”€ Directory:        /path/to/project/src/                         â”‚
â”‚  â””â”€ Recursive:        Automatically processes subdirectories         â”‚
â”‚                                                                       â”‚
â”‚  ğŸ™ GitHub Repositories (Public)                                     â”‚
â”‚  â”œâ”€ Entire repo:      https://github.com/user/repo                  â”‚
â”‚  â”œâ”€ Specific branch:  https://github.com/user/repo/tree/develop     â”‚
â”‚  â”œâ”€ Subdirectory:     https://github.com/user/repo/tree/main/src    â”‚
â”‚  â””â”€ Single file:      https://github.com/user/repo/blob/main/file.pyâ”‚
â”‚                                                                       â”‚
â”‚  ğŸ“„ Supported File Types (20+ formats)                               â”‚
â”‚  â”œâ”€ Python:           .py                                            â”‚
â”‚  â”œâ”€ JavaScript/TS:    .js, .jsx, .ts, .tsx                          â”‚
â”‚  â”œâ”€ Web:              .md, .txt, .json, .yaml, .yml                 â”‚
â”‚  â”œâ”€ Systems:          .java, .cpp, .c, .h, .go, .rs                 â”‚
â”‚  â””â”€ Scripts:          .sh, .bash, .rb, .php                         â”‚
â”‚                                                                       â”‚
â”‚  ğŸš« Automatically Ignored                                            â”‚
â”‚  â”œâ”€ Dependencies:     node_modules, venv, vendor                    â”‚
â”‚  â”œâ”€ Build outputs:    dist, build, target, out                      â”‚
â”‚  â”œâ”€ Caches:           __pycache__, .cache, .pytest_cache            â”‚
â”‚  â”œâ”€ Version control:  .git, .svn, .hg                               â”‚
â”‚  â”œâ”€ IDE files:        .idea, .vscode, .vs                           â”‚
â”‚  â””â”€ Large files:      Files over 10MB                               â”‚
â”‚                                                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ’¡ Try These Popular Repositories

**Web Frameworks:**
```
https://github.com/django/django          # Django web framework
https://github.com/pallets/flask          # Flask microframework  
https://github.com/fastapi/fastapi        # FastAPI modern framework
```

**Data Science:**
```
https://github.com/pandas-dev/pandas      # Pandas data analysis
https://github.com/scikit-learn/scikit-learn  # Machine learning
https://github.com/pytorch/pytorch        # PyTorch deep learning
```

**Utilities:**
```
https://github.com/psf/requests           # HTTP library
https://github.com/python/cpython         # Python itself!
https://github.com/pallets/click          # CLI framework
```

**Learn Specific Features:**
```
https://github.com/django/django/tree/main/django/contrib/auth  # Django auth
https://github.com/fastapi/fastapi/tree/master/fastapi          # FastAPI core
https://github.com/requests/requests/tree/main/requests         # Requests lib
```

## Available Tools

Once connected to your LLM, you get six powerful tools:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ“¥ add_document(file_path_or_url)                               â”‚
â”‚    Add documents to the knowledge base                          â”‚
â”‚    â†’ Local files: "/path/to/file.py"                            â”‚
â”‚    â†’ GitHub repos: "https://github.com/user/repo"               â”‚
â”‚    â†’ GitHub files: "https://github.com/user/repo/blob/main/..." â”‚
â”‚    â†’ Smart: Skips if already indexed with same content          â”‚
â”‚    â†’ Extracts content, creates embeddings, stores in LanceDB    â”‚
â”‚                                                                  â”‚
â”‚ ğŸ” search_documents(query, limit=10)                            â”‚
â”‚    Semantic search across all documents                         â”‚
â”‚    â†’ Finds relevant code/text by meaning, not just keywords     â”‚
â”‚                                                                  â”‚
â”‚ ğŸ“‹ list_documents(limit=100, offset=0)                          â”‚
â”‚    List all indexed documents with pagination                   â”‚
â”‚    â†’ Browse what's in your knowledge base                       â”‚
â”‚                                                                  â”‚
â”‚ â„¹ï¸  get_document_info(file_path)                                â”‚
â”‚    Get metadata about a specific document                       â”‚
â”‚    â†’ Check if indexed, when added, content hash, chunk count    â”‚
â”‚                                                                  â”‚
â”‚ ğŸ—‘ï¸  remove_document(file_path)                                  â”‚
â”‚    Remove a specific document from the knowledge base           â”‚
â”‚    â†’ Clean up outdated or unwanted files                        â”‚
â”‚                                                                  â”‚
â”‚ ğŸ§¹ clear_knowledge_base()                                       â”‚
â”‚    Remove all documents and start fresh                         â”‚
â”‚    â†’ Complete reset when needed                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Example Conversations

**Adding a GitHub Repository:**
```
You: Add the FastAPI repository to my knowledge base

LLM: I'll add the FastAPI repository for you.
     [Calls add_document("https://github.com/tiangolo/fastapi")]
     
     Cloning repository...
     Processing 247 Python files...
     âœ“ Added 247 files from repository with 1,543 chunks
     
     You can now ask questions about FastAPI's implementation!

You: How does FastAPI handle dependency injection?

LLM: [Searches the FastAPI codebase...]
     Based on the FastAPI source code, dependency injection works through...
```

**Adding Local Files:**
```
You: Add all Python files in my src/ directory to the knowledge base

LLM: I'll add those files for you.
     [Calls add_document for each .py file]
     âœ“ Added 15 Python files to the knowledge base

You: How do we handle authentication in this codebase?

LLM: Let me search for authentication-related code.
     [Calls search_documents with query "authentication handling"]
     
     Based on the code, you use JWT tokens with OAuth2. Here's what I found:
     - src/auth/jwt.py: Token generation and validation
     - src/auth/oauth.py: OAuth2 flow implementation
     - src/middleware/auth.py: Authentication middleware
     
     The main authentication flow is...
```

## Quick Start Examples

### Example 1: Add Your Project
```
You: Add all Python files from /Users/me/my-project/src to the knowledge base

LLM: [Processes all .py files in the directory]
     âœ“ Added 23 Python files with 156 chunks
```

### Example 2: Learn from Open Source
```
You: Add the FastAPI repository so I can learn how it works

LLM: [Calls add_document("https://github.com/tiangolo/fastapi")]
     Cloning repository...
     âœ“ Added 247 files from repository with 1,543 chunks
     
You: How does FastAPI handle dependency injection?

LLM: [Searches and explains based on actual FastAPI source code]
```

### Example 3: Research a Specific Feature
```
You: Add just the authentication module from Django

LLM: [Calls add_document("https://github.com/django/django/tree/main/django/contrib/auth")]
     âœ“ Added 45 files from django/contrib/auth with 312 chunks

You: Show me how Django implements password hashing

LLM: [Provides detailed explanation with code references]
```

## Example Queries

Once you've added documents, here are powerful queries you can ask:

### ğŸ” Understanding Code
```
"How does this codebase handle database connections?"
"Explain the authentication flow in this project"
"What design patterns are used in this repository?"
"How is error handling implemented?"
"Show me how the API endpoints are structured"
```

### ğŸ› Debugging & Problem Solving
```
"Find examples of how to handle file uploads"
"Where is the rate limiting logic implemented?"
"Show me similar error handling patterns"
"How do other files handle this exception?"
"Find all places where we validate user input"
```

### ğŸ“š Learning & Research
```
"How does FastAPI implement dependency injection?"
"Compare how Django and Flask handle routing"
"What's the difference between these two implementations?"
"Show me examples of async/await usage in this codebase"
"How does this library handle backwards compatibility?"
```

### â™»ï¸ Refactoring & Code Review
```
"Find all files that use the old authentication method"
"Where else do we use this deprecated function?"
"Show me similar code that might have the same bug"
"Find duplicate logic that could be refactored"
"What files would be affected if I change this interface?"
```

### ğŸ¯ Specific Implementation Questions
```
"How do I use the caching system in this project?"
"Show me examples of writing tests for API endpoints"
"How is configuration managed in this codebase?"
"Find examples of custom middleware implementation"
"How do I add a new database model?"
```

### ğŸŒŸ Open Source Exploration
```
"How does React implement hooks internally?"
"Show me how Django's ORM builds SQL queries"
"How does FastAPI achieve such high performance?"
"Explain how pytest's fixture system works"
"How does requests handle HTTP retries?"
```

### ğŸ’¡ Tips for Better Queries

**âœ… Good Queries:**
- Be specific: "How does FastAPI validate request bodies?"
- Ask about concepts: "Explain the middleware pattern in this code"
- Request examples: "Show me examples of async database queries"
- Compare: "How is this different from the old implementation?"

**âŒ Avoid:**
- Too vague: "Tell me about the code"
- Too broad: "Explain everything"
- Outside scope: Questions about code not in the knowledge base

## Advanced Configuration

### For Local Development (Not Yet Published)

If you're developing CodeLens locally:

**Claude Desktop:**
```json
{
  "mcpServers": {
    "context-lens": {
      "command": "context-lens"
    }
  }
}
```

**Kiro IDE:**
```json
{
  "mcpServers": {
    "context-lens": {
      "command": "python",
      "args": ["-m", "context_lens.main"],
      "disabled": false,
      "autoApprove": ["list_documents", "search_documents"]
    }
  }
}
```

### MCP Inspector (Testing & Development)

MCP Inspector is a web-based tool for testing MCP servers during development.

**Quick Start:**
```bash
# Test with MCP Inspector
DANGEROUSLY_OMIT_AUTH=true npx @modelcontextprotocol/inspector python -m context_lens.server
```

**What happens:**
1. Server starts in < 1 second (lazy initialization)
2. Inspector opens in your browser showing all 6 tools
3. First tool invocation loads embedding models (5-10 seconds, one-time)
4. Subsequent calls are fast (< 1 second)

**Testing workflow:**
- Use Inspector's UI to call tools with different parameters
- View request/response JSON in real-time
- Check logs in `./logs/context-lens.log` for detailed info
- Test error handling with invalid inputs

**Note:** The server uses lazy initialization, so startup is fast but the first tool call will take longer as it loads the embedding model. This is expected behavior and only happens once per session.

## How It Works

### The Magic Behind the Scenes

```
1. ğŸ“„ Document Ingestion
   â”œâ”€ Read file content with encoding detection
   â”œâ”€ Generate content hash for deduplication
   â”œâ”€ Extract metadata (size, timestamps, type)
   â””â”€ Split into overlapping chunks (~1000 chars)

2. ğŸ§® Vector Embedding
   â”œâ”€ Load sentence-transformers model (all-MiniLM-L6-v2)
   â”œâ”€ Convert each chunk to 384-dimensional vector
   â”œâ”€ Batch processing for efficiency
   â””â”€ Store vectors in LanceDB

3. ğŸ” Semantic Search
   â”œâ”€ Convert search query to vector
   â”œâ”€ Find similar vectors using ANN (Approximate Nearest Neighbor)
   â”œâ”€ Rank results by cosine similarity
   â””â”€ Return relevant chunks with metadata

4. ğŸ’¾ Storage
   â”œâ”€ LanceDB: Fast columnar vector database
   â”œâ”€ Two tables: documents + chunks
   â”œâ”€ Efficient updates and deletes
   â””â”€ All data stays local
```

### First Run

On first use, `uvx` automatically:
- Downloads and installs the package
- Installs all dependencies  
- Downloads the embedding model (~100MB, one-time)
- Starts the server

The server then:
- Creates `knowledge_base.db` in your current directory
- Stores logs in `./logs`
- Supports `.py` and `.txt` files by default

**Zero configuration needed!**

## Why Use This?

### Traditional Keyword Search
```
You: "Find authentication code"
Result: Files containing the word "authentication"
Problem: Misses related concepts like "login", "auth", "credentials"
```

### Semantic Search with This MCP
```
You: "Find authentication code"  
Result: All auth-related code including:
  âœ“ Files about "login" and "sign in"
  âœ“ Code handling "credentials" and "tokens"
  âœ“ "Authorization" and "access control"
  âœ“ Related security implementations

Why: Understands meaning, not just words
```

### Real-World Use Cases

- **ğŸ” Code Discovery** - "How do we handle database connections?"
- **ğŸ“š Onboarding** - New team members understand the codebase faster
- **ğŸ› Debugging** - "Find similar error handling patterns"
- **â™»ï¸ Refactoring** - "Where do we use this deprecated pattern?"
- **ğŸ“– Documentation** - "Explain how the auth system works"
- **ğŸ¯ Code Review** - "Find related code that might be affected"
- **ğŸŒŸ Learn from OSS** - "Add the React repository and explain how hooks work"
- **ğŸ“¦ Library Research** - "Add this library and show me how to use feature X"

## Troubleshooting

### Common Issues

**Server not starting?**
```bash
# Check installation
context-lens --version

# View detailed logs
tail -f logs/context-lens.log

# Check for errors
tail -f logs/errors.log
```

**First run is slow?**
The embedding model (~100MB) downloads on first use. This only happens once. Subsequent runs are fast.

**First tool call is slow?**
The server uses lazy initialization - it starts quickly but loads the embedding model on the first tool invocation. This takes 5-10 seconds and only happens once per session. This is intentional to provide fast startup times for MCP Inspector and other tools.

**MCP Inspector not connecting?**
```bash
# Make sure you're using the correct command
npx @modelcontextprotocol/inspector python -m context_lens.server

# NOT this (incorrect):
# npx @modelcontextprotocol/inspector fastmcp run context_lens.server:app

# Check that Python can find the module
python -m context_lens.server --help
```

**Tools not appearing in LLM client?**
1. Verify the server is configured correctly in your client's MCP settings
2. Restart your LLM client after configuration changes
3. Check the client's logs for connection errors
4. For Kiro IDE: Use Command Palette â†’ "MCP: Reload Servers"

**Database errors?**
```bash
# Check database location
ls -la knowledge_base.db

# If corrupted, you can reset it
rm -rf knowledge_base.db
# The server will create a new database on next run
```

**Import errors or missing dependencies?**
```bash
# Reinstall dependencies
pip install -r requirements.txt

# For development installation
pip install -e .
```

**Logs show "stdio transport" errors?**
This usually means something is writing to stdout when it shouldn't. The server is configured to log only to files to keep stdio clean for MCP protocol communication. If you see this:
1. Check for any `print()` statements in your code
2. Verify logging is configured correctly (should only write to files)
3. Check third-party libraries aren't writing to stdout

**Performance issues?**
- First document addition: Slow (model loading)
- Subsequent operations: Should be fast (< 1 second)
- Large files (>10MB): Automatically skipped
- Many files: Processed in batches

**Configuration issues?**
```bash
# Check environment variables
env | grep MCP_KB

# Use config file for complex setups
cp config.example.yaml config.yaml
# Edit config.yaml with your settings
context-lens --config config.yaml
```

**Still having issues?**
1. Check the [documentation](#documentation) below
2. Review logs in `./logs/` directory
3. Try with MCP Inspector to isolate the issue
4. Report bugs via [GitHub Issues](https://github.com/cornelcroi/codelens/issues)

## Technical Details

### Stack

- **MCP Framework**: FastMCP - Modern Python MCP implementation
- **Vector Database**: LanceDB - Fast, embedded vector database
- **Embeddings**: sentence-transformers/all-MiniLM-L6-v2 (384 dimensions)
- **Search**: Approximate Nearest Neighbor (ANN) with cosine similarity
- **Storage**: Columnar format with Apache Arrow

### Performance

- **Embedding Speed**: ~1000 tokens/second on CPU
- **Search Latency**: <100ms for most queries
- **Storage**: ~1KB per chunk (text + vector + metadata)
- **Memory**: ~500MB (model) + database size

### Supported File Types

Supported file types:
- `.py` - Python source code
- `.txt` - Plain text files
- `.md` - Markdown
- `.js`, `.ts` - JavaScript/TypeScript
- `.java`, `.cpp`, `.c`, `.h` - C/C++/Java
- `.go`, `.rs` - Go/Rust
- And more text-based formats

## Contributing

To contribute or run from source:

```bash
git clone https://github.com/yourusername/codelens.git
cd codelens
pip install -e .
pytest tests/
```

### Environment Variables

Configure via environment variables in your MCP client:

```json
{
  "env": {
    "LANCE_DB_PATH": "./codelens.db",
    "LOG_LEVEL": "INFO"
  }
}
```

## Contributing

Contributions are welcome! This is an open-source project.

- Report bugs and request features via [GitHub Issues](https://github.com/yourusername/codelens/issues)
- Submit pull requests for improvements
- Star the repo if you find it useful! â­

## License

MIT License
