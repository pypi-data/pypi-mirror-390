#!/usr/bin/env python3
"""
×“×•×’×××•×ª ××ª×§×“××•×ª ×œ-STT ×¢×‘×•×¨ ××¢×¨×›×•×ª ×™××•×ª ×”××©×™×—
Advanced STT examples for Yemot HaMashiach systems
"""
import sys
import os

# Add src to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', 'src'))

from yemot_speech import STT


def yemot_phone_call_example():
    """×“×•×’××” ××™×•×—×“×ª ×œ×©×™×—×•×ª ×˜×œ×¤×•×Ÿ ×©×œ ×™××•×ª ×”××©×™×—"""
    print("ğŸ“ Yemot Phone Call Example")
    
    # ×œ×©×™×—×•×ª ×˜×œ×¤×•×Ÿ ××•××œ×¥ ×œ×”×©×ª××© ×‘-Google ×¢× ×”×’×“×¨×•×ª ××•×ª×××•×ª
    stt = STT(
        provider='google',
        credentials_path='path/to/credentials.json'
    )
    
    # ×¢×‘×•×¨ ×§×•×‘×¦×™ ×©××¢ ×©×œ ×©×™×—×•×ª ×™××•×ª ×”××©×™×— (×‘×“×¨×š ×›×œ×œ ××•-×œ××• ××• WAV)
    phone_audio = 'path/to/yemot_call.wav'
    
    # ×”××¨×” ×¢× ×”×’×“×¨×•×ª ××•×ª×××•×ª ×œ×©×™×—×•×ª ×˜×œ×¤×•×Ÿ
    text = stt.transcribe(
        phone_audio,
        encoding='MULAW',  # ×§×™×“×•×“ × ×¤×•×¥ ×‘×©×™×—×•×ª ×˜×œ×¤×•×Ÿ
        sample_rate=8000,  # ×ª×“×™×¨×•×ª × ×¤×•×¦×” ×‘×˜×œ×¤×•× ×™×”
        language_code='he-IL',
        enable_automatic_punctuation=True,
        enable_word_time_offsets=True
    )
    
    print(f"ğŸ“± Yemot call transcribed: {text}")


def interactive_voice_menu_example():
    """×“×•×’××” ×œ×¢×™×‘×•×“ ×ª×¤×¨×™×˜ ×§×•×œ×™ ××™× ×˜×¨××§×˜×™×‘×™"""
    print("ğŸ›ï¸ Interactive Voice Menu Processing")
    
    stt = STT(provider='openai', api_key='your-key')
    
    # ×¡×™××•×œ×¦×™×” ×©×œ ×§×œ×˜ ××©×ª××© ×‘×ª×¤×¨×™×˜ ×§×•×œ×™
    user_responses = [
        'path/to/user_says_zmanim.wav',      # "×–×× ×™×"
        'path/to/user_says_hodaot.wav',      # "×”×•×“×¢×•×ª"
        'path/to/user_says_contact.wav',     # "×¦×•×¨ ×§×©×¨"
        'path/to/user_says_number_2.wav',    # "×©×ª×™×™×"
    ]
    
    # ××™×œ×•×Ÿ ×ª×’×•×‘×•×ª ××¤×©×¨×™×•×ª
    menu_responses = {
        '×–×× ×™×': '×–×× ×™ ×”×ª×¤×™×œ×•×ª',
        '×ª×¤×™×œ×•×ª': '×–×× ×™ ×”×ª×¤×™×œ×•×ª',
        '×”×•×“×¢×•×ª': '×”×•×“×¢×•×ª ×”××¢×¨×›×ª',
        '×—×“×©×•×ª': '×”×•×“×¢×•×ª ×”××¢×¨×›×ª',
        '×¦×•×¨ ×§×©×¨': '×™×¦×™×¨×ª ×§×©×¨',
        '×¢×–×¨×”': '××¨×›×– ×¢×–×¨×”',
        '××—×“': '1',
        '×©×ª×™×™×': '2',
        '×©×œ×•×©': '3',
        '××¨×‘×¢': '4',
    }
    
    for i, audio_file in enumerate(user_responses, 1):
        try:
            # ×”××¨×ª ×“×‘×•×¨ ×”××©×ª××© ×œ×˜×§×¡×˜
            user_text = stt.transcribe(audio_file, language='he')
            print(f"ğŸ‘¤ User {i} said: {user_text}")
            
            # ×–×™×”×•×™ ×›×•×•× ×ª ×”××©×ª××©
            intent = None
            for keyword, response in menu_responses.items():
                if keyword in user_text.lower():
                    intent = response
                    break
            
            if intent:
                print(f"ğŸ¯ Detected intent: {intent}")
            else:
                print("â“ Intent not recognized - redirecting to main menu")
                
        except Exception as e:
            print(f"âŒ Error processing audio {i}: {e}")


def batch_audio_processing_example():
    """×“×•×’××” ×œ×¢×™×‘×•×“ ××¦×•×•×” ×©×œ ×§×‘×¦×™ ×©××¢"""
    print("ğŸ“¦ Batch Audio Processing")
    
    stt = STT(provider='openai', api_key='your-key')
    
    # ×¨×©×™××ª ×§×‘×¦×™ ×©××¢ ×œ×¢×™×‘×•×“
    audio_files = [
        'recordings/call_001.wav',
        'recordings/call_002.wav', 
        'recordings/call_003.wav',
        'recordings/message_001.wav',
        'recordings/message_002.wav',
    ]
    
    results = []
    
    for i, audio_file in enumerate(audio_files, 1):
        try:
            print(f"ğŸµ Processing file {i}/{len(audio_files)}: {audio_file}")
            
            # ×”××¨×” ×œ×˜×§×¡×˜
            text = stt.transcribe(audio_file, language='he')
            
            # ×©××™×¨×ª ×ª×•×¦××”
            result = {
                'file': audio_file,
                'text': text,
                'length': len(text),
                'success': True
            }
            results.append(result)
            
            print(f"âœ… Success: {text[:50]}...")
            
        except Exception as e:
            print(f"âŒ Failed processing {audio_file}: {e}")
            results.append({
                'file': audio_file,
                'text': None,
                'error': str(e),
                'success': False
            })
    
    # ×¡×™×›×•× ×ª×•×¦××•×ª
    successful = sum(1 for r in results if r['success'])
    print(f"\nğŸ“Š Batch Results: {successful}/{len(audio_files)} successful")
    
    return results


def real_time_transcription_example():
    """×“×•×’××” ×œ×”××¨×” ×‘×–××Ÿ ×××ª (××“×•××”)"""
    print("â±ï¸ Real-time Transcription Simulation")
    
    stt = STT(provider='google', credentials_path='path/to/credentials.json')
    
    # ×¡×™××•×œ×¦×™×” ×©×œ ×—×œ×§×™ ×©××¢ ××’×™×¢×™× ×‘×–××Ÿ ×××ª
    audio_chunks = [
        'chunk_001.wav',  # "×©×œ×•×"
        'chunk_002.wav',  # "×× ×™ ×¨×•×¦×”"
        'chunk_003.wav',  # "×œ×©××•×¢"
        'chunk_004.wav',  # "×–×× ×™ ×ª×¤×™×œ×•×ª"
    ]
    
    full_transcription = ""
    
    for i, chunk in enumerate(audio_chunks, 1):
        try:
            print(f"ğŸ™ï¸ Processing chunk {i}...")
            
            # ×¢×™×‘×•×“ ×—×œ×§ ×©××¢
            chunk_text = stt.transcribe(chunk, language='he')
            full_transcription += f" {chunk_text}"
            
            print(f"ğŸ“ Chunk {i}: {chunk_text}")
            print(f"ğŸ“„ Full so far: {full_transcription.strip()}")
            
        except Exception as e:
            print(f"âŒ Error in chunk {i}: {e}")
    
    print(f"ğŸ¯ Final transcription: {full_transcription.strip()}")


def audio_quality_analysis_example():
    """×“×•×’××” ×œ× ×™×ª×•×— ××™×›×•×ª ×©××¢"""
    print("ğŸ” Audio Quality Analysis")
    
    stt = STT(provider='openai', api_key='your-key')
    
    test_files = [
        ('high_quality.wav', 'High quality studio recording'),
        ('phone_quality.wav', 'Phone call quality'),
        ('noisy_background.wav', 'Background noise'),
        ('low_volume.wav', 'Low volume recording'),
    ]
    
    for audio_file, description in test_files:
        try:
            print(f"ğŸµ Testing: {description}")
            
            # × ×¡×™×•×Ÿ ×œ×”××™×¨ ×œ×˜×§×¡×˜
            text = stt.transcribe(audio_file, language='he')
            
            # ×”×¢×¨×›×ª ××™×›×•×ª ×‘×¡×™×¡×™×ª ×¢×œ ×‘×¡×™×¡ ××•×¨×š ×”×˜×§×¡×˜ ×•××™×œ×™× ×‘×¨×•×¨×•×ª
            word_count = len(text.split())
            clarity_score = min(100, word_count * 10)  # ×¦×™×•×Ÿ ×¤×©×•×˜
            
            print(f"ğŸ“ Text: {text}")
            print(f"ğŸ“Š Words: {word_count}, Clarity Score: {clarity_score}%")
            
            if clarity_score < 30:
                print("âš ï¸ Poor audio quality detected")
            elif clarity_score < 70:
                print("ğŸ”¶ Moderate audio quality")
            else:
                print("âœ… Good audio quality")
                
        except Exception as e:
            print(f"âŒ Failed to process {audio_file}: {e}")
            print("ğŸ’¡ Consider audio preprocessing or different provider")
        
        print("-" * 40)


if __name__ == "__main__":
    print("ğŸ¯ ×“×•×’×××•×ª STT ××ª×§×“××•×ª - Advanced STT Examples")
    print("=" * 60)
    
    print("\nğŸ’¡ ×“×•×’×××•×ª ××œ×” ××™×•×¢×“×•×ª ×œ×©×™××•×© ××ª×§×“× ×‘××¢×¨×›×•×ª ×™××•×ª ×”××©×™×—")
    print("ğŸ’¡ These examples are for advanced usage in Yemot HaMashiach systems")
    
    print("\nğŸ”§ ×œ×¤× ×™ ×”×©×™××•×©:")
    print("  1. ×”×ª×§×Ÿ: pip install yemot-speech[openai] ××• [google]")
    print("  2. ×”×›×Ÿ API keys")
    print("  3. ×”×›×Ÿ ×§×‘×¦×™ ×©××¢ ×œ×‘×“×™×§×”")
    print("  4. ×”×ª×× × ×ª×™×‘×™ ×§×‘×¦×™× ×‘×§×•×“")
    
    # ×”×¨×¥ ×“×•×’××” ×‘×¡×™×¡×™×ª ×œ×”×“×’××”
    try:
        from yemot_speech import STT
        stt = STT(provider='demo')
        print(f"\nâœ… Library loaded successfully!")
        print(f"ğŸ“‹ Available providers: {stt.get_available_providers()}")
        
    except Exception as e:
        print(f"âŒ Error loading library: {e}")