import csv
import logging
import os
import shutil
import sys

import pandas as pd

from speccheck.collect import check_criteria, collect_files, write_to_file
from speccheck.criteria import get_criteria, get_species_field, validate_criteria
from speccheck.report import plot_charts
from speccheck.update_criteria import update_criteria_file
from speccheck.util import get_all_files, load_modules_with_checks


def collect(organism, input_filepaths, criteria_file, output_file, sample_id, metadata_file=None):
    """Collect and run checks from modules on input files."""
    # Check criteria file
    if not os.path.isfile(criteria_file):
        logging.error("Criteria file not found: %s", criteria_file)
        return
    errors, warnings = validate_criteria(criteria_file)
    if errors:
        for error in errors:
            logging.error("%s", error)
        sys.exit(1)
    if warnings:
        for warning in warnings:
            logging.warning("%s", warning)
    if not sample_id:
        logging.error("Sample name must be provided using --sample option.")
        return
    # Load metadata if provided
    metadata_dict = {}
    if metadata_file:
        if not os.path.isfile(metadata_file):
            logging.error("Metadata file not found: %s", metadata_file)
            return
        try:
            metadata_df = pd.read_csv(metadata_file)
            if "sample_id" not in metadata_df.columns:
                logging.error("Metadata file must contain a 'sample_id' column")
                return
            metadata_df.set_index("sample_id", inplace=True)
            metadata_dict = metadata_df.to_dict("index")
            logging.info(
                "Loaded metadata for %d samples from %s", len(metadata_dict), metadata_file
            )
        except Exception as e:
            logging.error("Error reading metadata file: %s", str(e))
            return

    # Get all files from the input paths
    all_files = get_all_files(input_filepaths)
    # Discover and load valid modules dynamically
    module_list = load_modules_with_checks()
    recovered_values = collect_files(all_files, module_list)
    if not recovered_values:
        logging.warning("No files passed the checks.")
        return

    # Need to resolve species if not provided
    org_list = []
    if not organism:
        species_fields = get_species_field(criteria_file)
        for field in species_fields:
            org_list.append(recovered_values.get(field["software"], {}).get(field["field"]))
        # remove None values
        org_list = [x for x in org_list if x is not None]
        organism = set(org_list)
        if len(organism) == 1:
            organism = list(organism)[0]
        elif len(organism) > 1:
            organism = None
            logging.error("Mixed species found in the files.")
        else:
            organism = None
    if not organism:
        logging.warning(
            "Organism name not provided and could not be resolved from the files. Using default values which are VERY lenient."
        )
        organism = "Unknown"
    logging.info("Finished checking %d files for %s", len(all_files), organism)
    logging.info("Found software: %s", ", ".join(recovered_values.keys()))
    # get criteria
    criteria = get_criteria(criteria_file, organism)
    # run checks
    qc_report = {}  # dict to store results
    all_checks_passed = True
    for software, result in recovered_values.items():
        logging.info("Running checks for %s", software)

        # âœ… Handle Depth hybrid output (list of dicts)
        if isinstance(result, list):
            for entry in result:
                read_type = entry.get("Read_type", "").lower()
                for res_name, res_value in entry.items():
                    if res_name in ("Read_type", "Sample_id"):
                        continue
                    col_name = f"{software}.{read_type}.{res_name}"
                    qc_report[col_name] = res_value
        else:
            for res_name, res_value in result.items():
                col_name = software + "." + res_name
                qc_report[col_name] = res_value

        all_fields_passed = True
        for field in criteria:
            if field["software"] == software:
                if field["field"] in result:
                    col_name = field["software"] + "." + field["field"] + ".check"
                    test_result = check_criteria(field, result)
                    all_fields_passed = all_fields_passed and test_result
                    if col_name not in qc_report:
                        qc_report[col_name] = test_result
                    elif not test_result:
                        qc_report[col_name] = test_result
        qc_report[software + ".all_checks_passed"] = all_fields_passed
        all_checks_passed = all_checks_passed and all_fields_passed
    qc_report["all_checks_passed"] = all_checks_passed
    # log results
    # Write qc_report to file
    qc_report["sample_id"] = sample_id

    # Merge metadata if available for this sample
    if metadata_file and sample_id in metadata_dict:
        logging.info("Merging metadata for sample: %s", sample_id)
        for meta_key, meta_value in metadata_dict[sample_id].items():
            qc_report[meta_key] = meta_value
    elif metadata_file and sample_id not in metadata_dict:
        logging.warning("No metadata found for sample: %s", sample_id)

    logging.info("Writing results to file.")
    write_to_file(output_file, qc_report)
    logging.info("All checks completed.")


def summary(directory, output, species, sample_id, template, plot=False):
    # TODO. CHECK ALL SAMPLE NAMES ARE UNIQUE
    os.makedirs(output, exist_ok=True)
    csv_files = []
    # collect all csv files
    for root, _dirs, files in os.walk(directory):
        for file in files:
            if file.endswith(".csv"):
                csv_files.append(os.path.join(root, file))
    # merge all csv files in a single dictionary
    # TODO: Need to check all sample ids are unique, and sample_id column exists.
    merged_data = {}
    for file in csv_files:
        df = pd.read_csv(file)
        df.set_index(sample_id, inplace=True)
        merged_data.update(df.to_dict(orient="index"))
    # check if the sample field is present in the data
    if not merged_data:
        logging.error("No data found in the merged files.")
        return
    if any(pd.isna(sample_id) for sample_id in merged_data.keys()):
        logging.error("Sample names not found in the data.")
        return
    logging.info("Merged data for %d samples from %d files", len(merged_data), len(csv_files))
    # write merged data to a csv file
    output_file = os.path.join(output, "report.csv")
    if plot:
        plot_dict = merged_data.copy()

    # Collect all unique fieldnames
    all_fieldnames = set()
    for values in merged_data.values():
        all_fieldnames.update(values.keys())

    # Sort fieldnames: sample_id first, then .check columns, then alphabetically
    check_columns = sorted([col for col in all_fieldnames if col.endswith(".check")])
    other_columns = sorted([col for col in all_fieldnames if not col.endswith(".check")])
    fieldnames = ["sample_id"] + check_columns + other_columns

    # Sort merged_data by sample_id keys
    sorted_sample_ids = sorted(merged_data.keys())

    with open(output_file, "w", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        for sample_id_key in sorted_sample_ids:
            values = merged_data[sample_id_key]
            row = {"sample_id": sample_id_key}
            row.update(values)
            writer.writerow(row)
            if plot:
                plot_dict[sample_id_key]["sample_id"] = sample_id_key
    # run plotting for each software (if available)
    if plot:
        plot_charts(
            plot_dict,
            species,
            output_html_path=os.path.join(output, "report.html"),
            input_template_path=template,
        )
        shutil.copy(
            os.path.join(os.path.dirname(template), "bulma.css"), os.path.join(output, "bulma.css")
        )
        logging.info("Plots generated.")


def check(
    criteria_file,
    update=False,
    update_url="https://raw.githubusercontent.com/happykhan/genomeqc/refs/heads/main/docs/summary/filtered_metrics.csv",
):
    logging.info("Checking criteria file: %s", criteria_file)
    # Check criteria file if it has all the required fields
    # Use the 'all' species to template which fields are required
    errors = []
    warnings = []
    # if update is True, download the latest criteria file
    if update:
        update_criteria_file(criteria_file, update_url)
        logging.info("Updated criteria file from %s", update_url)
    # Check its a valid csv file
    if not os.path.isfile(criteria_file):
        logging.error("Criteria file not found: %s", criteria_file)
        return

    # check if the file is a valid csv file
    if not criteria_file.endswith(".csv"):
        errors.append("Criteria file is not a valid csv file.")

    # check if the file has the required fields
    columns = [
        "assembly_type",
        "software",
        "field",
        "operator",
        "value",
        "species",
        "special_field",
    ]
    with open(criteria_file, encoding="utf-8") as f:
        reader = csv.reader(f)
        header = next(reader)
        for column in columns:
            if column not in header:
                errors.append(f"Missing required column: {column}")

    with open(criteria_file, encoding="utf-8") as f:
        criteria = csv.DictReader(f)
        required = {}
        species_rules = {}
        for row in criteria:
            required_name = row["assembly_type"] + "." + row["software"] + "." + row["field"]
            if row["species"] == "all":
                required[required_name] = {
                    "operator": row["operator"],
                    "value": row["value"],
                    "special_field": row["special_field"],
                }
            else:
                if row["species"] in species_rules:
                    species_rules[row["species"]].append(
                        {
                            required_name: {
                                "operator": row["operator"],
                                "value": row["value"],
                                "special_field": row["special_field"],
                            }
                        }
                    )
                else:
                    species_rules[row["species"]] = [
                        {
                            required_name: {
                                "operator": row["operator"],
                                "value": row["value"],
                                "special_field": row["special_field"],
                            }
                        }
                    ]

        for species, rules in species_rules.items():
            for field, rule in required.items():
                if field not in [list(x.keys())[0] for x in rules]:
                    errors.append(
                        f"Required field {field} not found for species {species}. 'all' value is {rule['operator']} {rule['value']} {rule['special_field']}"
                    )

    if not required:
        errors.append("No criteria found for species 'all'.")
    if warnings:
        for warning in warnings:
            logging.warning(warning)
    if errors:
        for error in errors:
            logging.error(error)
    if not errors or warnings:
        logging.info("Criteria file is valid.")
