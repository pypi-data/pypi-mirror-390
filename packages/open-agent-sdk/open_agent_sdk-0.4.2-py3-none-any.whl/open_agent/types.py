"""
Type definitions for Open Agent SDK

This module defines all core data structures used throughout the SDK:
- Message content blocks (TextBlock, ToolUseBlock, etc.)
- Configuration (AgentOptions)
- Message containers (AssistantMessage)

These types are inspired by the Claude SDK message format for familiarity,
but adapted to work with OpenAI-compatible local LLM endpoints.

Design Philosophy:
- Use dataclasses for simplicity and automatic __init__/__repr__
- Use Literal types for discriminated unions (type field)
- Keep types simple and serializable
- Avoid complex inheritance hierarchies
"""

from dataclasses import dataclass, field
from typing import Literal, TYPE_CHECKING, Any

# TYPE_CHECKING is only True during static type checking (mypy, pyright, etc.)
# This allows forward references to avoid circular imports at runtime
# while still providing proper type hints for development tools
if TYPE_CHECKING:
    from .tools import Tool  # Only imported for type hints
    from .hooks import HookHandler  # Only imported for type hints


@dataclass
class TextBlock:
    """
    Represents text content generated by the assistant.

    This is the most common content block type, containing natural language
    responses from the LLM. Each TextBlock represents a continuous chunk of
    text in the assistant's response.

    Attributes:
        text: The actual text content generated by the assistant
        type: Literal discriminator set to "text" for type checking and serialization

    Example:
        >>> block = TextBlock(text="Hello! How can I help you?")
        >>> block.text
        'Hello! How can I help you?'

    Note:
        Multiple TextBlocks may appear in a single AssistantMessage if the model
        generates text both before and after tool calls.
    """
    text: str  # The generated text content
    type: Literal["text"] = "text"  # Type discriminator for union types


@dataclass
class ToolUseBlock:
    """
    Represents a tool call request from the assistant.

    When the LLM decides to use a tool (function call), it generates a ToolUseBlock
    containing the tool name, arguments, and a unique ID. This block is yielded to
    the user, who can then execute the tool and provide results back via ToolResultBlock.

    Attributes:
        id: Unique identifier for this tool call (generated by the LLM)
            Format is provider-dependent (e.g., "call_abc123" for OpenAI-compatible)
        name: Name of the tool to execute (must match a tool defined in AgentOptions.tools)
        input: Dictionary of arguments to pass to the tool function
               Keys correspond to the parameter names in the tool's schema
        type: Literal discriminator set to "tool_use" for type checking

    Example:
        >>> block = ToolUseBlock(
        ...     id="call_123",
        ...     name="get_weather",
        ...     input={"city": "San Francisco", "units": "celsius"}
        ... )
        >>> block.name
        'get_weather'
        >>> block.input["city"]
        'San Francisco'

    Usage Flow:
        1. LLM generates ToolUseBlock
        2. SDK yields block to user code
        3. User executes tool with block.input
        4. User creates ToolResultBlock with block.id as tool_use_id
        5. User sends ToolResultBlock back to continue conversation
    """
    id: str  # Unique tool call identifier (e.g., "call_abc123")
    name: str  # Tool function name (e.g., "get_weather")
    input: dict  # Arguments dictionary (e.g., {"city": "NYC", "units": "celsius"})
    type: Literal["tool_use"] = "tool_use"  # Type discriminator


@dataclass
class ToolUseError:
    """
    Represents a malformed or unparseable tool call from the assistant.

    This error type is yielded when the LLM attempts to make a tool call but:
    - The JSON arguments are malformed/unparseable
    - The tool call format doesn't match OpenAI function calling spec
    - The streaming chunks couldn't be properly aggregated

    ToolUseError is different from tool execution errors (which use ToolResultBlock
    with is_error=True). This represents parsing/format errors, not runtime errors.

    Attributes:
        error: Human-readable error message describing what went wrong
        raw_data: Optional raw JSON/text that failed to parse (for debugging)
        type: Literal discriminator set to "tool_use_error"

    Example:
        >>> block = ToolUseError(
        ...     error="Invalid JSON in tool arguments",
        ...     raw_data='{"city": "NYC", incomplete...'
        ... )
        >>> block.error
        'Invalid JSON in tool arguments'

    Note:
        When auto_execute_tools=True, ToolUseErrors are automatically sent back
        to the LLM with the error message, giving it a chance to retry with
        correct formatting.
    """
    error: str  # Description of what went wrong during parsing
    raw_data: str | None = None  # Raw unparseable data for debugging (optional)
    type: Literal["tool_use_error"] = "tool_use_error"  # Type discriminator


@dataclass
class ToolResultBlock:
    """
    Represents the result of executing a tool, sent back to the assistant.

    After the user receives a ToolUseBlock and executes the corresponding tool,
    they create a ToolResultBlock to send the results back to the LLM. This
    allows the conversation to continue with the tool's output in context.

    Attributes:
        tool_use_id: Must match the 'id' field from the corresponding ToolUseBlock
                     This links the result back to the original tool call request
        content: The tool's output, can be:
                 - str: Simple text result (most common)
                 - dict: Structured data (will be JSON-serialized)
                 - list: Array of results (will be JSON-serialized)
        is_error: If True, indicates tool execution failed
                  The LLM will see this as an error and may retry or adjust strategy
        type: Literal discriminator set to "tool_result"

    Examples:
        Success case:
        >>> block = ToolResultBlock(
        ...     tool_use_id="call_123",
        ...     content="The weather in San Francisco is 18Â°C and sunny",
        ...     is_error=False
        ... )

        Error case:
        >>> block = ToolResultBlock(
        ...     tool_use_id="call_456",
        ...     content="API key invalid",
        ...     is_error=True
        ... )

        Structured data:
        >>> block = ToolResultBlock(
        ...     tool_use_id="call_789",
        ...     content={"temp": 18, "conditions": "sunny", "humidity": 65}
        ... )

    Note:
        When auto_execute_tools=True, the SDK automatically creates ToolResultBlocks
        from tool execution and sends them back to the LLM without user intervention.
    """
    tool_use_id: str  # Must match ToolUseBlock.id this result corresponds to
    content: str | dict[str, Any] | list[Any]  # Tool output (str/dict/list)
    is_error: bool = False  # True if tool execution failed
    type: Literal["tool_result"] = "tool_result"  # Type discriminator


@dataclass
class AssistantMessage:
    """
    Container for a complete assistant response with all content blocks.

    Represents one turn in the conversation from the assistant's perspective.
    A single AssistantMessage can contain multiple content blocks of different types:
    - Zero or more TextBlocks (text before/after/between tool calls)
    - Zero or more ToolUseBlocks (tool call requests)
    - Zero or more ToolUseErrors (malformed tool calls)
    - Zero or more ToolResultBlocks (only when manually added to history)

    This structure mirrors the Claude SDK's message format, making it familiar
    to developers who have used Claude, while being compatible with OpenAI's
    streaming format.

    Attributes:
        role: Always "assistant" for messages from the LLM
        content: List of content blocks in the order they were generated

    Example:
        >>> msg = AssistantMessage(
        ...     role="assistant",
        ...     content=[
        ...         TextBlock(text="I'll check the weather for you."),
        ...         ToolUseBlock(id="call_1", name="get_weather", input={"city": "NYC"}),
        ...     ]
        ... )
        >>> len(msg.content)
        2
        >>> msg.content[0].text
        "I'll check the weather for you."

    Usage:
        - Yielded from query() and Client.receive_messages()
        - Added to message history for context in multi-turn conversations
        - Can be manually constructed for testing or pre-seeding conversations

    Note:
        The content list is ordered - blocks appear in the sequence they were
        generated during streaming. Text usually appears before tool calls,
        but the LLM can interleave them.
    """
    role: Literal["assistant"] = "assistant"  # Always "assistant" for LLM messages
    # List of content blocks (TextBlock | ToolUseBlock | etc.) in generation order
    content: list[TextBlock | ToolUseBlock | ToolUseError | ToolResultBlock] = field(default_factory=list)


@dataclass
class AgentOptions:
    """
    Configuration dataclass for agent behavior and LLM connection settings.

    This is the main configuration object used by both query() and Client classes.
    It encapsulates all settings needed to connect to a local LLM and control
    agent behavior (tools, hooks, execution modes, sampling parameters, etc.).

    Design Philosophy:
    - Minimal required fields (system_prompt, model, base_url)
    - Sensible defaults for optional fields
    - Explicit is better than implicit (no magic environment variable reading here)
    - Use config.py helpers in your agent code for env var/provider shortcuts

    Required Attributes:
        system_prompt (str): System-level instructions for the LLM that define
            its role, behavior, and capabilities. This sets the context for all
            interactions and is prepended to every conversation.
            Example: "You are a helpful assistant that answers questions concisely."

        model (str): Name/identifier of the model to use. The exact value depends
            on your LLM server (e.g., "qwen2.5-32b-instruct" for Ollama,
            "local-model" for LM Studio). Check your server's model list endpoint.

        base_url (str): Full URL to the OpenAI-compatible API endpoint, including
            the /v1 suffix. Must start with http:// or https://.
            Examples:
            - "http://localhost:1234/v1" (LM Studio)
            - "http://localhost:11434/v1" (Ollama)
            - "http://localhost:8080/v1" (llama.cpp server)

    Tool System Attributes:
        tools (list[Tool]): List of Tool instances the agent can call. Tools are
            defined using the @tool decorator. Empty by default (no function calling).
            Example: [get_weather, calculate, search_docs]

        auto_execute_tools (bool): If True, automatically execute tools and send
            results back to the LLM without user intervention. If False (default),
            ToolUseBlocks are yielded to the user for manual execution.
            Default: False for backward compatibility and safety.
            Security Note: Only enable auto-execution with trusted tools.

        max_tool_iterations (int): Maximum number of tool execution loops to prevent
            infinite agent loops. Applies only when auto_execute_tools=True.
            The agent can make multiple rounds of tool calls (e.g., call search_docs,
            then call summarize), but will stop after this many iterations.
            Default: 5 iterations.

    Hooks System Attributes:
        hooks (dict[str, list[HookHandler]] | None): Dictionary mapping hook event
            names to lists of handler functions. Hooks provide lifecycle control:
            - HOOK_PRE_TOOL_USE: Before tool execution (can block/modify)
            - HOOK_POST_TOOL_USE: After tool execution (can modify results)
            - HOOK_USER_PROMPT_SUBMIT: Before processing user input (can sanitize)
            Example:
                {
                    HOOK_PRE_TOOL_USE: [security_check, audit_logger],
                    HOOK_POST_TOOL_USE: [result_sanitizer]
                }
            Default: None (no hooks)

    Conversation Control Attributes:
        max_turns (int): Maximum number of conversation turns (user + assistant pairs).
            Used by the Client class to limit conversation length. For query(),
            this is typically 1 since it's single-turn.
            Default: 1

    Sampling/Generation Attributes:
        max_tokens (int | None): Maximum number of tokens to generate in a single
            response. If None, uses the provider's default (usually unlimited or
            model context limit). Setting this helps control response length and costs.
            Default: 4096
            Note: This is max_tokens, not context window size.

        temperature (float): Sampling temperature for generation (0.0 to 2.0+).
            - 0.0: Deterministic, always picks highest probability token
            - 0.7: Balanced creativity and consistency (default)
            - 1.0+: More random and creative
            Default: 0.7

    Connection Attributes:
        timeout (float): HTTP request timeout in seconds. If the LLM doesn't respond
            within this time, the request will be cancelled with a timeout error.
            Useful for preventing indefinite hangs.
            Default: 60.0 seconds

        api_key (str): API key for authentication. Most local LLM servers don't
            require authentication, so this defaults to a placeholder value.
            Some servers may require a key for multi-user setups.
            Default: "not-needed"

    Examples:
        Basic usage (required fields only):
        >>> opts = AgentOptions(
        ...     system_prompt="You are a helpful coding assistant.",
        ...     model="qwen2.5-32b-instruct",
        ...     base_url="http://localhost:1234/v1"
        ... )

        With tools and auto-execution:
        >>> from open_agent import tool
        >>> @tool
        ... def add(a: int, b: int) -> int:
        ...     return a + b
        >>> opts = AgentOptions(
        ...     system_prompt="You are a math assistant.",
        ...     model="qwen2.5-32b-instruct",
        ...     base_url="http://localhost:1234/v1",
        ...     tools=[add],
        ...     auto_execute_tools=True,
        ...     max_tool_iterations=10
        ... )

        With configuration helpers (recommended):
        >>> from open_agent.config import get_base_url, get_model
        >>> opts = AgentOptions(
        ...     system_prompt="You are a helpful assistant.",
        ...     model=get_model("qwen2.5-32b"),  # Falls back to OPEN_AGENT_MODEL
        ...     base_url=get_base_url(provider="ollama")  # Uses Ollama default
        ... )

        With hooks for security:
        >>> from open_agent.hooks import HOOK_PRE_TOOL_USE, HookDecision
        >>> async def security_gate(event):
        ...     if event.tool_name == "dangerous_tool":
        ...         return HookDecision(continue_=False, reason="Blocked")
        ...     return None  # Or HookDecision(continue_=True)
        >>> opts = AgentOptions(
        ...     system_prompt="...",
        ...     model="qwen2.5-32b-instruct",
        ...     base_url="http://localhost:1234/v1",
        ...     tools=[safe_tool, dangerous_tool],
        ...     hooks={HOOK_PRE_TOOL_USE: [security_gate]}
        ... )

    Validation:
        The __post_init__ method validates configuration after initialization:
        - base_url must be non-empty and start with http:// or https://
        - model must be non-empty
        - Raises ValueError if validation fails
    """

    # Required configuration - must be provided by user
    system_prompt: str  # System instructions defining agent role and behavior
    model: str  # Model identifier (provider-specific, e.g., "qwen2.5-32b-instruct")
    base_url: str  # OpenAI-compatible endpoint URL (e.g., "http://localhost:1234/v1")

    # Tool system configuration
    tools: list["Tool"] = field(default_factory=list)  # Available tools (empty = no function calling)

    # Lifecycle hooks for monitoring and control (MUST be before auto_execute_tools for positional arg compatibility)
    hooks: dict[str, list["HookHandler"]] | None = None  # Event name -> handler list mapping

    # Tool execution control
    auto_execute_tools: bool = False  # Auto-execute tools (False = manual execution required)
    max_tool_iterations: int = 5  # Max tool call loops to prevent infinite recursion

    # Conversation control
    max_turns: int = 1  # Maximum conversation turns (user + assistant pairs)

    # Generation/sampling parameters
    max_tokens: int | None = 4096  # Max tokens to generate (None = provider default)
    temperature: float = 0.7  # Sampling temperature (0.0 = deterministic, 1.0+ = creative)

    # Connection settings
    timeout: float = 60.0  # HTTP request timeout in seconds
    api_key: str = "not-needed"  # API key (most local servers don't need this)

    def __post_init__(self):
        """
        Validate configuration after dataclass initialization.

        This method is automatically called by the dataclass after __init__.
        It performs basic validation to catch common configuration errors early,
        before making any API calls to the LLM server.

        Validations:
            - base_url is non-empty
            - base_url starts with http:// or https:// (catches typos like "localhost:1234")
            - model is non-empty (catches forgotten model names)

        Raises:
            ValueError: If any validation check fails, with descriptive error message

        Note:
            This does NOT validate that the URL is reachable or that the model exists.
            Those checks happen at request time when the AsyncOpenAI client makes calls.
            This only validates basic format/presence to fail fast on obvious errors.
        """
        # Validate base_url is provided
        if not self.base_url:
            raise ValueError("base_url cannot be empty")

        # Validate base_url has correct protocol
        # Common mistake: "localhost:1234/v1" instead of "http://localhost:1234/v1"
        if not (self.base_url.startswith("http://") or self.base_url.startswith("https://")):
            raise ValueError(f"base_url must start with http:// or https://, got: {self.base_url}")

        # Validate model name is provided
        if not self.model:
            raise ValueError("model cannot be empty")
