תכנון חבילת yemot-ai לחיבור סוכני AI למערכות ימות המשיח
הבנת ספריית yemot-flow והגדרת זרימת IVR
yemot-flow היא ספריית Python ליצירת מערכות IVR עבור פלטפורמת ימות המשיח, שמאפשרת כתיבת לוגיקת שיחה בצורה פשוטה עם תמיכה ב-async/await. הספרייה מנהלת את מצב השיחה בין קריאות HTTP עוקבות, כך שהקוד נראה רציף (סינכרוני) למרות שהוא מטופל בצורה אסינכרונית מאחורי הקלעים[1][2].
כדי להתחיל, מייצרים אובייקט Flow ומגדירים Handlers לנתבי השיחה באמצעות דקורטורים. למשל, ניתן להגדיר נתב לשלוחה הראשית כך:
from yemot_flow import Flowflow = Flow(timeout=30000, print_log=True)@flow.get("")  # שלוחה ראשיתdef welcome(call):    call.play_message([("text", "שלום וברוך הבא! מה ברצונך לעשות?")])    call.read([("text", "הקלד 1 לדבר עם סוכן AI או 2 לצאת")], max_digits=1)    if call.params.get("Digits") == "1":        call.goto("/ai_agent")  # מעבר לשלוחה שמטפלת בסוכן ה-AI
בדוגמה זו, call.play_message משמיע טקסט למתקשר, ו-call.read קורא קלט מהמשתמש (כאן ספרה אחת)[2]. אם המשתמש בחר באפשרות המתאימה, אנחנו מנתבים את השיחה לשלוחה ייעודית (למשל /ai_agent) שתטפל בחיבור ל-AI. הספרייה שומרת אוטומטית את מצב השיחה ב-Python בין הבקשות: מזהה השיחה (הפרמטר ApiCallId מימות) משמש כמפתח לזיהוי השיחה בתוך אובייקט active_calls בזיכרון[3]. המשמעות היא שנוכל לאחסן מידע מותאם לכל שיחת טלפון ולשלוף אותו בשלבים הבאים של אותה שיחה.
טיפול ב-async/await: הפונקציות המטפלות (Handlers) יכולות להיות כתובות כסינכרוניות (כמו בדוגמה למעלה) מכיוון ש-yemot-flow מטפל בפיצול השיחה לאירועים אסינכרוניים מאחורי הקלעים. אם עלינו לקרוא לפעולות חיצוניות איטיות (כמו קריאת API של AI), ניתן לעשות זאת באופן סינכרוני בתוך ה-Handler, והספרייה תמתין עד להשלמת הפעולה לפני שתמשיך לצעד הבא בשיחה. למשל, בקריאה למודל AI ייתכן שנרצה להמתין 2-3 שניות לתשובה – זה אפשרי בתוך ה-Handler, אך חשוב לזכור שזה יחסום את בקשת ה-HTTP עד לקבלת תשובה. בסביבת FastAPI או שרת אסינכרוני, ניתן לשלב את הקריאה ל-AI עם loop אסינכרוני (למשל בעזרת await או asyncio.to_thread) כדי לא לחסום את ה-loop הראשי. בפועל, לפשטות ההתחלתית, אפשר לקרוא למודל ה-AI בצורה סינכרונית בתוך הפונקציה המטפלת (כפי שנראה בהמשך עם הרצת subprocess), מאחר וכל שיחה מטופלת בנפרד. רק יש לוודא שתוחם זמן (timeout) השיחה מספיק ארוך (ב-Flow מוגדר כברירת מחדל 30 שניות או בהתאמה ידנית) כדי להכיל את זמן ההמתנה[3].
ניהול סשנים ב-Codex CLI לצורך שיחה רציפה
Codex CLI של OpenAI מאפשר לנו לנהל שיחה מתמשכת עם זיכרון ללא צורך להעביר את ההיסטוריה בכל פנייה. הרעיון המרכזי הוא לנצל את פקודת codex exec לפתיחת שיחה חדשה, ופקודת codex exec resume להמשכה. כך המודל שומר הקשר (context) של השיחה בתוך השרת של Codex, ואיננו צריכים לשמור ולשלוח ידנית את כל ההיסטוריה בכל שאילתה.
יצירת סשן חדש: כאשר מתחילה שיחת טלפון וצריך לשלוח פנייה ראשונה ל-AI, נריץ את הפקודה:
codex exec "<<<הודעת המשתמש הראשונית>>>"
במצב לא אינטראקטיבי (exec) Codex יריץ את המשימה וידפיס לתוצאה רק את הודעת הסיום של הסוכן[4]. עם סיום הריצה, Codex שומר את השיחה כ"סשן" שניתן לחידוש. יש לקבל את Session ID הייחודי של השיחה הזו כדי להשתמש בו לפניות הבאות. בגירסאות עדכניות, הפלט הסטנדרטי (stdout) מחזיר רק את תשובת ה-AI, ללא Session ID, ולכן נשתמש במצב JSON של Codex כדי לדלות את ה-ID:
הפעלת codex exec עם דגל --json תגרום לכך שכל האירועים של הסשן יודפסו כ-JSONL (שורת JSON לכל אירוע) ל-stdout[5]. בין האירועים, האירוע הראשון בדרך כלל הוא thread.started המכיל מזהה ייחודי thread_id – זהו ה-Session ID של השיחה[6]. בהמשך הזרימה יופיע גם אירוע של הודעת הסוכן, למשל item.completed עם type":"agent_message" וטקסט התשובה[6]. נוכל לנתח את פלט ה-JSON כדי לחלץ את ה-session_id ואת תשובת ה-AI. לדוגמה:
result = subprocess.run(    ["codex", "exec", "--json", user_input_text],    capture_output=True, text=True)session_id = Noneanswer = Nonefor line in result.stdout.splitlines():    event = json.loads(line)    if event.get("type") == "thread.started":        session_id = event.get("thread_id")        # מזהה הסשן    if event.get("type") == "item.completed" and event.get("item", {}).get("type") == "agent_message":        answer = event["item"]["text"]            # טקסט תשובת ה-AI
כעת יש בידינו את תשובת ה-AI (answer) להודעת המשתמש הראשונה, וכן את מזהה הסשן (session_id). את התשובה נשמיע למתקשר באמצעות call.play_message([("text", answer)]), ואת ה-session_id נשמור במבנה נתונים כדי להשתמש בו בהמשך השיחה.
שמירת Session ID לכל שיחה: הפתרון הפשוט והמהיר הוא להשתמש במבנה נתונים מקומי – למשל, מילון (dictionary) גלובלי או מחלקת Singleton – למיפוי בין מזהה השיחה הטלפונית (ApiCallId) לבין Session ID של Codex. לדוגמה:
sessions = {}  # מפה בין ApiCallId (מפתח) לסשן ID של Codex (ערך)# לאחר יצירת סשן חדש:sessions[call_id] = session_id
כאמור, ניתן להתחיל עם פתרון בזיכרון או בקובץ JSON מקומי, ולשדרג בעתיד ל-Redis או אחסון אחר אם יהיה צורך בשיתוף נתונים בין תהליכים. כל עוד האפליקציה רצה בתהליך יחיד, מילון גלובלי יספיק בשלב הראשון. יש לזכור לנקות את הרשומה בסיום השיחה (למשל, בעת ניתוק השיחה) כדי לא לצבור מידע מיותר – ניגע בכך בהמשך.
המשך שיחה קיימת: עבור כל אינטראקציה נוספת בתוך אותה שיחת טלפון, נרצה להמשיך את ההקשר ב-Codex מבלי לשלוח היסטוריה. אם למתקשר יש שאלה/אמירה נוספת (נניח אחרי ששמע תשובה ורוצה להמשיך את השיחה), נחפש את ה-Session ID ששמרנו עבור אותה שיחה, ונקרא לפקודה:
codex exec resume <SESSION_ID> "<הודעת המשתמש החדשה>"
כאן אנחנו מנצלים את פקודת ה-resume של Codex בצורה לא-אינטראקטיבית: היא מאפשרת להזין מיד הודעה חדשה ולצאת לאחר קבלת התשובה[7]. כך Codex יטען את ההיסטוריה של הסשן שזיהויו סופק, יוסיף את ההודעה החדשה כצעד נוסף, ויגיב בהתאם להקשר המצטבר. הדוגמה הבאה ממחישה את השימוש בחידוש שיחה באופן לא-אינטראקטיבי:
codex exec resume --last "Fix the race conditions you found"
בדוגמה מתועדת זו, הפקודה codex exec resume --last ממשיכה את השיחה האחרונה שנוהלה, ומוסיפה לה את הפרומפט "Fix the race conditions you found"[8]. בדומה, באפשרותנו לציין מזהה סשן ספציפי במקום --last אם נרצה (למשל codex exec resume 0199a213-81c0-7800-8aa1-bbab2a035a53 "Next user prompt"). הפקודה resume מקבלת כפרמטר את הטקסט החדש לשליחה, כך שהיא אינה נכנסת למצב TUI אינטראקטיבי כלל, אלא שולחת את הפרומפט ומחזירה תשובה מיד[7]. זה בדיוק המפתח להתגברות על הבעיה שהזכרת – הרצת codex resume לבדו תקפיא את התהליך כי הוא ממתין לקלט אינטראקטיבי, אבל הרצת codex exec resume <session> "<prompt>" בתוך subprocess תריץ את השיחה ותסתיים אוטומטית עם הפלט.
מבחינת מימוש בקוד, נשתמש שוב ב-Python subprocess. הפעם אפשר לא להשתמש ב---json (כי כבר יש לנו את ה-Session ID, ואנחנו מעוניינים רק בתשובה הסופית). מאחר ו-Codex מדפיס את תשובת הסוכן הסופית ל-stdout, אפשר לקרוא אותה ישירות. לדוגמה:
session_id = sessions[call_id]result = subprocess.run(    ["codex", "exec", "resume", session_id, user_new_input],    capture_output=True, text=True)answer = result.stdout.strip()
כאן אנחנו מפיקים את התשובה (רצוי לקרוא ל-strip() כדי להסיר תווי newline מיותרים). לאחר מכן משמיעים את answer למשתמש. Codex CLI דואג אוטומטית לכך שהתשובה נגזרת מכל ההקשר המצטבר של השיחה הנוכחית בשרשרת הפניות, ללא צורך שנשלח את ההיסטוריה – זו בדיוק המטרה בשימוש ב-resume. למעשה, בגישה זו חסכנו לחלוטין ניהול ידני של ההקשר: אין צורך לצרף לכל שאילתא את כל היסטוריית השיחה, כמו שהיינו עושים למשל בשימוש ישיר ב-OpenAI API. היתרון ברור כאשר מדובר בשיחה קולית מתמשכת שבה כל שאילתא יכולה להיות משפט ארוך – Codex CLI זוכר עבורנו. (ראוי לציין ש-Codex CLI כולל גם מנגנון של "דחיסת הקשר" אוטומטית כשהשיחה מתארכת ומתקרבת למגבלת החלון, שמייצרת סיכום אחורי של תחילת השיחה, אבל זה מעבר להיקף הדיון שלנו).
ניהול תקלות וחריגות: יש לוודא שטיפול השגיאות מתבצע כך שאם הקריאה ל-Codex נכשלת (למשל בעיית רשת או חריגה כלשהי), השיחה לא תתקע. ניתן לתפוס Exceptions מ-subprocess (או לבדוק result.returncode) ובמקרה של כשל – לנקוט פעולה, כגון השמעת הודעת שגיאה למשתמש או ניסיון חוזר. כמו כן, חשוב להגן על הקריאה ל-Codex מפני קלט לא צפוי. בגירסאות מתקדמות ניתן להגביל את כלי Codex (כגון מנוע פקודות shell) באמצעות הגדרת sandbox מתאים (--sandbox read-only כברירת מחדל, מה שמונע שינויים במערכת). אם סוכן ה-AI משמש כשיחה כללית, ייתכן שנרצה לקבוע הנחיות (prompt) שיגבילו אותו מלנסות להריץ קוד או חיפוש אינטרנט, אך זה נושא של הנחיית המודל ולא של ניהול הסשנים.
ארכיטקטורת מודול yemot-ai – עטיפה ל-yemot-flow ו-Codex/API
כעת, לאחר שהבנו את חלקי הבסיס, נוכל לתכנן את חבילת yemot-ai. המטרה היא לאפשר למפתח להשתמש ביכולת הזו בקלות, מבלי לדאוג לניהול ה-Session IDs או להמשכיות השיחה – הכל ינוהל אוטומטית ע"י המודול.
ארכיטקטורת המודול יכולה להתחלק למרכיבים הבאים:
מנהל סשנים (Session Manager): רכיב שאחראי לשמור ולשלוף את מזהי הסשנים של Codex בהתאמה לכל שיחת טלפון (ApiCallId). אפשר לממש זאת כמחלקה עם מילון פנימי, או אפילו ברמת המודול כמשתנה גלובלי. בשלב ראשון: מילון בזיכרון או קובץ JSON מקומי. לדוגמה, מילון sessions כפי שהראינו. בעתיד, המימוש יכול להיות אינטרפייס אחסון שניתן להחלפה (Memory/JSON/Redis), אבל נתחיל פשוט.
ספק AI (AI Provider) מופשט: המודול יתמוך בכמה דרכי תקשורת עם ה-AI – למשל OpenAI/Anthropic API מול כלי CLI. כדאי לתכנן מחלקה או ממשק עם מתודות כגון start_session(prompt) ו-continue_session(session_id, prompt) שמבצעות את הקריאות ומחזירות תשובה. לאחר מכן נממש שתי וריאציות עיקריות:
CodexCLIProvider – משתמש ב-Codex CLI (כפי שפירטנו לעיל) למימוש המתודות.
OpenAIProvider (או AnthropicProvider) – משתמש בקריאות HTTP ל-API הרלוונטי.
כך, למשל, מחלקת CodexCLIProvider תכיל בערך את הקוד שכתבנו מקודם:
class CodexCLIProvider:    def __init__(self):        self.sessions = {}  # מילון לניהול סשנים (חלופי/מורחב ל-session manager חיצוני)    def start_session(self, call_id, user_text):        # הרצת codex exec --json וניתוח ה-ID והתשובה        result = subprocess.run([... "--json", user_text], capture_output=True, text=True)        ...  # (ניתוח JSON כמתואר לעיל)        self.sessions[call_id] = session_id        return answer    def continue_session(self, call_id, user_text):        session_id = self.sessions.get(call_id)        if not session_id:            # לשם זהירות: אם אין סשן קיים, אולי להתחיל חדש            return self.start_session(call_id, user_text)        result = subprocess.run([... "resume", session_id, user_text], capture_output=True, text=True)        return result.stdout.strip()
במקביל, מחלקת OpenAIProvider תעבוד אחרת: במקום Session ID, היא יכולה לשמור את היסטוריית השיחה (רשימת הודעות) במילון. לדוגמה:
class OpenAIProvider:    def __init__(self, api_key):        openai.api_key = api_key        self.conversations = {}    def start_session(self, call_id, user_text):        # יוצרים היסטוריה התחלתית עם הודעת המשתמש        self.conversations[call_id] = [ {"role": "user", "content": user_text} ]        response = openai.ChatCompletion.create(            model="gpt-3.5-turbo",            messages=self.conversations[call_id]        )        answer = response.choices[0].message.content        # שומרים את תשובת הבוט בהיסטוריה        self.conversations[call_id].append({"role": "assistant", "content": answer})        return answer    def continue_session(self, call_id, user_text):        if call_id not in self.conversations:            return self.start_session(call_id, user_text)        # מעדכנים היסטוריה        convo = self.conversations[call_id]        convo.append({"role": "user", "content": user_text})        response = openai.ChatCompletion.create(model="gpt-3.5-turbo", messages=convo)        answer = response.choices[0].message.content        convo.append({"role": "assistant", "content": answer})        return answer
כך, מפתח הספרייה יכול לבחור את ספק ה-AI הרצוי – CLI או API – והלוגיקה הגבוהה לא משתנה.
Wrapper ל-yemot-flow (שכבת אינטגרציה): כאן הרעיון הוא להקל על השימוש על ידי שילוב מנגנון ה-AI בתוך ה-Flow של Yemot. יש מספר דרכים לעשות זאת:
דרך מינימלית: פונקציה/מתודה שהמפתח יוכל לקרוא מתוך ה-Handler שלו כדי לקבל תשובה מ-AI. למשל: answer = ai_manager.get_answer(call, user_text). פונקציה זו תטפל בבחירת הספק (CLI/API) בהתאם לקונפיגורציה, בניהול ה-session_id וכו'.
דרך מתקדמת יותר: להרחיב את המחלקה Call או להוסיף לה מתודות. למשל, ניתן לדמיין פונקציה כמו call.ask_ai(user_text) שתבצע את מה שתיארנו (אולי ע"י קריאה פנימית ל-ai_manager). כך הקוד בתוך ה-Handler יהיה נקי מאוד:
@flow.get("ai_agent")def ai_handler(call):    user_text = call.params.get("RecordingText")  # נניח שכאן יש טקסט מהמתקשר (לא DTMF)    answer = call.ask_ai(user_text)  # מתודת wrapper שסיפקנו    call.play_message([("text", answer)])    call.read(...לקליטת הודעה נוספת...)
המתודה ask_ai יכולה "מאחורי הקלעים" לבדוק אם ל-Call הנוכחי כבר קשור Session ID (נניח לשמור את זה כ- call.ai_session_id באובייקט השיחה), ואז לפנות בהתאם לספק ה-AI. מכיוון שאובייקט Call ב-yemot-flow מחזיק state, אפשר לצרף לו שדות חדשים דינמית בפייתון (למשל call.ai_session = ...) – אלו יישמרו בזיכרון כל עוד השיחה פעילה ב-active_calls. כך אפשר שלא להסתמך רק על מילון גלובלי חיצוני, אלא ממש להצמיד את ה-session ID אל אובייקט השיחה. זה משפר את הקפסולציה (אך עדיין ייתכן שנרצה גם מילון גלובלי לניהול כולל, תלוי במימוש).
דרך אחרת היא להגדיר מחלקת Flow מורחבת (למשל AIFlow(Flow)) שתדע לטפל אוטומטית בשלוחה מסוימת כשיחת AI. אפשר לממש דקורטור מיוחד, למשל:
@flow.ai_agent("/ai_agent", provider="codex")def handle_ai(call, text):    # אולי לא צריך לוגיקה פנימית בכלל - המודול ידאג לקרוא ל-AI ולהחזיר תשובה    return "טקסט ברירת מחדל כלשהו"  # (או פשוט pass, כי המודול יטפל)
הרעיון הוא שהדקורטור @flow.ai_agent ירשם כ-handler, אבל המימוש הפנימי שלו יתפוס את הבקשה, יקרא אוטומטית ל-AI (בעזרת ה-Session Manager וה-Provider), ואז ישמיע את התשובה ויחליט האם להמשיך לקרוא עוד קלט מהמשתמש או לסיים. ייתכן שהפונקציה המעוטרת לא תצטרך לבצע דבר (או רק לספק הודעת פתיחה/הנחיות למודל). זה מימוש יותר מתוחכם שדורש שהמודול יתזמר את השיחה כולה (לולאת הודעה→תגובה עד לניתוק או סיום יזום). בשלב ראשון, אפשר אולי להימנע ממימוש מלא כזה, ולהסתפק בלתת למפתח שליטה ידנית (כלומר שיקרא לפונקציה לקבלת תשובה).
נבחר בגישה הפשוטה להתחלה: נספק אובייקט Manager או פונקציה פשוטה שדרכה המפתח ישיג תשובות, ונראה דוגמה.
דוגמת קוד: שילוב הכל יחד (CLI בלבד, לפשטות) בתוך אפליקציית Flask עם yemot-flow:
from flask import Flask, request, Responsefrom yemot_flow import Flowimport subprocess, jsonapp = Flask(__name__)flow = Flow()sessions = {}  # מיפוי ApiCallId -> Codex Session ID# נניח שזיהוי דיבור כבר קיים ומכניס טקסט ב-RecordingText (במציאות דורש הרחבה למימוש STT)@flow.get("ai_agent")def ai_agent_handler(call):    user_text = call.params.get("RecordingText")  # הטקסט שהמשתמש אמר (לצורך דוגמה)    call_id = call.params.get("ApiCallId")    if not user_text:        # אם אין טקסט (למשל, שיחה חדשה), נשמיע הודעת הנחיה ונקרא דיבור        call.play_message([("text", "אתה כעת משוחח עם סוכן ה-AI. מה ברצונך לומר?")])        call.record(max_seconds=5, silence_timeout=2, speech_timeout=2)  # (דמיוני: הקלטה כדי לקבל RecordingText)        return  # נחכה לבקשה הבאה עם הטקסט המוקלט    # יש טקסט מהמשתמש:    if call_id in sessions:        # המשך שיחה קיימת        session_id = sessions[call_id]        result = subprocess.run(["codex", "exec", "resume", session_id, user_text],                                capture_output=True, text=True)        answer = result.stdout.strip()    else:        # התחלת שיחה חדשה עם ה-AI        result = subprocess.run(["codex", "exec", "--json", user_text],                                capture_output=True, text=True)        answer = ""        session_id = None        for line in result.stdout.splitlines():            try:                event = json.loads(line)            except json.JSONDecodeError:                continue            if event.get("type") == "thread.started":                session_id = event.get("thread_id")            if event.get("type") == "item.completed" and event.get("item", {}).get("type") == "agent_message":                answer = event["item"]["text"]        if session_id:            sessions[call_id] = session_id    # השמעת תשובת ה-AI למתקשר    call.play_message([("text", answer)])    # קריאה נוספת מהמשתמש להמשך השיחה    call.record(max_seconds=5, silence_timeout=2, speech_timeout=2)    # (נחכה לבקשה הבאה עם הטקסט החדש ונתפוס אותה ב-handler הזה עצמו, מאחר והשלוחה זהה)
בקטע הקוד הזה רואים את ניהול ה-session בפועל: - בשיחה חדשה (אין רשומה במילון), אנו מפעילים codex exec --json על טקסט המשתמש, מנתחים את ה-JSON כדי להוציא session_id ותשובה, ושומרים את ה-session. - בשיחה נמשכת (כבר יש session_id במילון עבור אותו ApiCallId), אנו מפעילים codex exec resume {session_id} "{טקסט}" ומקבלים תשובה ישירות.
לאחר שליחת תשובה למשתמש (play_message עם הטקסט שהתקבל), הקוד קורא שוב ל-record כדי לצפות לקלט נוסף ולהמשיך את הלולאה. כך, כל עוד המתקשר לא ניתק (או לחץ מקש לסיום), ההמשך הבא יחזור לאותו Handler עם RecordingText חדש, וה-session_id יימצא במילון ויאפשר לנו להמשיך את שיחת ה-AI מהיכן שהפסקנו.
ניתוק וניקוי: כאשר המשתמש מנתק את השיחה או בוחר לצאת, ימות המשיח ישלח בקשה עם hangup=yes. ניתן לנצל זאת כדי לנקות את המילון שלנו: אחרי הקריאה ל-flow.handle_request בנתיב Flask/FastAPI, לבדוק:
params = request.values.to_dict()resp_text = flow.handle_request(params)if params.get("hangup") == "yes":    sessions.pop(params.get("ApiCallId"), None)
כך אנו מסירים את ה-Session ID מהזיכרון. Codex ישמור אמנם לוג של השיחה בספריית הסשנים המקומית (~/.codex/sessions/<id>.jsonl), אך אין צורך שנשמור אותו אצלנו לאחר סיום השיחה. אם המתקשר יתקשר שוב מאוחר יותר (ApiCallId חדש), הוא יקבל סשן AI חדש ולא המשך ישן (אלא אם במכוון נרצה לממש זיהוי לפי מספר טלפון למשל ולהמשיך שיחה קודמת – זה ידרוש להשתמש במפתח אחר מהמילון, אבל כרגע אנו מניחים שכל שיחה טלפונית היא אינטראקציה עצמאית).
מתן API פשוט למפתח והמלצות למימוש
המטרה הסופית היא שהמפתח שמשתמש בחבילת yemot-ai לא יצטרך להתעסק עם ניהול session_id או subprocesses. להלן מספר המלצות קונקרטיות:
הפרדת לוגיקה משימוש: ממצים את הקוד שכתבנו למחלקות פנימיות במודול. למשל, מחלקה _CodexCLISessionManager ומחלקה _OpenAISessionManager (בהתאם לספק). המפתח לא אמור לראות את אלו ישירות, אלא לעבוד מול ממשק נוח.
ממשק ברמת המודול: ספקו פונקציה או מחלקה ראשית, למשל YemotAI, שבה מוגדר הפרמטר האם להשתמש ב-CLI או ב-API. למשל:
ai = YemotAI(mode="codex", openai_api_key=None)
בפנים, המחלקה הזו תאתחל את הסשן מנג'ר הנכון לפי מצב.
לאחר מכן, אפשרות אחת – לאפשר למפתח לרשום Handler בצורה פשוטה:
@flow.get("assistant")def assistant(call):    # נניח שהטקסט נכנס בפרמטר 'Text'    answer = ai.reply(call.params.get("ApiCallId"), call.params.get("Text"))    call.play_message([("text", answer)])    call.goto("/continue_or_end")  # דוגמה: מעבר להמשך או סיום
כאן ai.reply() היא הממשק שניתן – הוא ידאג פנימית אם צריך לקרוא ל-start או ל-resume, בהתאם לקיום הסשן. ייתכן ש-reply יקבל גם פרמטרים כמו "פרומפט מערכת" או הגדרות תצורה, אבל בהכי פשוט הוא מקבל מזהה שיחה וטקסט משתמש ומחזיר תשובה.
דרך שנייה – כפי שהזכרנו – היא לשלב זאת ממש בתוך Flow כך שהמפתח אפילו לא יצטרך לקרוא ל-ai.reply. לדוגמה:
@flow.ai_agent("")def ai_root(call, agent):  # agent could be an object representing the AI session    pass  # The decorator's internal logic handles reading input, calling AI, etc.
אך מימוש כזה דורש להשתלט על ה-loop של קריאת הקלט והשמעת הפלט, מה שמורכב יותר. סביר שבהתחלה נעדיף את הממשק הידני הגמיש.
תמיכה ב-CLI וגם ב-API: ודאו שהממשק שתכננתם אחיד ככל הניתן. לדוגמה, YemotAI.reply(call_id, text) יעבוד בלי תלות בספק שנבחר. מאחורי הקלעים, זה יעביר את הקריאה ל-SessionManager המתאים. המשמעות למפתח – החלפה בין שימוש ב-OpenAI API לבין Codex CLI תהיה שקופה (פרט לעניין הביצועים).
ריבוי שיחות בו-זמנית: הפתרון עם מילון בזיכרון עובד גם למצב של מספר שיחות טלפון בו-זמנית, כי כל שיחה זוהתה באמצעות ApiCallId ייחודי וכל סשן Codex מנוהל בנפרד. הקריאות ל-Codex CLI (או ל-API) יכולות לרוץ במקביל (Python subprocess אינו חוסם את ה-GIL, רק ממתין לסיום תהליך חיצוני; ובמקרה של OpenAI API – הקריאות רצות במקביל כי ממתינים לתשובת HTTP). אם האפליקציה פרוסה בכמה תהליכים (workers), אז מילון בזיכרון לא משותף – במצב זה כדאי לעבור לפתרון משותף (כמו Redis). אבל בהנחה שלתהליך אחד מטפל בכל שיחה באופן מסודר, ניתן להתחיל עם מילון מקומי.
ניקוי וניטור: כפי שהצענו, הוסיפו ניקוי של סשן במפה בעת ניתוק. אפשר גם להוסיף Timeout לסשן – למשל, למחוק סשנים שלא הייתה בהם פעילות X דקות (ייתכן ש-flow כבר עושה זאת עבור הקריאה, אבל הסשן Codex יישאר בקובץ). זה יכול להתבצע ע"י Thread רקע או פשוט לסמוך על כך שסיומי שיחה ינקו.
דוגמה משלבת (API & CLI): נניח ומפתח רוצה להתחיל עם OpenAI API ואז לעבור ל-Codex CLI – כל מה שהוא ישנה זה את הגדרת YemotAI(mode="openai", openai_api_key="...") למשל, והשאר בקוד שלו יישאר אותו דבר. ודאו בתכנון שההחלפה אכן כזו חלקה.
לסיכום, חבילת yemot-ai תספק שכבה מעל Yemot Flow המנהלת את השיחה עם סוכן AI בצורה רציפה. השימוש ב-codex exec עם תת-הפקודה resume הוא המפתח לשמירת ההקשר ללא משלוח חוזר של היסטוריה[8]. באמצעות מיפוי מזהה השיחה הטלפונית ל-Session ID של Codex, אפשר לקשר כל אינטראקציה קולית לסשן מתמשך ב-AI. הקפדנו לפתור את בעיית הקריאה ל-resume ע"י שליחת הפרומפט ישירות בפקודה, כדי למנוע כניסה למצב אינטראקטיבי של ה-CLI[7]. המודול יתרפס את הפרטים הללו, כך שהמשתמש (המפתח) יוכל להתמקד בלוגיקת השיחה ולא בניהול מצבים.
עם ההנחיות והדוגמאות הנ"ל, אתה מצויד להתחיל ביישום yemot-ai. מומלץ להתחיל בקטן – מימוש CLI בסיסי כפי שתואר – ולבדוק את הזרימה מקצה לקצה (שיחת טלפון אחת עם מספר חילופי דברים). לאחר מכן, תוכל להרחיב לטיפול במקרי קצה, לשפר את ממשק ה-API למפתחים (אולי להוסיף דקורטורים נוחים), ולהוסיף תמיכה בספקי AI נוספים. בהצלחה! [3][6]

[1] [2] [3] yemot-flow · PyPI
https://pypi.org/project/yemot-flow/
[4] [5] [6] [8] Codex SDK
https://developers.openai.com/codex/sdk/
[7] Codex CLI reference
https://developers.openai.com/codex/cli/reference/